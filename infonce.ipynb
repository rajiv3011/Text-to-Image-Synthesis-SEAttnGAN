{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11268938,"sourceType":"datasetVersion","datasetId":7044128},{"sourceId":11611757,"sourceType":"datasetVersion","datasetId":7283454},{"sourceId":11628994,"sourceType":"datasetVersion","datasetId":7296012}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:25:02.023231Z","iopub.execute_input":"2025-04-30T16:25:02.023889Z","iopub.status.idle":"2025-04-30T16:27:21.889507Z","shell.execute_reply.started":"2025-04-30T16:25:02.023863Z","shell.execute_reply":"2025-04-30T16:27:21.888695Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Text Encoder","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\n\nclass RNNEncoder(nn.Module):\n    def __init__(self, ntoken, ninput=300, drop_prob=0.5,\n                 nhidden=128, nlayers=1, bidirectional=True):\n        super().__init__()\n        self.n_steps = 18\n        self.ntoken = ntoken  # size of the dictionary\n        self.ninput = ninput  # size of each embedding vector\n        self.drop_prob = drop_prob  # probability of an element to be zeroed\n        self.nlayers = nlayers  # Number of recurrent layers\n        self.bidirectional = bidirectional\n        if bidirectional:\n            self.num_directions = 2\n        else:\n            self.num_directions = 1\n        # number of features in the hidden state\n        self.nhidden = nhidden // self.num_directions\n\n        self.encoder = nn.Embedding(self.ntoken, self.ninput)\n        self.drop = nn.Dropout(self.drop_prob)\n\n        # dropout: If non-zero, introduces a dropout layer on\n        # the outputs of each RNN layer except the last layer\n        self.rnn = nn.LSTM(self.ninput, self.nhidden,\n                           self.nlayers, batch_first=True,\n                           dropout=self.drop_prob,\n                           bidirectional=self.bidirectional)\n\n\n    def forward(self, captions, cap_lens):\n        # input: torch.LongTensor of size batch x n_steps\n        # --> emb: batch x n_steps x ninput\n        emb = self.drop(self.encoder(captions))\n        #\n        # Returns: a PackedSequence object\n        cap_lens = cap_lens.data.tolist()\n        emb = pack_padded_sequence(emb, cap_lens, batch_first=True)\n        # #hidden and memory (num_layers * num_directions, batch, hidden_size):\n        # tensor containing the initial hidden state for each element in batch.\n        # #output (batch, seq_len, hidden_size * num_directions)\n        # #or a PackedSequence object:\n        # tensor containing output features (h_t) from the last layer of RNN\n        output, hidden = self.rnn(emb)\n        # PackedSequence object\n        # --> (batch, seq_len, hidden_size * num_directions)\n        output = pad_packed_sequence(output, batch_first=True)[0]\n        words_emb = output.transpose(1, 2)\n        #print('word',words_emb.shape)\n        # output = self.drop(output)\n        # --> batch x hidden_size*num_directions x seq_len\n        # --> batch x num_directions*hidden_size\n        sent_emb = hidden[0].transpose(0, 1).contiguous()\n        sent_emb = sent_emb.view(-1, self.nhidden * self.num_directions)\n        return sent_emb, words_emb\n\n    @staticmethod\n    def load(weights_path: str, ntoken: int) -> 'RNNEncoder':\n        text_encoder = RNNEncoder(ntoken, nhidden=256)\n        state_dict = torch.load(weights_path, map_location=lambda storage, loc: storage)\n        text_encoder.load_state_dict(state_dict)\n        return text_encoder\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:27:25.499136Z","iopub.execute_input":"2025-04-30T16:27:25.499733Z","iopub.status.idle":"2025-04-30T16:27:25.507451Z","shell.execute_reply.started":"2025-04-30T16:27:25.499709Z","shell.execute_reply":"2025-04-30T16:27:25.506790Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Generator","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\n\ndef conv1x1(in_planes, out_planes):\n    \"\"\"1x1 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1,\n                   padding=0, bias=False)\n\nclass GlobalAttentionGeneral(nn.Module):\n    def __init__(self, idf, cdf):\n        super().__init__()\n        self.conv_context = conv1x1(cdf, idf)\n        self.sm = nn.Softmax(dim=1)\n        self.mask = None\n\n    def applyMask(self, mask):\n        self.mask = mask  # batch x sourceL\n\n    def forward(self, input, context):\n        \"\"\"\n        input: batch x idf x ih x iw (queryL=ihxiw)\n        context: batch x cdf x sourceL\n        \"\"\"\n        ih, iw = input.size(2), input.size(3)\n        queryL = ih * iw\n        batch_size, sourceL = context.size(0), context.size(2)\n\n        # --> batch x queryL x idf\n        target = input.view(batch_size, -1, queryL)\n        targetT = torch.transpose(target, 1, 2).contiguous()\n        \n        # batch x cdf x sourceL --> batch x cdf x sourceL x 1\n        sourceT = context.unsqueeze(3)\n        # --> batch x idf x sourceL\n        sourceT = self.conv_context(sourceT).squeeze(3)\n\n        # Get attention\n        attn = torch.bmm(targetT, sourceT)  # batch x queryL x sourceL\n        attn = attn.view(batch_size * queryL, sourceL)\n        \n        if self.mask is not None:\n            mask = self.mask.repeat(queryL, 1)\n            attn.data.masked_fill_(mask.data, -float('inf'))\n        \n        attn = self.sm(attn)\n        attn = attn.view(batch_size, queryL, sourceL)\n        attn = torch.transpose(attn, 1, 2).contiguous()\n\n        # Apply attention\n        weightedContext = torch.bmm(sourceT, attn)\n        weightedContext = weightedContext.view(batch_size, -1, ih, iw)\n        attn = attn.view(batch_size, -1, ih, iw)\n\n        return weightedContext, attn\n\nclass AffineBlock(nn.Module):\n    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n        super().__init__()\n        self.gamma_mlp = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, output_dim)\n        )\n        self.beta_mlp = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, output_dim)\n        )\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for module in self.modules():\n            if isinstance(module, nn.Linear):\n                nn.init.xavier_normal_(module.weight)\n                nn.init.zeros_(module.bias)\n\n    def forward(self, x: Tensor, sentence_embed: Tensor) -> Tensor:\n        scale_param = self.gamma_mlp(sentence_embed).unsqueeze(-1).unsqueeze(-1)\n        shift_param = self.beta_mlp(sentence_embed).unsqueeze(-1).unsqueeze(-1)\n        return scale_param * x + shift_param\n\nclass ResidualBlockG(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, text_dim: int = 256):\n        super().__init__()\n        hidden_dim = text_dim // 2\n        \n        self.affine1 = AffineBlock(text_dim, hidden_dim, in_channels)\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n        \n        self.affine2 = AffineBlock(text_dim, hidden_dim, out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n        \n        self.skip = nn.Conv2d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else nn.Identity()\n        self.gamma = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x: Tensor, sentence_embed: Tensor) -> Tensor:\n        residual = self.skip(x)\n        x = F.leaky_relu(self.affine1(x, sentence_embed), 0.2)\n        x = self.conv1(x)\n        x = F.leaky_relu(self.affine2(x, sentence_embed), 0.2)\n        x = self.conv2(x)\n        return residual + self.gamma * x\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\n\nclass Generator(nn.Module):\n    def __init__(self, n_channels: int = 32, latent_dim: int = 100):\n        super().__init__()\n        self.n_channels = n_channels\n        \n        # Initial projection (4x4 spatial size)\n        self.linear_in = nn.Linear(latent_dim, 8 * n_channels * 4 * 4)\n        \n        # Residual blocks with progressive upsampling\n        self.res_blocks = nn.ModuleList([\n            ResidualBlockG(8 * n_channels, 8 * n_channels),\n            ResidualBlockG(8 * n_channels, 4 * n_channels),\n            ResidualBlockG(4 * n_channels, 2 * n_channels),\n            ResidualBlockG(2 * n_channels, n_channels),\n        ])\n        \n        # Attention mechanism at 64x64 resolution\n        self.att = GlobalAttentionGeneral(n_channels, 256)\n        self.att_conv = nn.Conv2d(2 * n_channels, n_channels, kernel_size=1)\n        \n        # Final upsampling to 256x256\n        self.upsample = nn.Sequential(\n            nn.Upsample(scale_factor=2, mode='nearest'),  # 64->128\n            nn.Conv2d(n_channels, n_channels//2, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n_channels//2),\n            nn.LeakyReLU(0.2, True),\n            \n            nn.Upsample(scale_factor=2, mode='nearest'),  # 128->256\n            nn.Conv2d(n_channels//2, n_channels//4, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n_channels//4),\n            nn.LeakyReLU(0.2, True),\n        )\n        \n        # Output layers\n        self.conv_out = nn.Sequential(\n            nn.Conv2d(n_channels//4, 3, kernel_size=3, padding=1),\n            nn.Tanh()\n        )\n\n    def forward(self, noise: Tensor, sentence_embed: Tensor, word_embed: Tensor) -> Tensor:\n        # Initial projection\n        x = self.linear_in(noise).view(-1, 8 * self.n_channels, 4, 4)\n        \n        # Process through residual blocks with 2x upsampling each\n        for block in self.res_blocks:\n            x = block(x, sentence_embed)\n            x = F.interpolate(x, scale_factor=2, mode='nearest')  # 4->8->16->32->64\n        \n        # Apply attention at 64x64 resolution\n        attn_out, _ = self.att(x, word_embed)\n        x = torch.cat([x, attn_out], dim=1)\n        x = self.att_conv(x)\n        \n        # Final upsampling to 256x256\n        x = self.upsample(x)\n        return self.conv_out(x)\n        \n     \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:27:35.358875Z","iopub.execute_input":"2025-04-30T16:27:35.359369Z","iopub.status.idle":"2025-04-30T16:27:35.377563Z","shell.execute_reply.started":"2025-04-30T16:27:35.359346Z","shell.execute_reply":"2025-04-30T16:27:35.376822Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torchinfo\n\ngen = Generator(n_channels=32, latent_dim=100)\n\nnoise = torch.rand((24, 100))\nsent = torch.rand((24, 256))\nword = torch.rand((24, 256, 18))\n\ntorchinfo.summary(gen, input_data=(noise, sent, word))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:28:01.317277Z","iopub.execute_input":"2025-04-30T16:28:01.317788Z","iopub.status.idle":"2025-04-30T16:28:02.253998Z","shell.execute_reply.started":"2025-04-30T16:28:01.317762Z","shell.execute_reply":"2025-04-30T16:28:02.253219Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nGenerator                                [24, 3, 256, 256]         --\n├─Linear: 1-1                            [24, 4096]                413,696\n├─ModuleList: 1-2                        --                        --\n│    └─ResidualBlockG: 2-1               [24, 256, 4, 4]           1\n│    │    └─Identity: 3-1                [24, 256, 4, 4]           --\n│    │    └─AffineBlock: 3-2             [24, 256, 4, 4]           131,840\n│    │    └─Conv2d: 3-3                  [24, 256, 4, 4]           590,080\n│    │    └─AffineBlock: 3-4             [24, 256, 4, 4]           131,840\n│    │    └─Conv2d: 3-5                  [24, 256, 4, 4]           590,080\n│    └─ResidualBlockG: 2-2               [24, 128, 8, 8]           1\n│    │    └─Conv2d: 3-6                  [24, 128, 8, 8]           32,896\n│    │    └─AffineBlock: 3-7             [24, 256, 8, 8]           131,840\n│    │    └─Conv2d: 3-8                  [24, 128, 8, 8]           295,040\n│    │    └─AffineBlock: 3-9             [24, 128, 8, 8]           98,816\n│    │    └─Conv2d: 3-10                 [24, 128, 8, 8]           147,584\n│    └─ResidualBlockG: 2-3               [24, 64, 16, 16]          1\n│    │    └─Conv2d: 3-11                 [24, 64, 16, 16]          8,256\n│    │    └─AffineBlock: 3-12            [24, 128, 16, 16]         98,816\n│    │    └─Conv2d: 3-13                 [24, 64, 16, 16]          73,792\n│    │    └─AffineBlock: 3-14            [24, 64, 16, 16]          82,304\n│    │    └─Conv2d: 3-15                 [24, 64, 16, 16]          36,928\n│    └─ResidualBlockG: 2-4               [24, 32, 32, 32]          1\n│    │    └─Conv2d: 3-16                 [24, 32, 32, 32]          2,080\n│    │    └─AffineBlock: 3-17            [24, 64, 32, 32]          82,304\n│    │    └─Conv2d: 3-18                 [24, 32, 32, 32]          18,464\n│    │    └─AffineBlock: 3-19            [24, 32, 32, 32]          74,048\n│    │    └─Conv2d: 3-20                 [24, 32, 32, 32]          9,248\n├─GlobalAttentionGeneral: 1-3            [24, 32, 64, 64]          --\n│    └─Conv2d: 2-5                       [24, 32, 18, 1]           8,192\n│    └─Softmax: 2-6                      [98304, 18]               --\n├─Conv2d: 1-4                            [24, 32, 64, 64]          2,080\n├─Sequential: 1-5                        [24, 8, 256, 256]         --\n│    └─Upsample: 2-7                     [24, 32, 128, 128]        --\n│    └─Conv2d: 2-8                       [24, 16, 128, 128]        4,624\n│    └─BatchNorm2d: 2-9                  [24, 16, 128, 128]        32\n│    └─LeakyReLU: 2-10                   [24, 16, 128, 128]        --\n│    └─Upsample: 2-11                    [24, 16, 256, 256]        --\n│    └─Conv2d: 2-12                      [24, 8, 256, 256]         1,160\n│    └─BatchNorm2d: 2-13                 [24, 8, 256, 256]         16\n│    └─LeakyReLU: 2-14                   [24, 8, 256, 256]         --\n├─Sequential: 1-6                        [24, 3, 256, 256]         --\n│    └─Conv2d: 2-15                      [24, 3, 256, 256]         219\n│    └─Tanh: 2-16                        [24, 3, 256, 256]         --\n==========================================================================================\nTotal params: 3,066,279\nTrainable params: 3,066,279\nNon-trainable params: 0\nTotal mult-adds (Units.GIGABYTES): 6.87\n==========================================================================================\nInput size (MB): 0.48\nForward/backward pass size (MB): 401.25\nParams size (MB): 12.27\nEstimated Total Size (MB): 413.99\n=========================================================================================="},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"# Discriminator","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\n\nclass ResidualBlockD(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int):\n        super().__init__()\n        \n        # Main convolution path\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True)\n        )\n\n        # Shortcut path (channel and spatial adjustment)\n        self.shortcut = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False) \n            if in_channels != out_channels else nn.Identity(),\n            nn.AvgPool2d(2)\n        )\n\n        self.gamma = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x: Tensor) -> Tensor:\n        return self.shortcut(x) + self.gamma * self.conv(x)\n\nclass Discriminator(nn.Module):\n    def __init__(self, n_c: int = 32, sentence_embed_dim: int = 256):\n        super().__init__()\n        \n        # Image processing pathway\n        self.img_encoder = nn.Sequential(\n            # Initial convolution (no residual)\n            nn.Conv2d(3, n_c, kernel_size=3, stride=1, padding=1),\n            \n            # Residual downsampling blocks\n            ResidualBlockD(n_c * 1, n_c * 2),  # 256x256 -> 128x128\n            ResidualBlockD(n_c * 2, n_c * 4),  # 128x128 -> 64x64\n            ResidualBlockD(n_c * 4, n_c * 8),  # 64x64 -> 32x32\n            ResidualBlockD(n_c * 8, n_c * 16), # 32x32 -> 16x16\n            \n            # Final downsampling\n            nn.Conv2d(n_c*16, n_c*16, 4, 2, 1),  # 16x16 -> 8x8\n            nn.Conv2d(n_c*16, n_c*16, 4, 2, 1)    # 8x8 -> 4x4\n        )\n\n        # Text-image fusion\n        self.judge_net = nn.Sequential(\n            nn.Conv2d(n_c*16 + sentence_embed_dim, n_c*2, 3, 1, 1),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(n_c*2, 1, 4, 1, 0)  # Final 1x1 output\n        )\n\n    def build_embeds(self, image: Tensor) -> Tensor:\n        \"\"\"Extract image features (same as original)\"\"\"\n        return self.img_encoder(image)\n\n    def get_logits(self, image_embed: Tensor, sentence_embed: Tensor) -> Tensor:\n        \"\"\"Fuse image and text features (same interface)\"\"\"\n        # Expand text to spatial dimensions\n        sentence_embed = sentence_embed.view(-1, 256, 1, 1).expand(-1, -1, 4, 4)\n        \n        # Concatenate and classify\n        combined = torch.cat((image_embed, sentence_embed), dim=1)\n        return self.judge_net(combined)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:28:06.581735Z","iopub.execute_input":"2025-04-30T16:28:06.581999Z","iopub.status.idle":"2025-04-30T16:28:06.591404Z","shell.execute_reply.started":"2025-04-30T16:28:06.581978Z","shell.execute_reply":"2025-04-30T16:28:06.590875Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"import os\nimport pickle\nfrom typing import Dict, List, Optional, Tuple\n\nimport numpy as np\nimport numpy.random as random\nimport pandas as pd\nimport torchvision.transforms as transforms\nfrom PIL import Image\nfrom torch import Tensor\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms import Compose\n\n\nclass DFGANDataset(Dataset):\n    def __init__(self, data_dir: str, split: str = \"train\", transform: Optional[Compose] = None):\n        self.split = split\n        self.data_dir = data_dir\n\n        self.split_dir = os.path.join(data_dir, split)\n        self.captions_path = os.path.join(self.data_dir, \"captions.pickle\")\n        self.filenames_path = os.path.join(self.split_dir, \"filenames.pickle\")\n\n        self.transform = transform\n\n        self.embeddings_num = 10\n\n        self.normalize = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])\n\n        self.images_dir = os.path.join(self.data_dir, \"CUB_200_2011/CUB_200_2011/images\")\n        self.bbox_path = os.path.join(self.data_dir, \"CUB_200_2011/CUB_200_2011/bounding_boxes.txt\")\n        self.images_path = os.path.join(self.data_dir, \"CUB_200_2011/CUB_200_2011/images.txt\")\n\n        self.bbox = self._load_bbox()\n\n        self.file_names, self.captions, self.code2word, self.word2code = self._load_text_data()\n\n        self.n_words = len(self.code2word)\n        self.num_examples = len(self.file_names)\n\n        self._print_info()\n\n    def _print_info(self):\n        #print(f\"Total filenames: {len(self.bbox)}\")\n        #print(f\"Load captions from: {self.captions_path}\")\n        #print(f\"Load file names from: {self.filenames_path} ({self.num_examples})\")\n        #print(f\"Dictionary size: {self.n_words}\")\n        #print(f\"Embeddings number: {self.embeddings_num}\")\n        print()\n\n    def _load_bbox(self) -> Dict[str, List[int]]:\n        df_bbox = pd.read_csv(self.bbox_path, delim_whitespace=True, header=None).astype(int)\n\n        df_image_names = pd.read_csv(self.images_path, delim_whitespace=True, header=None)\n        image_names = df_image_names[1].tolist()\n\n        filename_bbox = dict()\n        for i, file_name in enumerate(image_names):\n            bbox = df_bbox.iloc[i][1:].tolist()\n            filename_bbox[file_name[:-4]] = bbox\n\n        return filename_bbox\n\n    def _load_text_data(self) -> Tuple[List[str], List[List[int]],\n                                       Dict[int, str], Dict[str, int]]:\n        with open(self.captions_path, 'rb') as file:\n            train_captions, test_captions, code2word, word2code = pickle.load(file)\n\n        filenames = self._load_filenames()\n\n        if self.split == 'train':\n            return filenames, train_captions, code2word, word2code\n\n        return filenames, test_captions, code2word, word2code\n\n    def _load_filenames(self) -> List[str]:\n        if os.path.isfile(self.filenames_path):\n            with open(self.filenames_path, 'rb') as file:\n                return pickle.load(file)\n\n        raise ValueError(f\"File {self.filenames_path} does not exist\")\n\n    def _get_caption(self, caption_idx: int) -> Tuple[np.ndarray, int]:\n        caption = np.array(self.captions[caption_idx])\n        pad_caption = np.zeros((18, 1), dtype='int64')\n\n        if len(caption) <= 18:\n            pad_caption[:len(caption), 0] = caption\n            return pad_caption, len(caption)\n\n        indices = list(np.arange(len(caption)))\n        np.random.shuffle(indices)\n        pad_caption[:, 0] = caption[np.sort(indices[:18])]\n\n        return pad_caption, 18\n\n    def _get_image(self, image_path: str, bbox: List[int]) -> Tensor:\n\n        image = Image.open(image_path).convert('RGB')\n        width, height = image.size\n\n        r = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n        center_x = int((2 * bbox[0] + bbox[2]) / 2)\n        center_y = int((2 * bbox[1] + bbox[3]) / 2)\n\n        y1 = np.maximum(0, center_y - r)\n        y2 = np.minimum(height, center_y + r)\n        x1 = np.maximum(0, center_x - r)\n        x2 = np.minimum(width, center_x + r)\n\n        image = image.crop((x1, y1, x2, y2))\n        image = self.normalize(self.transform(image))\n\n        return image\n\n    def _get_random_caption(self, idx: int) -> Tuple[np.ndarray, int]:\n        caption_shift = random.randint(0, self.embeddings_num-1)\n        caption_idx = idx * self.embeddings_num + caption_shift\n\n        if caption_idx >= len(self.captions):\n            caption_idx = len(self.captions) - 1 \n            \n        return self._get_caption(caption_idx)\n\n    def __getitem__(self, idx: int) -> Tuple[Tensor, np.ndarray, int, str]:\n        file_name = self.file_names[idx]\n        image = self._get_image(f\"{self.images_dir}/{file_name}.jpg\", self.bbox[file_name])\n\n        encoded_caption, caption_len = self._get_random_caption(idx)\n\n        return image, encoded_caption, caption_len, file_name\n\n    def __len__(self) -> int:\n        return self.num_examples","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:28:09.656904Z","iopub.execute_input":"2025-04-30T16:28:09.657426Z","iopub.status.idle":"2025-04-30T16:28:12.730889Z","shell.execute_reply.started":"2025-04-30T16:28:09.657404Z","shell.execute_reply":"2025-04-30T16:28:12.730151Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from typing import List, Tuple\n\nimport torch\nfrom torch import Tensor\n\n\ndef prepare_data(batch: Tuple[Tensor, Tensor, Tensor, Tuple[str]],\n                 device: torch.device) -> Tuple[Tensor, Tensor, Tensor, List[str]]:\n    images, captions, captions_len, file_names = batch\n\n    sorted_cap_lens, sorted_cap_indices = torch.sort(captions_len, 0, True)\n    sorted_cap_lens = sorted_cap_lens.to(device)\n\n    sorted_images = images[sorted_cap_indices].to(device)\n    sorted_captions = captions[sorted_cap_indices].squeeze().to(device)\n    sorted_file_names = [file_names[i] for i in sorted_cap_indices.numpy()]\n\n    return sorted_images, sorted_captions, sorted_cap_lens, sorted_file_names\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:28:14.800462Z","iopub.execute_input":"2025-04-30T16:28:14.801097Z","iopub.status.idle":"2025-04-30T16:28:14.806182Z","shell.execute_reply.started":"2025-04-30T16:28:14.801073Z","shell.execute_reply":"2025-04-30T16:28:14.805527Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import random\nfrom typing import List, Tuple\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader,Subset\nfrom torchvision.transforms import transforms\n\n#from objects.dataset import DFGANDataset\n\n\ndef create_loader(imsize: int, batch_size: int, data_dir: str, split: str) -> DataLoader:\n    assert split in [\"train\", \"test\"], \"Wrong split type, expected train or test\"\n    image_transform = transforms.Compose([\n        transforms.Resize(int(imsize * 76 / 64)),\n        transforms.RandomCrop(imsize),\n        transforms.RandomHorizontalFlip()\n    ])\n\n    dataset = DFGANDataset(data_dir, split, image_transform)\n    \n    n_words = dataset.n_words\n    \n    subset_size=6000\n    shuffled_indices = torch.randperm(len(dataset))[:6000].tolist()\n    dataset = Subset(dataset, shuffled_indices)\n    \n\n    print(len(dataset))\n\n    return DataLoader(dataset, batch_size=batch_size, drop_last=True,shuffle=True),n_words\n\n\ndef fix_seed(seed: int = 123321):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n    print(f\"Seed {seed} fixed\")\n\n\ndef plot_losses(g_losses: List[float], d_losses: List[float], d_gp_losses: List[float],\n                path_save: str = \"losses.png\"):\n    plt.style.use(\"seaborn\")\n\n    plt.figure(dpi=256)\n\n    plt.plot(g_losses, label=\"G loss\")\n    plt.plot(d_losses, label=\"D loss\")\n    plt.plot(d_gp_losses, label=\"D MA-GP loss\")\n\n    plt.xlabel(\"Number of epochs\")\n    plt.ylabel(\"Loss value\")\n\n    plt.legend()\n\n    plt.title(\"DF-GAN losses\")\n\n    plt.tight_layout()\n    plt.savefig(path_save)\n    plt.show()\n\n\ndef plot_metrics(fid: List[float], iscore: List[float], epochs: Tuple[int],\n                 path_save: str = \"metrics.png\"):\n    plt.style.use(\"seaborn\")\n    \n    plt.figure(dpi=256)\n\n    plt.plot(fid, label=\"FID\")\n    plt.plot(iscore, label=\"Inception Score\")\n\n    plt.xticks(np.arange(len(epochs)), epochs)\n\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Metric value\")\n\n    plt.legend()\n\n    plt.title(\"Deep Fusion GAN metrics values per epochs\")\n\n    plt.tight_layout()\n    plt.savefig(path_save)\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:28:16.015534Z","iopub.execute_input":"2025-04-30T16:28:16.015813Z","iopub.status.idle":"2025-04-30T16:28:16.027829Z","shell.execute_reply.started":"2025-04-30T16:28:16.015794Z","shell.execute_reply":"2025-04-30T16:28:16.027169Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import os.path\nfrom typing import Tuple, List\nimport torch.nn.functional as F\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.utils as vutils\nfrom torch import Tensor\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import trange\n\n#from discriminator.model import Discriminator\n#from generator.model import Generator\n#from objects.utils import prepare_data\n#from text_encoder.model import RNNEncoder\n\nclass DeepFusionGAN:\n    def __init__(self, n_words, encoder_weights_path: str, image_save_path: str, gen_path_save: str):\n        super().__init__()\n        self.image_save_path = image_save_path\n        self.gen_path_save = gen_path_save\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.generator = Generator(n_channels=32, latent_dim=100).to(self.device)\n        self.discriminator = Discriminator(n_c=32).to(self.device)\n\n        self.text_encoder = RNNEncoder.load(encoder_weights_path, n_words)\n        self.text_encoder.to(self.device)\n\n        for p in self.text_encoder.parameters():\n            p.requires_grad = False\n        self.text_encoder.eval()\n\n        # self.g_optim = torch.optim.Adam(self.generator.parameters(), lr=0.0001, betas=(0.0, 0.9))\n        # self.d_optim = torch.optim.Adam(self.discriminator.parameters(), lr=0.0004, betas=(0.0, 0.9))\n        self.g_optim = torch.optim.Adam(self.generator.parameters(), lr=0.0002,betas=(0.5, 0.99))\n        self.d_optim = torch.optim.Adam(self.discriminator.parameters(), lr=0.0008, betas=(0.5, 0.99))\n\n        \n        self.relu = nn.ReLU()\n\n        self.proj_text = nn.Linear(256, 512).to(self.device)  # Projects text embeddings to match image dim\n        \n        self.temperature = 0.07 \n\n        # Initialize projection layer weights\n        nn.init.xavier_uniform_(self.proj_text.weight)\n        nn.init.zeros_(self.proj_text.bias)\n        \n\n    def _infonce_loss(self, real_embeds, fake_embeds, sentence_embeds):\n        \"\"\"\n        Device-aware InfoNCE loss\n        \"\"\"\n        # Ensure all inputs are on the same device\n        real_embeds = real_embeds.to(self.device)\n        fake_embeds = fake_embeds.to(self.device)\n        sentence_embeds = sentence_embeds.to(self.device)\n        \n        batch_size = real_embeds.size(0)\n        \n        # 1. Average spatial dimensions\n        real_embeds = real_embeds.mean(dim=[2,3])  # [batch_size, 512]\n        fake_embeds = fake_embeds.mean(dim=[2,3])  # [batch_size, 512]\n        \n        # 2. Project text embeddings\n        sentence_embeds = self.proj_text(sentence_embeds)  # [batch_size, 512]\n        \n        # 3. Normalize\n        real_embeds = F.normalize(real_embeds, p=2, dim=1)\n        fake_embeds = F.normalize(fake_embeds, p=2, dim=1)\n        sentence_embeds = F.normalize(sentence_embeds, p=2, dim=1)\n        \n        # 4. Compute similarities\n        pos_sim = torch.sum(real_embeds * sentence_embeds, dim=1) / self.temperature\n        \n        # 5. Negative pairs\n        neg_sim_1 = torch.mm(real_embeds, sentence_embeds.t()) / self.temperature\n        neg_sim_2 = torch.mm(fake_embeds, sentence_embeds.t()) / self.temperature\n        \n        # 6. Combine\n        logits = torch.cat([\n            pos_sim.unsqueeze(1),\n            neg_sim_1,\n            neg_sim_2\n        ], dim=1)\n        \n        # 7. Labels\n        labels = torch.zeros(batch_size, dtype=torch.long).to(self.device)\n        \n        return F.cross_entropy(logits, labels)\n\n    def _zero_grad(self):\n        self.d_optim.zero_grad()\n        self.g_optim.zero_grad()\n\n    def _compute_gp(self, images: Tensor, sentence_embeds: Tensor) -> Tensor:\n        batch_size = images.shape[0]\n\n        images_interpolated = images.data.requires_grad_()\n        sentences_interpolated = sentence_embeds.data.requires_grad_()\n\n        embeds = self.discriminator.build_embeds(images_interpolated)\n        logits = self.discriminator.get_logits(embeds, sentences_interpolated)\n\n        grad_outputs = torch.ones_like(logits)\n        grads = torch.autograd.grad(\n            outputs=logits,\n            inputs=(images_interpolated, sentences_interpolated),\n            grad_outputs=grad_outputs,\n            retain_graph=True,\n            create_graph=True\n        )\n\n        grad_0 = grads[0].reshape(batch_size, -1)\n        grad_1 = grads[1].reshape(batch_size, -1)\n\n        grad = torch.cat((grad_0, grad_1), dim=1)\n        grad_norm = grad.norm(2, 1)\n\n        return grad_norm\n\n    def fit(self, train_loader: DataLoader, num_epochs: int = 500,checkpoint_path: str = None) -> Tuple[List[float], List[float], List[float]]:\n        g_losses_epoch, d_losses_epoch, d_gp_losses_epoch = [], [], []\n\n        start_epoch = 0\n        if checkpoint_path and os.path.exists(checkpoint_path):\n            start_epoch = self._load_gen_weights(checkpoint_path)\n            print(f\"Resuming from epoch {start_epoch}\")\n            \n        for epoch in trange(start_epoch,num_epochs, desc=\"Train Deep Fusion GAN\"):\n\n            g_losses, d_losses, d_gp_losses = [], [], []\n            for batch in train_loader:\n                images, captions, captions_len, _ = prepare_data(batch, self.device)\n                batch_size = images.shape[0]\n\n\n                sentence_embeds, words_embs = self.text_encoder(captions, captions_len)\n                sentence_embeds, words_embs = sentence_embeds.detach(), words_embs.detach()\n                #sentence_embeds, word_embeds = self.text_encoder(captions, captions_len).detach()\n                #print(\"images\",images.shape)\n                real_embeds = self.discriminator.build_embeds(images)\n                #print(\"real_embeds\",real_embeds.shape)\n                #print(\"sent\",sentence_embeds.shape)\n                real_logits = self.discriminator.get_logits(real_embeds, sentence_embeds)\n                #print(\"real_\",real_logits.shape)\n                d_loss_real = self.relu(1.0 - real_logits).mean()\n\n                shift_embeds = real_embeds[:(batch_size - 1)]\n                shift_sentence_embeds = sentence_embeds[1:batch_size]\n                shift_real_image_embeds = self.discriminator.get_logits(shift_embeds, shift_sentence_embeds)\n\n                d_loss_mismatch = self.relu(1.0 + shift_real_image_embeds).mean()\n\n                noise = torch.randn(batch_size, 100).to(self.device)\n\n                #print(\"noise:\", noise.shape)\n                #print(\"sent \",sentence_embeds.shape)\n#                print(\"word\",words_embs.shape)\n                \n                fake_images = self.generator(noise, sentence_embeds, words_embs)\n               # print(\"fake\",fake_images.shape)\n               # if 6==6:\n               #     return [],[],[]\n                \n\n                fake_embeds = self.discriminator.build_embeds(fake_images.detach())\n                fake_logits = self.discriminator.get_logits(fake_embeds, sentence_embeds)\n\n                d_loss_fake = self.relu(1.0 + fake_logits).mean()\n\n                d_loss = d_loss_real + (d_loss_fake + d_loss_mismatch) / 2.0\n\n                # Inside training loop (after computing real/fake logits)\n                contrastive_loss = self._infonce_loss(real_embeds, fake_embeds, sentence_embeds)\n                d_loss = d_loss + 0.1 * contrastive_loss  # Weighted auxiliary loss\n\n                self._zero_grad()\n                d_loss.backward()\n                self.d_optim.step()\n\n                d_losses.append(d_loss.item())\n\n                grad_l2norm = self._compute_gp(images, sentence_embeds)\n                d_loss_gp = 2.0 * torch.mean(grad_l2norm ** 6)\n\n                self._zero_grad()\n                d_loss_gp.backward()\n                self.d_optim.step()\n\n                d_gp_losses.append(d_loss_gp.item())\n\n                fake_embeds = self.discriminator.build_embeds(fake_images)\n                fake_logits = self.discriminator.get_logits(fake_embeds, sentence_embeds)\n                g_loss = -fake_logits.mean()\n\n                self._zero_grad()\n                g_loss.backward()\n                self.g_optim.step()\n\n                g_losses.append(g_loss.item())\n\n\n            g_losses_epoch.append(np.mean(g_losses))\n            d_losses_epoch.append(np.mean(d_losses))\n            d_gp_losses_epoch.append(np.mean(d_gp_losses))\n\n\n            if epoch % 1 == 0:\n             #self._save_fake_image(fake_images, epoch)\n             self._save_gen_weights(epoch)\n\n\n            #print('g_losses_epoch', g_losses_epoch)\n            #print('d_losses_epoch', d_losses_epoch)\n            #print('gp_losses_epoch', d_gp_losses_epoch)\n\n\n\n        return g_losses_epoch, d_losses_epoch, d_gp_losses_epoch\n\n    def _save_fake_image(self, fake_images: Tensor, epoch: int):\n        img_path = os.path.join(self.image_save_path, f\"fake_samplenormal_epoch_{epoch}.png\")\n        vutils.save_image(fake_images.data, img_path, normalize=True)\n\n\n    def _save_gen_weights(self, epoch: int):\n        gen_path = os.path.join(self.gen_path_save, f\"gennormal_{epoch}.pth\")\n        checkpoint = {\n            'epoch': epoch,\n            'generator_state_dict': self.generator.state_dict(),\n            'discriminator_state_dict': self.discriminator.state_dict(),\n            'g_optim_state_dict': self.g_optim.state_dict(),\n            'd_optim_state_dict': self.d_optim.state_dict(),\n        }\n        torch.save(checkpoint, gen_path)\n        print(f\"Model checkpoint saved at epoch {epoch} to {gen_path}\")\n\n    def _load_gen_weights(self, checkpoint_path: str) -> int:\n        checkpoint = torch.load(checkpoint_path)\n        self.generator.load_state_dict(checkpoint['generator_state_dict'])\n        self.discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n        self.g_optim.load_state_dict(checkpoint['g_optim_state_dict'])\n        self.d_optim.load_state_dict(checkpoint['d_optim_state_dict'])\n        start_epoch = checkpoint['epoch'] + 1\n        print(f\"Checkpoint loaded from {checkpoint_path} - Starting from epoch {start_epoch}\")\n        return start_epoch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:28:20.424922Z","iopub.execute_input":"2025-04-30T16:28:20.425195Z","iopub.status.idle":"2025-04-30T16:28:20.447181Z","shell.execute_reply.started":"2025-04-30T16:28:20.425174Z","shell.execute_reply":"2025-04-30T16:28:20.446377Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def train() -> Tuple[List[float], List[float], List[float]]:\n    fix_seed()\n\n    data_path = \"/kaggle/input/seagan-data/cv_seattention/data\"\n    encoder_weights_path = \"/kaggle/input/seagan-data/cv_seattention/text_encoder_weights/text_encoder200.pth\"\n    image_save_path = \"/kaggle/working/gen_images\"\n    gen_path_save = \"/kaggle/working/gen_weights\"\n\n    os.makedirs(image_save_path, exist_ok=True)\n    os.makedirs(gen_path_save, exist_ok=True)\n\n    train_loader, n_words = create_loader(256, 32, data_path, \"train\")\n    model = DeepFusionGAN(n_words=n_words,\n                         encoder_weights_path=encoder_weights_path,\n                         image_save_path=image_save_path,\n                         gen_path_save=gen_path_save)\n\n    # Load your specific checkpoint\n    checkpoint_path = os.path.join(gen_path_save, \"/kaggle/input/500-epoch/gennormal_450.pth\")\n    \n    checkpoint = torch.load(checkpoint_path, map_location=model.device)\n    \n    # Load model states\n    model.generator.load_state_dict(checkpoint['generator_state_dict'])\n    model.discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n    \n    # Load optimizer states\n    model.g_optim.load_state_dict(checkpoint['g_optim_state_dict'])\n    model.d_optim.load_state_dict(checkpoint['d_optim_state_dict'])\n    \n    start_epoch = checkpoint['epoch'] + 1  # Resume from next epoch\n    print(f\"Loaded checkpoint from epoch 50, resuming from epoch {start_epoch}\")\n    \n    num_epochs = 50  # Train for 100 total epochs (including resumed ones)\n    \n    # Train the model\n    return model.fit(train_loader, num_epochs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T17:57:18.075564Z","iopub.execute_input":"2025-04-29T17:57:18.076275Z","iopub.status.idle":"2025-04-29T17:57:18.082069Z","shell.execute_reply.started":"2025-04-29T17:57:18.076254Z","shell.execute_reply":"2025-04-29T17:57:18.081372Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:48:01.754805Z","iopub.execute_input":"2025-04-30T16:48:01.755508Z","iopub.status.idle":"2025-04-30T16:48:01.759194Z","shell.execute_reply.started":"2025-04-30T16:48:01.755457Z","shell.execute_reply":"2025-04-30T16:48:01.758447Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"import os\nimport sys\nimport csv\n\ncurrent_cwd = os.getcwd()\nsrc_path = '/'.join(current_cwd.split('/')[:-1])\nsys.path.append(src_path)\n#from train import train\n#from utils import plot_losses\n\ng_losses_epoch, d_losses_epoch, d_gp_losses_epoch = train()\n\npath = \"/kaggle/working/loss\"\n\nos.makedirs(path, exist_ok=True)\n\nfilenames = [f'{path}/loss1.csv', f'{path}/loss2.csv', f'{path}/loss3.csv']\n\nloss_values = [g_losses_epoch, d_losses_epoch, d_gp_losses_epoch]\n\nfor i in range(len(loss_values)):\n    filename = filenames[i]\n    with open(filename, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Loss'])\n        for loss in loss_values[i]:  \n            writer.writerow([loss])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:42:00.189718Z","iopub.execute_input":"2025-04-03T15:42:00.190091Z","iopub.status.idle":"2025-04-03T22:34:11.775080Z","shell.execute_reply.started":"2025-04-03T15:42:00.190058Z","shell.execute_reply":"2025-04-03T22:34:11.774353Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\n\ndef get_latest_checkpoint(checkpoint_dir):\n    \"\"\"Returns the latest checkpoint file from a directory.\"\"\"\n    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith(\".pth\")]\n    \n    if not checkpoints:\n        print(\"No checkpoint found.\")\n        return None\n    \n    # Sort by epoch number (assuming filenames are like gennormal_49.pth)\n    latest_checkpoint = max(checkpoints, key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n    return os.path.join(checkpoint_dir, latest_checkpoint)\n\ndef save_latest_model_to_kaggle(source_dir, kaggle_working_dir=\"/kaggle/working/\"):\n    \"\"\"Finds the latest checkpoint in source_dir and copies it to /kaggle/working/.\"\"\"\n    latest_checkpoint = get_latest_checkpoint(source_dir)\n    \n    if latest_checkpoint:\n        os.makedirs(kaggle_working_dir, exist_ok=True)  # Ensure the target directory exists\n        target_path = os.path.join(kaggle_working_dir, os.path.basename(latest_checkpoint))\n        shutil.copy(latest_checkpoint, target_path)\n        print(f\"✅ Latest model saved to: {target_path}\")\n    else:\n        print(\"❌ No checkpoint found in the source directory.\")\n\n# Example usage\nsave_latest_model_to_kaggle(\"/kaggle/working/gen_weights\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom torch import Tensor\n\n\n\n\n@torch.no_grad()\ndef generate_images(generator: Generator, sentence_embeds: Tensor,\n                    device: torch.device) -> Tensor:\n    batch_size = sentence_embeds.shape[0]\n    noise = torch.randn(batch_size, 100).to(device)\n    return generator(noise, sentence_embeds)\n\n\ndef save_image(image: np.ndarray, save_dir: str, file_name: str):\n    # [-1, 1] --> [0, 255]\n    image = (image + 1.0) * 127.5\n    image = image.astype(np.uint8)\n    image = np.transpose(image, (1, 2, 0))\n    image = Image.fromarray(image)\n    fullpath = os.path.join(save_dir, f\"{file_name.replace('/', '_')}.png\")\n    image.save(fullpath)\n\n\ndef sample(generator: Generator, text_encoder: RNNEncoder, batch, save_dir: str):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    os.makedirs(save_dir, exist_ok=True)\n\n    images, captions, captions_len, file_names = prepare_data(batch, device)\n    sent_emb = text_encoder(captions, captions_len).detach()\n\n    fake_images = generate_images(generator, sent_emb, device)\n\n    for i in range(images.shape[0]):\n        im = fake_images[i].data.cpu().numpy()\n        save_image(im, save_dir, file_names[i])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport sys\nimport numpy as np\nimport torch\nimport time\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Ensure correct path handling\ncurrent_cwd = os.getcwd()\nsrc_path = '/'.join(current_cwd.split('/')[:-1])\nsys.path.append(src_path)\n\n  # Assuming save_image is a defined function\n\n# Initialize generator\ngenerator = Generator(n_channels=32, latent_dim=100).to(device)\n\n# Load generator checkpoint\ncheckpoint_path = \"/kaggle/input/seagan49/pytorch/default/1/gennormal_49.pth\"\ncheckpoint = torch.load(checkpoint_path, map_location=device)\n\ngenerator.load_state_dict(checkpoint['generator_state_dict'])  # Corrected\ngenerator.eval()  # Set to evaluation mode\n\n# Create data loader\ntrain_loader,n_words = create_loader(256, 24, \"/kaggle/input/seagan-data/cv_seattention/data\", \"test\")\n\n\n\n# Load text encoder\ntext_encoder = RNNEncoder.load(\"/kaggle/input/seagan-data/cv_seattention/text_encoder_weights/text_encoder200.pth\", n_words)\ntext_encoder.to(device)\n\n# Freeze text encoder parameters\nfor p in text_encoder.parameters():\n    p.requires_grad = False\ntext_encoder.eval()\n\n# Create output directory\npath = \"/kaggle/working\"\nos.makedirs(path, exist_ok=True)\n\n# Function to generate an image from text\ndef gen_own_bird(word_caption, name, i):\n    dataset = train_loader.dataset\n    codes = [dataset.word2code[w] for w in word_caption.lower().split()]\n    \n    caption = np.array(codes)\n    pad_caption = np.zeros((18, 1), dtype='int64')\n\n    if len(caption) <= 18:\n        pad_caption[:len(caption), 0] = caption\n        len_ = len(caption)\n    else:\n        indices = list(np.arange(len(caption)))\n        np.random.shuffle(indices)\n        pad_caption[:, 0] = caption[np.sort(indices[:18])]\n        len_ = 18\n\n    tensor1 = torch.tensor(pad_caption).reshape(1, -1).to(device)\n    tensor2 = torch.tensor([len_]).to(device)\n    \n    # Encode text\n    embed, word = text_encoder(tensor1, tensor2)\n\n    # Generate image\n    batch_size = embed.shape[0]\n    noise = torch.randn(batch_size, 100).to(device)\n    img = generator(noise, embed, word)\n\n    # Save image\n    save_image(img[0].data.cpu().numpy(), path, name + str(i))\n\n# Run generation\nstart_time = time.time()\ncaption = \"A blue bird\"\ni = 1\ngen_own_bird(caption, caption, i)\n\nprint(f\"Time taken: {time.time() - start_time:.2f} seconds\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_images(generator: Generator, sentence_embeds: Tensor,word,\n                    device: torch.device) -> Tensor:\n    batch_size = sentence_embeds.shape[0]\n    noise = torch.randn(batch_size, 100).to(device)\n    return generator(noise, sentence_embeds,word)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:28:37.697382Z","iopub.execute_input":"2025-04-30T16:28:37.697940Z","iopub.status.idle":"2025-04-30T16:28:37.701928Z","shell.execute_reply.started":"2025-04-30T16:28:37.697919Z","shell.execute_reply":"2025-04-30T16:28:37.701223Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"\n'''\nimport os\nimport sys\n\n\n\ncurrent_cwd = os.getcwd()\nsrc_path = '/'.join(current_cwd.split('/')[:-1])\nsys.path.append(src_path)\n\n'''\n\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom scipy import linalg\nfrom scipy.linalg import sqrtm\nfrom scipy.stats import entropy\nfrom torch.nn.functional import adaptive_avg_pool2d\nfrom tqdm.auto import tqdm\n\n#from sample import prepare_data, generate_images\n#from src.generator.model import Generator\n#from src.text_encoder.model import RNNEncoder\n#from utils import create_loader\n\n\n\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\n\n\n\n\nclass InceptionV3(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.device = f'cuda:{0}' if torch.cuda.is_available() else 'cpu'\n        self.model = torch.hub.load('pytorch/vision:v0.6.0', 'inception_v3', pretrained=True).to(self.device)\n        print(self.model.fc)\n        self.linear = self.model.fc\n        self.model.fc, self.model.dropout = [nn.Sequential()] * 2\n      \n    @torch.no_grad()\n    def get_last_layer(self, x):\n        x = F.interpolate(x, size=300, mode='bilinear', align_corners=False, recompute_scale_factor=False)\n        return self.model(x)\n\n\n\n\n\nclassifier = InceptionV3().to(device)\nclassifier = classifier.eval()\n\n\n\n\nbatch_size = 32\ntest_loader, n_words = create_loader(256, batch_size, \"/kaggle/input/seagan-data/cv_seattention/data\", \"test\")\n\ncheckpoint = torch.load(\"/kaggle/input/500-epoch/gennormal_450.pth\", map_location=device)\n#generator = Generator(n_channels=32, latent_dim=100).to(device)\n#generator.load_state_dict(checkpoint['generator_state_dict'])\n#generator.eval()\n\n\n\ngenerator = Generator(n_channels=32, latent_dim=100).to(device)\ngenerator.load_state_dict(checkpoint['generator_state_dict'])\ngenerator.eval()\n\n\n\n\ntext_encoder = RNNEncoder.load(\"/kaggle/input/seagan-data/cv_seattention/text_encoder_weights/text_encoder200.pth\", n_words)\ntext_encoder.to(device)\n\nfor p in text_encoder.parameters():\n    p.requires_grad = False\ntext_encoder = text_encoder.eval()\n\n\n\ndef calculate_fid(repr1, repr2):\n    # shape of reprs: (-1, embed_dim)\n    \n    # shape of mus: (embed_dim, )\n    mu_r, mu_g = np.mean(repr1, axis=0), np.mean(repr2, axis=0)\n    # rowvar=False:\n    #     each column represents a variable, while the rows contain observations\n    # shape of sigmas: (embed_dim, embed_dim)\n    sigma_r, sigma_g = np.cov(repr1, rowvar=False), np.cov(repr2, rowvar=False)\n    \n    diff = mu_r - mu_g\n    diff_square_norm = diff.dot(diff)\n    \n    product = sigma_r.dot(sigma_g)\n    sqrt_product, _ = sqrtm(product, disp=False)\n    \n\n    if not np.isfinite(sqrt_product).all():\n        eye_matrix = np.eye(sigma_r.shape[0]) * 1e-8\n        sqrt_product = linalg.sqrtm((sigma_r + eye_matrix).dot(sigma_g + eye_matrix))\n    \n    # np.iscomplexobj:\n    #     Check for a complex type or an array of complex numbers.\n    #     The return value, True if x is of a complex type\n    #     or has at least one complex element.\n    if np.iscomplexobj(sqrt_product):\n        sqrt_product = sqrt_product.real\n\n    fid = diff_square_norm + np.trace(sigma_r + sigma_g - 2 * sqrt_product)\n\n    \n    return fid\n\n\n\n\ndef build_representations():\n    real_reprs = np.zeros((len(test_loader) * batch_size, 2048))\n    fake_reprs = np.zeros((len(test_loader) * batch_size, 2048))\n    #real_reprs = np.zeros(1, 2048))\n    #fake_reprs = np.zeros(1, 2048))\n\n    \n    for i, batch in enumerate(tqdm(test_loader, desc=\"Build representations\")):\n        images, captions, captions_len, file_names = prepare_data(batch, device)\n        sent_emb, word = text_encoder(captions, captions_len)\n        sent_emb = sent_emb.detach()\n        fake_images = generate_images(generator, sent_emb,word, device)\n\n        clf_out_real = classifier.get_last_layer(images)\n        clf_out_fake = classifier.get_last_layer(fake_images)\n\n\n        real_reprs[i * batch_size: (i + 1) * batch_size] = clf_out_real.cpu().numpy()\n        fake_reprs[i * batch_size: (i + 1) * batch_size] = clf_out_fake.cpu().numpy()\n            \n    return real_reprs, fake_reprs\n\n\n\ndef inception_score(reprs, batch_size):\n    def get_pred(x):\n        x = classifier.linear(torch.tensor(x, dtype=torch.float))\n        return F.softmax(x).data.cpu().numpy()\n\n\n    preds = np.zeros((reprs.shape[0], 1000))\n\n    splits = 0\n    for i in range(0, len(preds), batch_size):\n        aaai = reprs[i:i + batch_size]\n        aai = torch.tensor(aaai)\n        aai = aai.to(device)\n        z = get_pred(aai)\n        preds[i:i + batch_size] = z\n        splits += 1\n    \n    split_scores = []\n\n    for k in range(splits):\n        part = preds[k * batch_size: (k+1) * batch_size, :]\n        py = np.mean(part, axis=0)\n        \n        scores = []\n        for i in range(part.shape[0]):\n            pyx = part[i, :]\n            scores.append(entropy(pyx, py))\n            \n        split_scores.append(np.exp(np.mean(scores)))\n\n    return np.mean(split_scores), np.std(split_scores)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:48:58.795077Z","iopub.execute_input":"2025-04-30T16:48:58.795351Z","iopub.status.idle":"2025-04-30T16:48:59.963198Z","shell.execute_reply.started":"2025-04-30T16:48:58.795330Z","shell.execute_reply":"2025-04-30T16:48:59.962647Z"}},"outputs":[{"name":"stderr","text":"Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.6.0\n","output_type":"stream"},{"name":"stdout","text":"Linear(in_features=2048, out_features=1000, bias=True)\n\n2933\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"model_path=[\"/kaggle/input/gen-data/gennormal_50.pth\",\n           \"/kaggle/input/gen-data/gennormal_100.pth\",\n           \"/kaggle/input/gen-data/gennormal_150.pth\",\n           \"/kaggle/input/gen-data/gennormal_200.pth\",\n           \"/kaggle/input/gen-data/gennormal_250.pth\",\n           \"/kaggle/input/gen-data/gennormal_300.pth\",\n           \"/kaggle/input/gen-data/gennormal_350.pth\",\n           \"/kaggle/input/gen-data/gennormal_400.pth\",\n           \"/kaggle/input/gen-data/gennormal_450.pth\",\n           \"/kaggle/input/gen-data/gennormal_500.pth\" ]\n\nfid_test=[]\ninception_test=[]\nfor i in range(10):\n    batch_size = 32\n    test_loader, n_words = create_loader(256, batch_size, \"/kaggle/input/seagan-data/cv_seattention/data\", \"test\")\n\n    checkpoint = torch.load(model_path[i], map_location=device)\n\n    generator = Generator(n_channels=32, latent_dim=100).to(device)\n    generator.load_state_dict(checkpoint['generator_state_dict'])\n    generator.eval()\n\n    text_encoder = RNNEncoder.load(\"/kaggle/input/seagan-data/cv_seattention/text_encoder_weights/text_encoder200.pth\", n_words)\n    text_encoder.to(device)\n\n    for p in text_encoder.parameters():\n        p.requires_grad = False\n    text_encoder = text_encoder.eval()\n\n    real_values, fake_values = build_representations()\n    real_values = torch.tensor(real_values)\n    fake_values = torch.tensor(fake_values)\n\n    fid_value = calculate_fid(real_values.numpy(), fake_values.numpy())\n    fid_test.append(fid_value)\n    print(\"epoch : \", 50*(i+1))\n    print(f\"FID value = {fid_value}\")\n\n\n    a,b = inception_score(fake_values, batch_size)\n    inception_test.append(a)\n    print('inception score mean: ',a)\n    print('inception score std: ',b)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T16:52:07.885698Z","iopub.execute_input":"2025-04-30T16:52:07.886259Z","iopub.status.idle":"2025-04-30T17:00:48.992300Z","shell.execute_reply.started":"2025-04-30T16:52:07.886231Z","shell.execute_reply":"2025-04-30T17:00:48.991678Z"}},"outputs":[{"name":"stdout","text":"\n2933\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Build representations:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba9681d77f114169b76a53ca54b54f71"}},"metadata":{}},{"name":"stdout","text":"epoch :  50\nFID value = 239.16315648374888\ninception score mean:  3.665368562833094\ninception score std:  0.4564176166540998\n\n2933\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Build representations:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac78ed70b6a34b89b1d1f9bbeac9c030"}},"metadata":{}},{"name":"stdout","text":"epoch :  100\nFID value = 199.70167566127094\ninception score mean:  4.013044417196\ninception score std:  0.449900430253348\n\n2933\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Build representations:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcf3eabe93864331b4e167dcaf43970d"}},"metadata":{}},{"name":"stdout","text":"epoch :  150\nFID value = 188.72013319423274\ninception score mean:  4.912149990810083\ninception score std:  0.5084494270539012\n\n2933\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Build representations:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a6ab16de97b4d08a285ead2c1c7ded6"}},"metadata":{}},{"name":"stdout","text":"epoch :  200\nFID value = 165.3300923717885\ninception score mean:  4.6177054821053725\ninception score std:  0.5074478407645858\n\n2933\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Build representations:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c5489b320dc40c8b7a6c5d8bd17368e"}},"metadata":{}},{"name":"stdout","text":"epoch :  250\nFID value = 152.93118386676804\ninception score mean:  4.7066562333778545\ninception score std:  0.47906367723975896\n\n2933\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Build representations:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f59eed232c30434abd2f47f5a571e181"}},"metadata":{}},{"name":"stdout","text":"epoch :  300\nFID value = 143.1520524315741\ninception score mean:  5.1015656691406415\ninception score std:  0.6574442416405288\n\n2933\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Build representations:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9dd976f50a044ad6a749b99b66423d43"}},"metadata":{}},{"name":"stdout","text":"epoch :  350\nFID value = 134.26604557884923\ninception score mean:  4.878952122132435\ninception score std:  0.5305331121740264\n\n2933\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Build representations:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3a3e83e7c3445389a03f9e739d9b82a"}},"metadata":{}},{"name":"stdout","text":"epoch :  400\nFID value = 118.75548316299229\ninception score mean:  5.159469692968909\ninception score std:  0.536023739920701\n\n2933\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Build representations:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec55bd26e4664629a992db31596637a1"}},"metadata":{}},{"name":"stdout","text":"epoch :  450\nFID value = 154.16633024715497\ninception score mean:  5.016804800624812\ninception score std:  0.5691209073481324\n\n2933\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Build representations:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0f5899e741d413a9824b2f5cb634945"}},"metadata":{}},{"name":"stdout","text":"epoch :  500\nFID value = 154.5018108731702\ninception score mean:  4.67622754118785\ninception score std:  0.5365915143315668\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"x=[50,100,150,200,250,300,350,400,450,500]\nprint(fid_test)\nplt.plot(x,fid_test)\nplt.xlabel()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}