{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-30T04:51:46.067313Z",
     "iopub.status.busy": "2025-04-30T04:51:46.066739Z",
     "iopub.status.idle": "2025-04-30T04:51:48.595125Z",
     "shell.execute_reply": "2025-04-30T04:51:48.594326Z",
     "shell.execute_reply.started": "2025-04-30T04:51:46.067288Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Subset\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "#from objects.dataset import DFGANDataset\n",
    "\n",
    "\n",
    "def create_loader(imsize: int, batch_size: int, data_dir: str, split: str) -> DataLoader:\n",
    "    assert split in [\"train\", \"test\"], \"Wrong split type, expected train or test\"\n",
    "    image_transform = transforms.Compose([\n",
    "        transforms.Resize(int(imsize * 76 / 64)),\n",
    "        transforms.RandomCrop(imsize),\n",
    "        transforms.RandomHorizontalFlip()\n",
    "    ])\n",
    "\n",
    "    dataset = DFGANDataset(data_dir, split, image_transform)\n",
    "    \n",
    "    n_words = dataset.n_words\n",
    "    \n",
    "    subset_size=6000\n",
    "    shuffled_indices = torch.randperm(len(dataset))[:6000].tolist()\n",
    "    dataset = Subset(dataset, shuffled_indices)\n",
    "    \n",
    "\n",
    "    print(len(dataset))\n",
    "\n",
    "    return DataLoader(dataset, batch_size=batch_size, drop_last=True,shuffle=True),n_words\n",
    "\n",
    "\n",
    "def fix_seed(seed: int = 123321):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    print(f\"Seed {seed} fixed\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T04:51:48.699079Z",
     "iopub.status.busy": "2025-04-30T04:51:48.698734Z",
     "iopub.status.idle": "2025-04-30T04:51:49.086818Z",
     "shell.execute_reply": "2025-04-30T04:51:49.086249Z",
     "shell.execute_reply.started": "2025-04-30T04:51:48.699057Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as random\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "\n",
    "class DFGANDataset(Dataset):\n",
    "    def __init__(self, data_dir: str, split: str = \"train\", transform: Optional[Compose] = None):\n",
    "        self.split = split\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "        self.split_dir = os.path.join(data_dir, split)\n",
    "        self.captions_path = os.path.join(self.data_dir, \"captions.pickle\")\n",
    "        self.filenames_path = os.path.join(self.split_dir, \"filenames.pickle\")\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        self.embeddings_num = 10\n",
    "\n",
    "        self.normalize = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "\n",
    "        self.images_dir = os.path.join(self.data_dir, \"CUB_200_2011/CUB_200_2011/images\")\n",
    "        self.bbox_path = os.path.join(self.data_dir, \"CUB_200_2011/CUB_200_2011/bounding_boxes.txt\")\n",
    "        self.images_path = os.path.join(self.data_dir, \"CUB_200_2011/CUB_200_2011/images.txt\")\n",
    "\n",
    "        self.bbox = self._load_bbox()\n",
    "\n",
    "        self.file_names, self.captions, self.code2word, self.word2code = self._load_text_data()\n",
    "\n",
    "        self.n_words = len(self.code2word)\n",
    "        self.num_examples = len(self.file_names)\n",
    "\n",
    "        self._print_info()\n",
    "\n",
    "    def _print_info(self):\n",
    "        print(f\"Total filenames: {len(self.bbox)}\")\n",
    "        print(f\"Load captions from: {self.captions_path}\")\n",
    "        print(f\"Load file names from: {self.filenames_path} ({self.num_examples})\")\n",
    "        print(f\"Dictionary size: {self.n_words}\")\n",
    "        print(f\"Embeddings number: {self.embeddings_num}\")\n",
    "\n",
    "    def _load_bbox(self) -> Dict[str, List[int]]:\n",
    "        df_bbox = pd.read_csv(self.bbox_path, delim_whitespace=True, header=None).astype(int)\n",
    "\n",
    "        df_image_names = pd.read_csv(self.images_path, delim_whitespace=True, header=None)\n",
    "        image_names = df_image_names[1].tolist()\n",
    "\n",
    "        filename_bbox = dict()\n",
    "        for i, file_name in enumerate(image_names):\n",
    "            bbox = df_bbox.iloc[i][1:].tolist()\n",
    "            filename_bbox[file_name[:-4]] = bbox\n",
    "\n",
    "        return filename_bbox\n",
    "\n",
    "    def _load_text_data(self) -> Tuple[List[str], List[List[int]],\n",
    "                                       Dict[int, str], Dict[str, int]]:\n",
    "        with open(self.captions_path, 'rb') as file:\n",
    "            train_captions, test_captions, code2word, word2code = pickle.load(file)\n",
    "\n",
    "        filenames = self._load_filenames()\n",
    "\n",
    "        if self.split == 'train':\n",
    "            return filenames, train_captions, code2word, word2code\n",
    "\n",
    "        return filenames, test_captions, code2word, word2code\n",
    "\n",
    "    def _load_filenames(self) -> List[str]:\n",
    "        if os.path.isfile(self.filenames_path):\n",
    "            with open(self.filenames_path, 'rb') as file:\n",
    "                return pickle.load(file)\n",
    "\n",
    "        raise ValueError(f\"File {self.filenames_path} does not exist\")\n",
    "\n",
    "    def _get_caption(self, caption_idx: int) -> Tuple[np.ndarray, int]:\n",
    "        caption = np.array(self.captions[caption_idx])\n",
    "        pad_caption = np.zeros((18, 1), dtype='int64')\n",
    "\n",
    "        if len(caption) <= 18:\n",
    "            pad_caption[:len(caption), 0] = caption\n",
    "            return pad_caption, len(caption)\n",
    "\n",
    "        indices = list(np.arange(len(caption)))\n",
    "        np.random.shuffle(indices)\n",
    "        pad_caption[:, 0] = caption[np.sort(indices[:18])]\n",
    "\n",
    "        return pad_caption, 18\n",
    "\n",
    "    def _get_image(self, image_path: str, bbox: List[int]) -> Tensor:\n",
    "\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        width, height = image.size\n",
    "\n",
    "        r = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n",
    "        center_x = int((2 * bbox[0] + bbox[2]) / 2)\n",
    "        center_y = int((2 * bbox[1] + bbox[3]) / 2)\n",
    "\n",
    "        y1 = np.maximum(0, center_y - r)\n",
    "        y2 = np.minimum(height, center_y + r)\n",
    "        x1 = np.maximum(0, center_x - r)\n",
    "        x2 = np.minimum(width, center_x + r)\n",
    "\n",
    "        image = image.crop((x1, y1, x2, y2))\n",
    "        image = self.normalize(self.transform(image))\n",
    "\n",
    "        return image\n",
    "\n",
    "    def _get_random_caption(self, idx: int) -> Tuple[np.ndarray, int]:\n",
    "        caption_shift = random.randint(0, self.embeddings_num-1)\n",
    "        caption_idx = idx * self.embeddings_num + caption_shift\n",
    "\n",
    "        if caption_idx >= len(self.captions):\n",
    "            caption_idx = len(self.captions) - 1 \n",
    "            \n",
    "        return self._get_caption(caption_idx)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Tensor, np.ndarray, int, str]:\n",
    "        file_name = self.file_names[idx]\n",
    "        image = self._get_image(f\"{self.images_dir}/{file_name}.jpg\", self.bbox[file_name])\n",
    "\n",
    "        encoded_caption, caption_len = self._get_random_caption(idx)\n",
    "\n",
    "        return image, encoded_caption, caption_len, file_name\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T04:51:52.628884Z",
     "iopub.status.busy": "2025-04-30T04:51:52.628494Z",
     "iopub.status.idle": "2025-04-30T04:51:52.634546Z",
     "shell.execute_reply": "2025-04-30T04:51:52.633817Z",
     "shell.execute_reply.started": "2025-04-30T04:51:52.628860Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def prepare_data(batch: Tuple[Tensor, Tensor, Tensor, Tuple[str]],\n",
    "                 device: torch.device) -> Tuple[Tensor, Tensor, Tensor, List[str]]:\n",
    "    images, captions, captions_len, file_names = batch\n",
    "\n",
    "    sorted_cap_lens, sorted_cap_indices = torch.sort(captions_len, 0, True)\n",
    "    sorted_cap_lens = sorted_cap_lens.to(device)\n",
    "\n",
    "    sorted_images = images[sorted_cap_indices].to(device)\n",
    "    sorted_captions = captions[sorted_cap_indices].squeeze().to(device)\n",
    "    sorted_file_names = [file_names[i] for i in sorted_cap_indices.numpy()]\n",
    "\n",
    "    return sorted_images, sorted_captions, sorted_cap_lens, sorted_file_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T04:51:54.332846Z",
     "iopub.status.busy": "2025-04-30T04:51:54.332160Z",
     "iopub.status.idle": "2025-04-30T04:51:54.345465Z",
     "shell.execute_reply": "2025-04-30T04:51:54.344519Z",
     "shell.execute_reply.started": "2025-04-30T04:51:54.332818Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class RNNEncoder(nn.Module):\n",
    "    def __init__(self, ntoken, ninput=300, drop_prob=0.5,\n",
    "                 nhidden=128, nlayers=1, bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.n_steps = 18\n",
    "        self.ntoken = ntoken  # size of the dictionary\n",
    "        self.ninput = ninput  # size of each embedding vector\n",
    "        self.drop_prob = drop_prob  # probability of an element to be zeroed\n",
    "        self.nlayers = nlayers  # Number of recurrent layers\n",
    "        self.bidirectional = bidirectional\n",
    "        if bidirectional:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "        # number of features in the hidden state\n",
    "        self.nhidden = nhidden // self.num_directions\n",
    "\n",
    "        self.encoder = nn.Embedding(self.ntoken, self.ninput)\n",
    "        self.drop = nn.Dropout(self.drop_prob)\n",
    "\n",
    "        # dropout: If non-zero, introduces a dropout layer on\n",
    "        # the outputs of each RNN layer except the last layer\n",
    "        self.rnn = nn.LSTM(self.ninput, self.nhidden,\n",
    "                           self.nlayers, batch_first=True,\n",
    "                           dropout=self.drop_prob,\n",
    "                           bidirectional=self.bidirectional)\n",
    "\n",
    "\n",
    "    def forward(self, captions, cap_lens):\n",
    "        # input: torch.LongTensor of size batch x n_steps\n",
    "        # --> emb: batch x n_steps x ninput\n",
    "        emb = self.drop(self.encoder(captions))\n",
    "        #\n",
    "        # Returns: a PackedSequence object\n",
    "        cap_lens = cap_lens.data.tolist()\n",
    "        emb = pack_padded_sequence(emb, cap_lens, batch_first=True)\n",
    "        # #hidden and memory (num_layers * num_directions, batch, hidden_size):\n",
    "        # tensor containing the initial hidden state for each element in batch.\n",
    "        # #output (batch, seq_len, hidden_size * num_directions)\n",
    "        # #or a PackedSequence object:\n",
    "        # tensor containing output features (h_t) from the last layer of RNN\n",
    "        output, hidden = self.rnn(emb)\n",
    "        # PackedSequence object\n",
    "        # --> (batch, seq_len, hidden_size * num_directions)\n",
    "        output = pad_packed_sequence(output, batch_first=True)[0]\n",
    "        words_emb = output.transpose(1, 2)\n",
    "        #print('word',words_emb.shape)\n",
    "        # output = self.drop(output)\n",
    "        # --> batch x hidden_size*num_directions x seq_len\n",
    "        # --> batch x num_directions*hidden_size\n",
    "        sent_emb = hidden[0].transpose(0, 1).contiguous()\n",
    "        sent_emb = sent_emb.view(-1, self.nhidden * self.num_directions)\n",
    "        return sent_emb, words_emb\n",
    "\n",
    "    @staticmethod\n",
    "    def load(weights_path: str, ntoken: int) -> 'RNNEncoder':\n",
    "        text_encoder = RNNEncoder(ntoken, nhidden=256)\n",
    "        state_dict = torch.load(weights_path, map_location=lambda storage, loc: storage)\n",
    "        text_encoder.load_state_dict(state_dict)\n",
    "        return text_encoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T04:51:56.939456Z",
     "iopub.status.busy": "2025-04-30T04:51:56.939161Z",
     "iopub.status.idle": "2025-04-30T04:53:17.461917Z",
     "shell.execute_reply": "2025-04-30T04:53:17.461041Z",
     "shell.execute_reply.started": "2025-04-30T04:51:56.939434Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: open-clip-torch in /usr/local/lib/python3.11/dist-packages (2.32.0)\n",
      "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (2.5.1+cu124)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (0.20.1+cu124)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (2024.11.6)\n",
      "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (6.3.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (0.30.2)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (0.5.2)\n",
      "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from open-clip-torch) (1.0.14)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (4.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9.0->open-clip-torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.9.0->open-clip-torch) (1.3.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->open-clip-torch) (0.2.13)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open-clip-torch) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open-clip-torch) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open-clip-torch) (2.32.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->open-clip-torch) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->open-clip-torch) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9.0->open-clip-torch) (3.0.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open-clip-torch) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open-clip-torch) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open-clip-torch) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open-clip-torch) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open-clip-torch) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->open-clip-torch) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->open-clip-torch) (2025.1.31)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->open-clip-torch) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->open-clip-torch) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->open-clip-torch) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision->open-clip-torch) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision->open-clip-torch) (2024.2.0)\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-k_c9637w\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-k_c9637w\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.5.1+cu124)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.20.1+cu124)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->clip==1.0) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision->clip==1.0) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision->clip==1.0) (2024.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_136/3649513097.py:54: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df_bbox = pd.read_csv(self.bbox_path, delim_whitespace=True, header=None).astype(int)\n",
      "/tmp/ipykernel_136/3649513097.py:56: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df_image_names = pd.read_csv(self.images_path, delim_whitespace=True, header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total filenames: 11788\n",
      "Load captions from: /kaggle/input/projec/cv_seattention/cv_seattention/data/captions.pickle\n",
      "Load file names from: /kaggle/input/projec/cv_seattention/cv_seattention/data/train/filenames.pickle (8855)\n",
      "Dictionary size: 5450\n",
      "Embeddings number: 10\n",
      "6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_136/2969949593.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(weights_path, map_location=lambda storage, loc: storage)\n",
      "Creating CLIP adapter dataset: 100%|██████████| 187/187 [01:01<00:00,  3.05it/s]\n",
      "100%|████████████████████████████████████████| 338M/338M [00:02<00:00, 127MiB/s]\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch torchvision clip-by-openai tqdm\n",
    "!pip install open-clip-torch\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import open_clip\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import clip\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Step 2: Implement CLIP Encoder\n",
    "class CLIPTextEncoder(nn.Module):\n",
    "    def __init__(self, ntoken, ninput=300, drop_prob=0.5, nhidden=128, bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.n_steps = 18  # same as RNNEncoder\n",
    "        self.ntoken = ntoken  # vocabulary size\n",
    "        self.ninput = ninput  # embedding size (ignored for CLIP, but kept for compatibility)\n",
    "        self.drop_prob = drop_prob  # dropout probability\n",
    "        self.nlayers = 1  # transformers don't use nlayers in the same way as RNNs\n",
    "        self.bidirectional = bidirectional  # not applicable for CLIP\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.nhidden = nhidden // self.num_directions  # match dimension of RNNEncoder\n",
    "\n",
    "        # Load CLIP model\n",
    "        self.clip_model, _ = clip.load(\"ViT-B/32\", device=\"cpu\")\n",
    "        self.clip_model.eval()  # CLIP should be in eval mode\n",
    "\n",
    "        # Adapter layer to match output dimension of RNNEncoder\n",
    "        self.adapter = nn.Linear(512, self.nhidden * self.num_directions)\n",
    "\n",
    "    def forward(self, captions, cap_lens):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            captions: Tuple or list of file paths or tokenized captions\n",
    "            cap_lens: List of caption lengths\n",
    "        Returns:\n",
    "            sent_emb: Sentence embedding (batch_size, nhidden * num_directions)\n",
    "            words_emb: Word embeddings (batch_size, nhidden * num_directions, seq_len)\n",
    "        \"\"\"\n",
    "        batch_size = len(captions) if isinstance(captions, (list, tuple)) else captions.shape[0]\n",
    "        \n",
    "        # Process file paths directly - this is the key fix\n",
    "        if isinstance(captions, tuple) or (isinstance(captions, list) and isinstance(captions[0], str)):\n",
    "            # These are file paths, so we'll use them directly as text input\n",
    "            caption_texts = []\n",
    "            for path in captions:\n",
    "                # Extract bird name from path (e.g., \"Blue_Jay\" from \"073.Blue_Jay/Blue_Jay_0048_62433\")\n",
    "                bird_name = path.split('/')[0].split('.')[-1].replace('_', ' ').lower()\n",
    "                caption_texts.append(f\"a photo of a {bird_name}\")\n",
    "        else:\n",
    "            # Handle token IDs if that's what we're getting\n",
    "            caption_texts = []\n",
    "            for i, length in enumerate(cap_lens):\n",
    "                if isinstance(captions, torch.Tensor):\n",
    "                    tokens = captions[i, :length].cpu().tolist()\n",
    "                    text = \" \".join([str(token) for token in tokens])\n",
    "                else:\n",
    "                    tokens = captions[i][:length]\n",
    "                    text = \" \".join([str(token) for token in tokens])\n",
    "                caption_texts.append(text)\n",
    "        \n",
    "        tokens = clip.tokenize(caption_texts).to(next(self.clip_model.parameters()).device)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        # Get CLIP text embeddings\n",
    "        with torch.no_grad():\n",
    "            text_features = self.clip_model.encode_text(tokens)  # (batch, 512)\n",
    "        \n",
    "        # Project to match RNNEncoder output dimension\n",
    "        sent_emb = self.adapter(text_features)  # (batch, nhidden * num_directions)\n",
    "        \n",
    "        # Simulate word embeddings by repeating sentence embeddings along sequence dimension\n",
    "        words_emb = sent_emb.unsqueeze(2).repeat(1, 1, self.n_steps)  # (batch, hidden, seq_len)\n",
    "        \n",
    "        return sent_emb, words_emb\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def load(weights_path: str, ntoken: int) -> 'CLIPTextEncoder':\n",
    "        model = CLIPTextEncoder(ntoken, nhidden=256)\n",
    "        state_dict = torch.load(weights_path, map_location=\"cpu\")\n",
    "        model.load_state_dict(state_dict)\n",
    "        return model\n",
    "\n",
    "# Create a dataset class for training the CLIP adapter\n",
    "class CLIPAdapterDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_loader, rnn_encoder):\n",
    "        self.samples = []\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Pre-compute RNN embeddings for training data\n",
    "        rnn_encoder.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(data_loader, desc=\"Creating CLIP adapter dataset\"):\n",
    "                images, captions, captions_len, file_names = prepare_data(batch, self.device)\n",
    "                \n",
    "                # Get RNN embeddings as target\n",
    "                sent_emb, words_emb = rnn_encoder(captions, captions_len)\n",
    "                \n",
    "                # Store the original file names and their RNN embeddings\n",
    "                for i in range(len(file_names)):\n",
    "                    self.samples.append((\n",
    "                        file_names[i],  # Store filename as caption source\n",
    "                        captions_len[i].item(),\n",
    "                        sent_emb[i].cpu(),\n",
    "                        words_emb[i].cpu()\n",
    "                    ))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "\n",
    "# Train and Save the models\n",
    "# def train_clip_adapter(model, dataset, epochs=300, lr=1e-4):\n",
    "#     optimizer = torch.optim.Adam(model.adapter.parameters(), lr=lr)\n",
    "#     criterion = nn.MSELoss()  # Match with precomputed RNN embeddings\n",
    "#     model.train()\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model = model.to(device)\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         epoch_loss = 0\n",
    "#         batch_count = 0\n",
    "        \n",
    "#         for captions, cap_lens, target_sent_emb, target_words_emb in tqdm(dataset):\n",
    "#             # Move tensors to device\n",
    "#             if isinstance(target_sent_emb, torch.Tensor):\n",
    "#                 target_sent_emb = target_sent_emb.to(device)\n",
    "#             if isinstance(target_words_emb, torch.Tensor):\n",
    "#                 target_words_emb = target_words_emb.to(device)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             sent_emb, words_emb = model(captions, cap_lens)\n",
    "            \n",
    "#             # Ensure dimensions match\n",
    "#             if sent_emb.shape != target_sent_emb.shape:\n",
    "#                 print(f\"Shape mismatch: sent_emb {sent_emb.shape}, target_sent_emb {target_sent_emb.shape}\")\n",
    "#                 continue\n",
    "                \n",
    "#             if words_emb.shape != target_words_emb.shape:\n",
    "#                 print(f\"Shape mismatch: words_emb {words_emb.shape}, target_words_emb {target_words_emb.shape}\")\n",
    "#                 continue\n",
    "            \n",
    "#             loss = criterion(sent_emb, target_sent_emb) + criterion(words_emb, target_words_emb)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             epoch_loss += loss.item()\n",
    "#             batch_count += 1\n",
    "            \n",
    "#             # Print batch loss occasionally for debugging\n",
    "#             if batch_count % 10 == 0:\n",
    "#                 print(f\"Batch {batch_count}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "#         avg_epoch_loss = epoch_loss / max(batch_count, 1)  # Avoid division by zero\n",
    "#         print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_epoch_loss:.4f}, Batches processed: {batch_count}\")\n",
    "\n",
    "#     torch.save(model.state_dict(), \"/kaggle/working/clip_text_encoder.pth\")\n",
    "#     print(\"Model saved as /kaggle/working/clip_text_encoder.pth\")\n",
    "\n",
    "\n",
    "data_path = \"/kaggle/input/projec/cv_seattention/cv_seattention/data\"\n",
    "train_loader, n_words = create_loader(256, 32, data_path, \"train\")\n",
    "\n",
    "# Load the RNN encoder to get target embeddings\n",
    "rnn_encoder = RNNEncoder.load(\"/kaggle/input/projec/cv_seattention/cv_seattention/text_encoder_weights/text_encoder200.pth\", n_words)\n",
    "rnn_encoder.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Create the adapter training dataset\n",
    "clip_adapter_dataset = CLIPAdapterDataset(train_loader, rnn_encoder)\n",
    "clip_adapter_loader = DataLoader(clip_adapter_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize CLIP Encoder\n",
    "clip_encoder = CLIPTextEncoder(ntoken=n_words, nhidden=256)\n",
    "\n",
    "# Train adapter\n",
    "# train_clip_adapter(clip_encoder, clip_adapter_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T04:53:19.586128Z",
     "iopub.status.busy": "2025-04-30T04:53:19.585774Z",
     "iopub.status.idle": "2025-04-30T04:53:19.605376Z",
     "shell.execute_reply": "2025-04-30T04:53:19.604675Z",
     "shell.execute_reply.started": "2025-04-30T04:53:19.586104Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "def conv1x1(in_planes, out_planes):\n",
    "    \"\"\"1x1 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1,\n",
    "                   padding=0, bias=False)\n",
    "\n",
    "class GlobalAttentionGeneral(nn.Module):\n",
    "    def __init__(self, idf, cdf):\n",
    "        super().__init__()\n",
    "        self.conv_context = conv1x1(cdf, idf)\n",
    "        self.sm = nn.Softmax(dim=1)\n",
    "        self.mask = None\n",
    "\n",
    "    def applyMask(self, mask):\n",
    "        self.mask = mask  # batch x sourceL\n",
    "\n",
    "    def forward(self, input, context):\n",
    "        \"\"\"\n",
    "        input: batch x idf x ih x iw (queryL=ihxiw)\n",
    "        context: batch x cdf x sourceL\n",
    "        \"\"\"\n",
    "        ih, iw = input.size(2), input.size(3)\n",
    "        queryL = ih * iw\n",
    "        batch_size, sourceL = context.size(0), context.size(2)\n",
    "\n",
    "        # --> batch x queryL x idf\n",
    "        target = input.view(batch_size, -1, queryL)\n",
    "        targetT = torch.transpose(target, 1, 2).contiguous()\n",
    "        \n",
    "        # batch x cdf x sourceL --> batch x cdf x sourceL x 1\n",
    "        sourceT = context.unsqueeze(3)\n",
    "        # --> batch x idf x sourceL\n",
    "        sourceT = self.conv_context(sourceT).squeeze(3)\n",
    "\n",
    "        # Get attention\n",
    "        attn = torch.bmm(targetT, sourceT)  # batch x queryL x sourceL\n",
    "        attn = attn.view(batch_size * queryL, sourceL)\n",
    "        \n",
    "        if self.mask is not None:\n",
    "            mask = self.mask.repeat(queryL, 1)\n",
    "            attn.data.masked_fill_(mask.data, -float('inf'))\n",
    "        \n",
    "        attn = self.sm(attn)\n",
    "        attn = attn.view(batch_size, queryL, sourceL)\n",
    "        attn = torch.transpose(attn, 1, 2).contiguous()\n",
    "\n",
    "        # Apply attention\n",
    "        weightedContext = torch.bmm(sourceT, attn)\n",
    "        weightedContext = weightedContext.view(batch_size, -1, ih, iw)\n",
    "        attn = attn.view(batch_size, -1, ih, iw)\n",
    "\n",
    "        return weightedContext, attn\n",
    "\n",
    "class AffineBlock(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.gamma_mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        self.beta_mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_normal_(module.weight)\n",
    "                nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, x: Tensor, sentence_embed: Tensor) -> Tensor:\n",
    "        scale_param = self.gamma_mlp(sentence_embed).unsqueeze(-1).unsqueeze(-1)\n",
    "        shift_param = self.beta_mlp(sentence_embed).unsqueeze(-1).unsqueeze(-1)\n",
    "        return scale_param * x + shift_param\n",
    "\n",
    "class ResidualBlockG(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, text_dim: int = 256):\n",
    "        super().__init__()\n",
    "        hidden_dim = text_dim // 2\n",
    "        \n",
    "        self.affine1 = AffineBlock(text_dim, hidden_dim, in_channels)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.affine2 = AffineBlock(text_dim, hidden_dim, out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.skip = nn.Conv2d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else nn.Identity()\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x: Tensor, sentence_embed: Tensor) -> Tensor:\n",
    "        residual = self.skip(x)\n",
    "        x = F.leaky_relu(self.affine1(x, sentence_embed), 0.2)\n",
    "        x = self.conv1(x)\n",
    "        x = F.leaky_relu(self.affine2(x, sentence_embed), 0.2)\n",
    "        x = self.conv2(x)\n",
    "        return residual + self.gamma * x\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, n_channels: int = 32, latent_dim: int = 100):\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        \n",
    "        # Initial projection (4x4 spatial size)\n",
    "        self.linear_in = nn.Linear(latent_dim, 8 * n_channels * 4 * 4)\n",
    "        \n",
    "        # Residual blocks with progressive upsampling\n",
    "        self.res_blocks = nn.ModuleList([\n",
    "            ResidualBlockG(8 * n_channels, 8 * n_channels),\n",
    "            ResidualBlockG(8 * n_channels, 4 * n_channels),\n",
    "            ResidualBlockG(4 * n_channels, 2 * n_channels),\n",
    "            ResidualBlockG(2 * n_channels, n_channels),\n",
    "        ])\n",
    "        \n",
    "        # Attention mechanism at 64x64 resolution\n",
    "        self.att = GlobalAttentionGeneral(n_channels, 256)\n",
    "        self.att_conv = nn.Conv2d(2 * n_channels, n_channels, kernel_size=1)\n",
    "        \n",
    "        # Final upsampling to 256x256\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),  # 64->128\n",
    "            nn.Conv2d(n_channels, n_channels//2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n_channels//2),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            \n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),  # 128->256\n",
    "            nn.Conv2d(n_channels//2, n_channels//4, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n_channels//4),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "        \n",
    "        # Output layers\n",
    "        self.conv_out = nn.Sequential(\n",
    "            nn.Conv2d(n_channels//4, 3, kernel_size=3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise: Tensor, sentence_embed: Tensor, word_embed: Tensor) -> Tensor:\n",
    "        # Initial projection\n",
    "        x = self.linear_in(noise).view(-1, 8 * self.n_channels, 4, 4)\n",
    "        \n",
    "        # Process through residual blocks with 2x upsampling each\n",
    "        for block in self.res_blocks:\n",
    "            x = block(x, sentence_embed)\n",
    "            x = F.interpolate(x, scale_factor=2, mode='nearest')  # 4->8->16->32->64\n",
    "        \n",
    "        # Apply attention at 64x64 resolution\n",
    "        attn_out, _ = self.att(x, word_embed)\n",
    "        x = torch.cat([x, attn_out], dim=1)\n",
    "        x = self.att_conv(x)\n",
    "        \n",
    "        # Final upsampling to 256x256\n",
    "        x = self.upsample(x)\n",
    "        return self.conv_out(x)\n",
    "        \n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T04:53:23.945955Z",
     "iopub.status.busy": "2025-04-30T04:53:23.945246Z",
     "iopub.status.idle": "2025-04-30T04:53:24.656759Z",
     "shell.execute_reply": "2025-04-30T04:53:24.655952Z",
     "shell.execute_reply.started": "2025-04-30T04:53:23.945926Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Generator                                [24, 3, 256, 256]         --\n",
       "├─Linear: 1-1                            [24, 4096]                413,696\n",
       "├─ModuleList: 1-2                        --                        --\n",
       "│    └─ResidualBlockG: 2-1               [24, 256, 4, 4]           1\n",
       "│    │    └─Identity: 3-1                [24, 256, 4, 4]           --\n",
       "│    │    └─AffineBlock: 3-2             [24, 256, 4, 4]           131,840\n",
       "│    │    └─Conv2d: 3-3                  [24, 256, 4, 4]           590,080\n",
       "│    │    └─AffineBlock: 3-4             [24, 256, 4, 4]           131,840\n",
       "│    │    └─Conv2d: 3-5                  [24, 256, 4, 4]           590,080\n",
       "│    └─ResidualBlockG: 2-2               [24, 128, 8, 8]           1\n",
       "│    │    └─Conv2d: 3-6                  [24, 128, 8, 8]           32,896\n",
       "│    │    └─AffineBlock: 3-7             [24, 256, 8, 8]           131,840\n",
       "│    │    └─Conv2d: 3-8                  [24, 128, 8, 8]           295,040\n",
       "│    │    └─AffineBlock: 3-9             [24, 128, 8, 8]           98,816\n",
       "│    │    └─Conv2d: 3-10                 [24, 128, 8, 8]           147,584\n",
       "│    └─ResidualBlockG: 2-3               [24, 64, 16, 16]          1\n",
       "│    │    └─Conv2d: 3-11                 [24, 64, 16, 16]          8,256\n",
       "│    │    └─AffineBlock: 3-12            [24, 128, 16, 16]         98,816\n",
       "│    │    └─Conv2d: 3-13                 [24, 64, 16, 16]          73,792\n",
       "│    │    └─AffineBlock: 3-14            [24, 64, 16, 16]          82,304\n",
       "│    │    └─Conv2d: 3-15                 [24, 64, 16, 16]          36,928\n",
       "│    └─ResidualBlockG: 2-4               [24, 32, 32, 32]          1\n",
       "│    │    └─Conv2d: 3-16                 [24, 32, 32, 32]          2,080\n",
       "│    │    └─AffineBlock: 3-17            [24, 64, 32, 32]          82,304\n",
       "│    │    └─Conv2d: 3-18                 [24, 32, 32, 32]          18,464\n",
       "│    │    └─AffineBlock: 3-19            [24, 32, 32, 32]          74,048\n",
       "│    │    └─Conv2d: 3-20                 [24, 32, 32, 32]          9,248\n",
       "├─GlobalAttentionGeneral: 1-3            [24, 32, 64, 64]          --\n",
       "│    └─Conv2d: 2-5                       [24, 32, 18, 1]           8,192\n",
       "│    └─Softmax: 2-6                      [98304, 18]               --\n",
       "├─Conv2d: 1-4                            [24, 32, 64, 64]          2,080\n",
       "├─Sequential: 1-5                        [24, 8, 256, 256]         --\n",
       "│    └─Upsample: 2-7                     [24, 32, 128, 128]        --\n",
       "│    └─Conv2d: 2-8                       [24, 16, 128, 128]        4,624\n",
       "│    └─BatchNorm2d: 2-9                  [24, 16, 128, 128]        32\n",
       "│    └─LeakyReLU: 2-10                   [24, 16, 128, 128]        --\n",
       "│    └─Upsample: 2-11                    [24, 16, 256, 256]        --\n",
       "│    └─Conv2d: 2-12                      [24, 8, 256, 256]         1,160\n",
       "│    └─BatchNorm2d: 2-13                 [24, 8, 256, 256]         16\n",
       "│    └─LeakyReLU: 2-14                   [24, 8, 256, 256]         --\n",
       "├─Sequential: 1-6                        [24, 3, 256, 256]         --\n",
       "│    └─Conv2d: 2-15                      [24, 3, 256, 256]         219\n",
       "│    └─Tanh: 2-16                        [24, 3, 256, 256]         --\n",
       "==========================================================================================\n",
       "Total params: 3,066,279\n",
       "Trainable params: 3,066,279\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 6.87\n",
       "==========================================================================================\n",
       "Input size (MB): 0.48\n",
       "Forward/backward pass size (MB): 401.25\n",
       "Params size (MB): 12.27\n",
       "Estimated Total Size (MB): 413.99\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchinfo\n",
    "\n",
    "gen = Generator(n_channels=32, latent_dim=100)\n",
    "\n",
    "noise = torch.rand((24, 100))\n",
    "sent = torch.rand((24, 256))\n",
    "word = torch.rand((24, 256, 18))\n",
    "\n",
    "torchinfo.summary(gen, input_data=(noise, sent, word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T04:53:28.041045Z",
     "iopub.status.busy": "2025-04-30T04:53:28.040303Z",
     "iopub.status.idle": "2025-04-30T04:53:28.050719Z",
     "shell.execute_reply": "2025-04-30T04:53:28.050164Z",
     "shell.execute_reply.started": "2025-04-30T04:53:28.040967Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "class ResidualBlockD(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Main convolution path\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        # Shortcut path (channel and spatial adjustment)\n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False) \n",
    "            if in_channels != out_channels else nn.Identity(),\n",
    "            nn.AvgPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.shortcut(x) + self.gamma * self.conv(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, n_c: int = 32, sentence_embed_dim: int = 256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Image processing pathway\n",
    "        self.img_encoder = nn.Sequential(\n",
    "            # Initial convolution (no residual)\n",
    "            nn.Conv2d(3, n_c, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "            # Residual downsampling blocks\n",
    "            ResidualBlockD(n_c * 1, n_c * 2),  # 256x256 -> 128x128\n",
    "            ResidualBlockD(n_c * 2, n_c * 4),  # 128x128 -> 64x64\n",
    "            ResidualBlockD(n_c * 4, n_c * 8),  # 64x64 -> 32x32\n",
    "            ResidualBlockD(n_c * 8, n_c * 16), # 32x32 -> 16x16\n",
    "            \n",
    "            # Final downsampling\n",
    "            nn.Conv2d(n_c*16, n_c*16, 4, 2, 1),  # 16x16 -> 8x8\n",
    "            nn.Conv2d(n_c*16, n_c*16, 4, 2, 1)    # 8x8 -> 4x4\n",
    "        )\n",
    "\n",
    "        # Text-image fusion\n",
    "        self.judge_net = nn.Sequential(\n",
    "            nn.Conv2d(n_c*16 + sentence_embed_dim, n_c*2, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(n_c*2, 1, 4, 1, 0)  # Final 1x1 output\n",
    "        )\n",
    "\n",
    "    def build_embeds(self, image: Tensor) -> Tensor:\n",
    "        \"\"\"Extract image features (same as original)\"\"\"\n",
    "        return self.img_encoder(image)\n",
    "\n",
    "    def get_logits(self, image_embed: Tensor, sentence_embed: Tensor) -> Tensor:\n",
    "        \"\"\"Fuse image and text features (same interface)\"\"\"\n",
    "        # Expand text to spatial dimensions\n",
    "        sentence_embed = sentence_embed.view(-1, 256, 1, 1).expand(-1, -1, 4, 4)\n",
    "        \n",
    "        # Concatenate and classify\n",
    "        combined = torch.cat((image_embed, sentence_embed), dim=1)\n",
    "        return self.judge_net(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T04:53:38.706773Z",
     "iopub.status.busy": "2025-04-30T04:53:38.706492Z",
     "iopub.status.idle": "2025-04-30T04:53:38.725742Z",
     "shell.execute_reply": "2025-04-30T04:53:38.724906Z",
     "shell.execute_reply.started": "2025-04-30T04:53:38.706751Z"
    }
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "from typing import Tuple, List\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.utils as vutils\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import trange\n",
    "\n",
    "#from discriminator.model import Discriminator\n",
    "#from generator.model import Generator\n",
    "#from objects.utils import prepare_data\n",
    "#from text_encoder.model import RNNEncoder\n",
    "\n",
    "class DeepFusionGAN:\n",
    "    def __init__(self, n_words, encoder_weights_path: str, image_save_path: str, gen_path_save: str):\n",
    "        super().__init__()\n",
    "        self.image_save_path = image_save_path\n",
    "        self.gen_path_save = gen_path_save\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.generator = Generator(n_channels=32, latent_dim=100).to(self.device)\n",
    "        self.discriminator = Discriminator(n_c=32).to(self.device)\n",
    "\n",
    "        # self.text_encoder = RNNEncoder.load(encoder_weights_path, n_words)\n",
    "        self.text_encoder = CLIPTextEncoder.load(encoder_weights_path, n_words)\n",
    "\n",
    "        self.text_encoder.to(self.device)\n",
    "\n",
    "        for p in self.text_encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.text_encoder.eval()\n",
    "\n",
    "        # self.g_optim = torch.optim.Adam(self.generator.parameters(), lr=0.0001, betas=(0.0, 0.9))\n",
    "        # self.d_optim = torch.optim.Adam(self.discriminator.parameters(), lr=0.0004, betas=(0.0, 0.9))\n",
    "        self.g_optim = torch.optim.Adam(self.generator.parameters(), lr=0.0002,betas=(0.5, 0.99))\n",
    "        self.d_optim = torch.optim.Adam(self.discriminator.parameters(), lr=0.0008, betas=(0.5, 0.99))\n",
    "\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def _zero_grad(self):\n",
    "        self.d_optim.zero_grad()\n",
    "        self.g_optim.zero_grad()\n",
    "\n",
    "    def _compute_gp(self, images: Tensor, sentence_embeds: Tensor) -> Tensor:\n",
    "        batch_size = images.shape[0]\n",
    "\n",
    "        images_interpolated = images.data.requires_grad_()\n",
    "        sentences_interpolated = sentence_embeds.data.requires_grad_()\n",
    "\n",
    "        embeds = self.discriminator.build_embeds(images_interpolated)\n",
    "        logits = self.discriminator.get_logits(embeds, sentences_interpolated)\n",
    "\n",
    "        grad_outputs = torch.ones_like(logits)\n",
    "        grads = torch.autograd.grad(\n",
    "            outputs=logits,\n",
    "            inputs=(images_interpolated, sentences_interpolated),\n",
    "            grad_outputs=grad_outputs,\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )\n",
    "\n",
    "        grad_0 = grads[0].reshape(batch_size, -1)\n",
    "        grad_1 = grads[1].reshape(batch_size, -1)\n",
    "\n",
    "        grad = torch.cat((grad_0, grad_1), dim=1)\n",
    "        grad_norm = grad.norm(2, 1)\n",
    "\n",
    "        return grad_norm\n",
    "\n",
    "    def fit(self, train_loader: DataLoader, num_epochs: int = 500,checkpoint_path: str = None) -> Tuple[List[float], List[float], List[float]]:\n",
    "        g_losses_epoch, d_losses_epoch, d_gp_losses_epoch = [], [], []\n",
    "\n",
    "        start_epoch = 0\n",
    "        if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "            start_epoch = self._load_gen_weights(checkpoint_path)\n",
    "            print(f\"Resuming from epoch {start_epoch}\")\n",
    "\n",
    "        path=\"/kaggle/input/projec/gennormal_338.pth\"\n",
    "        start_epoch=self._load_gen_weights(path)\n",
    "            \n",
    "        for epoch in trange(start_epoch,num_epochs+start_epoch, desc=\"Train Deep Fusion GAN\"):\n",
    "\n",
    "            g_losses, d_losses, d_gp_losses = [], [], []\n",
    "            for batch in train_loader:\n",
    "                images, captions, captions_len, _ = prepare_data(batch, self.device)\n",
    "                batch_size = images.shape[0]\n",
    "\n",
    "\n",
    "                sentence_embeds, words_embs = self.text_encoder(captions, captions_len)\n",
    "                sentence_embeds, words_embs = sentence_embeds.detach(), words_embs.detach()\n",
    "                #sentence_embeds, word_embeds = self.text_encoder(captions, captions_len).detach()\n",
    "                #print(\"images\",images.shape)\n",
    "                real_embeds = self.discriminator.build_embeds(images)\n",
    "                #print(\"real_embeds\",real_embeds.shape)\n",
    "                #print(\"sent\",sentence_embeds.shape)\n",
    "                real_logits = self.discriminator.get_logits(real_embeds, sentence_embeds)\n",
    "                #print(\"real_\",real_logits.shape)\n",
    "                d_loss_real = self.relu(1.0 - real_logits).mean()\n",
    "\n",
    "                shift_embeds = real_embeds[:(batch_size - 1)]\n",
    "                shift_sentence_embeds = sentence_embeds[1:batch_size]\n",
    "                shift_real_image_embeds = self.discriminator.get_logits(shift_embeds, shift_sentence_embeds)\n",
    "\n",
    "                d_loss_mismatch = self.relu(1.0 + shift_real_image_embeds).mean()\n",
    "\n",
    "                noise = torch.randn(batch_size, 100).to(self.device)\n",
    "\n",
    "                #print(\"noise:\", noise.shape)\n",
    "                #print(\"sent \",sentence_embeds.shape)\n",
    "#                print(\"word\",words_embs.shape)\n",
    "                \n",
    "                fake_images = self.generator(noise, sentence_embeds, words_embs)\n",
    "               # print(\"fake\",fake_images.shape)\n",
    "               # if 6==6:\n",
    "               #     return [],[],[]\n",
    "                \n",
    "\n",
    "                fake_embeds = self.discriminator.build_embeds(fake_images.detach())\n",
    "                fake_logits = self.discriminator.get_logits(fake_embeds, sentence_embeds)\n",
    "\n",
    "                d_loss_fake = self.relu(1.0 + fake_logits).mean()\n",
    "\n",
    "                d_loss = d_loss_real + (d_loss_fake + d_loss_mismatch) / 2.0\n",
    "\n",
    "                self._zero_grad()\n",
    "                d_loss.backward()\n",
    "                self.d_optim.step()\n",
    "\n",
    "                d_losses.append(d_loss.item())\n",
    "\n",
    "                grad_l2norm = self._compute_gp(images, sentence_embeds)\n",
    "                d_loss_gp = 2.0 * torch.mean(grad_l2norm ** 6)\n",
    "\n",
    "                self._zero_grad()\n",
    "                d_loss_gp.backward()\n",
    "                self.d_optim.step()\n",
    "\n",
    "                d_gp_losses.append(d_loss_gp.item())\n",
    "\n",
    "                fake_embeds = self.discriminator.build_embeds(fake_images)\n",
    "                fake_logits = self.discriminator.get_logits(fake_embeds, sentence_embeds)\n",
    "                g_loss = -fake_logits.mean()\n",
    "\n",
    "                self._zero_grad()\n",
    "                g_loss.backward()\n",
    "                self.g_optim.step()\n",
    "\n",
    "                g_losses.append(g_loss.item())\n",
    "\n",
    "\n",
    "            g_losses_epoch.append(np.mean(g_losses))\n",
    "            d_losses_epoch.append(np.mean(d_losses))\n",
    "            d_gp_losses_epoch.append(np.mean(d_gp_losses))\n",
    "\n",
    "\n",
    "            if epoch % 1 == 0:\n",
    "             self._save_fake_image(fake_images, epoch)\n",
    "             self._save_gen_weights(epoch)\n",
    "\n",
    "\n",
    "            #print('g_losses_epoch', g_losses_epoch)\n",
    "            #print('d_losses_epoch', d_losses_epoch)\n",
    "            #print('gp_losses_epoch', d_gp_losses_epoch)\n",
    "\n",
    "\n",
    "\n",
    "        return g_losses_epoch, d_losses_epoch, d_gp_losses_epoch\n",
    "\n",
    "    def _save_fake_image(self, fake_images: Tensor, epoch: int):\n",
    "        img_path = os.path.join(self.image_save_path, f\"fake_samplenormal_epoch_{epoch}.png\")\n",
    "        vutils.save_image(fake_images.data, img_path, normalize=True)\n",
    "\n",
    "\n",
    "    def _save_gen_weights(self, epoch: int):\n",
    "        gen_path = os.path.join(self.gen_path_save, f\"gennormal_{epoch}.pth\")\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'generator_state_dict': self.generator.state_dict(),\n",
    "            'discriminator_state_dict': self.discriminator.state_dict(),\n",
    "            'g_optim_state_dict': self.g_optim.state_dict(),\n",
    "            'd_optim_state_dict': self.d_optim.state_dict(),\n",
    "        }\n",
    "        torch.save(checkpoint, gen_path)\n",
    "        print(f\"Model checkpoint saved at epoch {epoch} to {gen_path}\")\n",
    "\n",
    "    def _load_gen_weights(self, checkpoint_path: str) -> int:\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        self.generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "        self.discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "        self.g_optim.load_state_dict(checkpoint['g_optim_state_dict'])\n",
    "        self.d_optim.load_state_dict(checkpoint['d_optim_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        print(f\"Checkpoint loaded from {checkpoint_path} - Starting from epoch {start_epoch}\")\n",
    "        return start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T04:53:45.027689Z",
     "iopub.status.busy": "2025-04-30T04:53:45.027102Z",
     "iopub.status.idle": "2025-04-30T04:53:45.034303Z",
     "shell.execute_reply": "2025-04-30T04:53:45.033563Z",
     "shell.execute_reply.started": "2025-04-30T04:53:45.027665Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Tuple\n",
    "\n",
    "#from model import DeepFusionGAN\n",
    "#from utils import create_loader, fix_seed\n",
    "def train() -> Tuple[List[float], List[float], List[float]]:\n",
    "    fix_seed()\n",
    "    \n",
    "    data_path = \"/kaggle/input/projec/cv_seattention/cv_seattention/data\"\n",
    "    encoder_weights_path = \"/kaggle/input/projec/clip_text_encoder.pth\"  # Path to your trained CLIP encoder\n",
    "    image_save_path = \"/kaggle/working/gen_images\"\n",
    "    gen_path_save = \"/kaggle/working/gen_weights\"\n",
    "\n",
    "    os.makedirs(image_save_path, exist_ok=True)\n",
    "    os.makedirs(gen_path_save, exist_ok=True)\n",
    "\n",
    "    train_loader, n_words = create_loader(256, 32, data_path, \"train\")\n",
    "    model = DeepFusionGAN(n_words=n_words,\n",
    "                         encoder_weights_path=encoder_weights_path,\n",
    "                         image_save_path=image_save_path,\n",
    "                         gen_path_save=gen_path_save)\n",
    "    \n",
    "    checkpoint_path = None\n",
    "    checkpoints = sorted(\n",
    "        [f for f in os.listdir(gen_path_save) if f.startswith(\"gennormal_\") and f.endswith(\".pth\")],\n",
    "        key=lambda x: int(x.split('_')[-1].split('.')[0])  # Extract the epoch number from the filename\n",
    "    )\n",
    "\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = checkpoints[-1]  # Get the latest one\n",
    "        checkpoint_path = os.path.join(gen_path_save, latest_checkpoint)\n",
    "        print(f\"Resuming from checkpoint: {checkpoint_path}\")\n",
    "\n",
    "    num_epochs= 50\n",
    "    #print(len(train_loader))\n",
    "    #i=0\n",
    "    #for batch in train_loader:\n",
    "    #    images, captions, captions_len, _ = prepare_data(batch,device)\n",
    "    #    batch_size = images.shape[0]\n",
    "    #    i=i+batch_size\n",
    "    #print(i)\n",
    "\n",
    "    #if 5==5:\n",
    "    #    return [],[],[]\n",
    "    return model.fit(train_loader,num_epochs)\n",
    "\n",
    "\n",
    "#if __name__ == '__main__':\n",
    " #   train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T04:53:47.691563Z",
     "iopub.status.busy": "2025-04-30T04:53:47.690787Z",
     "iopub.status.idle": "2025-04-30T04:53:47.695659Z",
     "shell.execute_reply": "2025-04-30T04:53:47.694751Z",
     "shell.execute_reply.started": "2025-04-30T04:53:47.691538Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T01:39:02.567914Z",
     "iopub.status.busy": "2025-04-29T01:39:02.567636Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 123321 fixed\n",
      "Total filenames: 11788\n",
      "Load captions from: /kaggle/input/projec/cv_seattention/cv_seattention/data/captions.pickle\n",
      "Load file names from: /kaggle/input/projec/cv_seattention/cv_seattention/data/train/filenames.pickle (8855)\n",
      "Dictionary size: 5450\n",
      "Embeddings number: 10\n",
      "6000\n",
      "Checkpoint loaded from /kaggle/input/projec/gennormal_338.pth - Starting from epoch 339\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b30e50f07d04801948eff83a992a8f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Deep Fusion GAN:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved at epoch 339 to /kaggle/working/gen_weights/gennormal_339.pth\n",
      "Model checkpoint saved at epoch 340 to /kaggle/working/gen_weights/gennormal_340.pth\n",
      "Model checkpoint saved at epoch 341 to /kaggle/working/gen_weights/gennormal_341.pth\n",
      "Model checkpoint saved at epoch 342 to /kaggle/working/gen_weights/gennormal_342.pth\n",
      "Model checkpoint saved at epoch 343 to /kaggle/working/gen_weights/gennormal_343.pth\n",
      "Model checkpoint saved at epoch 344 to /kaggle/working/gen_weights/gennormal_344.pth\n",
      "Model checkpoint saved at epoch 345 to /kaggle/working/gen_weights/gennormal_345.pth\n",
      "Model checkpoint saved at epoch 346 to /kaggle/working/gen_weights/gennormal_346.pth\n",
      "Model checkpoint saved at epoch 347 to /kaggle/working/gen_weights/gennormal_347.pth\n",
      "Model checkpoint saved at epoch 348 to /kaggle/working/gen_weights/gennormal_348.pth\n",
      "Model checkpoint saved at epoch 349 to /kaggle/working/gen_weights/gennormal_349.pth\n",
      "Model checkpoint saved at epoch 350 to /kaggle/working/gen_weights/gennormal_350.pth\n",
      "Model checkpoint saved at epoch 351 to /kaggle/working/gen_weights/gennormal_351.pth\n",
      "Model checkpoint saved at epoch 352 to /kaggle/working/gen_weights/gennormal_352.pth\n",
      "Model checkpoint saved at epoch 353 to /kaggle/working/gen_weights/gennormal_353.pth\n",
      "Model checkpoint saved at epoch 354 to /kaggle/working/gen_weights/gennormal_354.pth\n",
      "Model checkpoint saved at epoch 355 to /kaggle/working/gen_weights/gennormal_355.pth\n",
      "Model checkpoint saved at epoch 356 to /kaggle/working/gen_weights/gennormal_356.pth\n",
      "Model checkpoint saved at epoch 357 to /kaggle/working/gen_weights/gennormal_357.pth\n",
      "Model checkpoint saved at epoch 358 to /kaggle/working/gen_weights/gennormal_358.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "current_cwd = os.getcwd()\n",
    "src_path = '/'.join(current_cwd.split('/')[:-1])\n",
    "sys.path.append(src_path)\n",
    "#from train import train\n",
    "#from utils import plot_losses\n",
    "\n",
    "g_losses_epoch, d_losses_epoch, d_gp_losses_epoch = train()\n",
    "\n",
    "path = \"/kaggle/working/loss\"\n",
    "\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "filenames = [f'{path}/loss1.csv', f'{path}/loss2.csv', f'{path}/loss3.csv']\n",
    "\n",
    "loss_values = [g_losses_epoch, d_losses_epoch, d_gp_losses_epoch]\n",
    "\n",
    "for i in range(len(loss_values)):\n",
    "    filename = filenames[i]\n",
    "    with open(filename, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Loss'])\n",
    "        for loss in loss_values[i]:  \n",
    "            writer.writerow([loss])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T04:54:13.742959Z",
     "iopub.status.busy": "2025-04-30T04:54:13.742679Z",
     "iopub.status.idle": "2025-04-30T04:54:13.750023Z",
     "shell.execute_reply": "2025-04-30T04:54:13.749281Z",
     "shell.execute_reply.started": "2025-04-30T04:54:13.742937Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_images(generator: Generator, sentence_embeds: Tensor,\n",
    "                    device: torch.device) -> Tensor:\n",
    "    batch_size = sentence_embeds.shape[0]\n",
    "    noise = torch.randn(batch_size, 100).to(device)\n",
    "    return generator(noise, sentence_embeds)\n",
    "\n",
    "\n",
    "def save_image(image: np.ndarray, save_dir: str, file_name: str):\n",
    "    # [-1, 1] --> [0, 255]\n",
    "    image = (image + 1.0) * 127.5\n",
    "    image = image.astype(np.uint8)\n",
    "    image = np.transpose(image, (1, 2, 0))\n",
    "    image = Image.fromarray(image)\n",
    "    fullpath = os.path.join(save_dir, f\"{file_name.replace('/', '_')}.png\")\n",
    "    image.save(fullpath)\n",
    "\n",
    "\n",
    "def sample(generator: Generator, text_encoder: RNNEncoder, batch, save_dir: str):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    images, captions, captions_len, file_names = prepare_data(batch, device)\n",
    "    sent_emb = text_encoder(captions, captions_len).detach()\n",
    "\n",
    "    fake_images = generate_images(generator, sent_emb, device)\n",
    "\n",
    "    for i in range(images.shape[0]):\n",
    "        im = fake_images[i].data.cpu().numpy()\n",
    "        save_image(im, save_dir, file_names[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T04:54:17.636922Z",
     "iopub.status.busy": "2025-04-30T04:54:17.636648Z",
     "iopub.status.idle": "2025-04-30T04:54:26.934595Z",
     "shell.execute_reply": "2025-04-30T04:54:26.933852Z",
     "shell.execute_reply.started": "2025-04-30T04:54:17.636901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Checkpoint not found at /kaggle/working/gen_weights/gennormal_299.pth\n",
      "Total filenames: 11788\n",
      "Load captions from: /kaggle/input/projec/cv_seattention/cv_seattention/data/captions.pickle\n",
      "Load file names from: /kaggle/input/projec/cv_seattention/cv_seattention/data/test/filenames.pickle (2933)\n",
      "Dictionary size: 5450\n",
      "Embeddings number: 10\n",
      "2933\n",
      "Time taken: 0.34 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Ensure correct path handling\n",
    "current_cwd = os.getcwd()\n",
    "src_path = '/'.join(current_cwd.split('/')[:-1])\n",
    "sys.path.append(src_path)\n",
    "\n",
    "  # Assuming save_image is a defined function\n",
    "\n",
    "# Initialize generator\n",
    "generator = Generator(n_channels=32, latent_dim=100).to(device)\n",
    "\n",
    "# Load generator checkpoint - update with your actual path\n",
    "checkpoint_path = \"/kaggle/working/gen_weights/gennormal_299.pth\"  # Use your local path\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "    generator.eval()  # Set to evaluation mode\n",
    "else:\n",
    "    print(f\"Warning: Checkpoint not found at {checkpoint_path}\")\n",
    "\n",
    "# Create data loader with your actual data path\n",
    "train_loader, n_words = create_loader(256, 24, \"/kaggle/input/projec/cv_seattention/cv_seattention/data\", \"test\")\n",
    "\n",
    "# Load text encoder with your actual encoder path\n",
    "text_encoder = CLIPTextEncoder.load(\"/kaggle/input/projec/clip_text_encoder.pth\", n_words)\n",
    "text_encoder.to(device)\n",
    "\n",
    "\n",
    "# Freeze text encoder parameters\n",
    "for p in text_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "text_encoder.eval()\n",
    "\n",
    "# Create output directory\n",
    "path = \"/kaggle/working\"\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Function to generate an image from text\n",
    "def gen_own_bird(word_caption, name, i):\n",
    "    dataset = train_loader.dataset\n",
    "    \n",
    "    # For CLIPTextEncoder, we can just pass the text directly\n",
    "    caption_text = word_caption.lower()\n",
    "    \n",
    "    # Tokenize with CLIP's tokenizer\n",
    "    tokens = clip.tokenize([caption_text]).to(device)\n",
    "    \n",
    "    # Get text features directly from CLIP\n",
    "    with torch.no_grad():\n",
    "        text_features = text_encoder.clip_model.encode_text(tokens)\n",
    "    \n",
    "    # Project to match expected dimensions\n",
    "    embed = text_encoder.adapter(text_features)\n",
    "    \n",
    "    # Create word embeddings by repeating sentence embedding\n",
    "    word = embed.unsqueeze(2).repeat(1, 1, text_encoder.n_steps)\n",
    "    \n",
    "    # Generate image\n",
    "    batch_size = embed.shape[0]\n",
    "    noise = torch.randn(batch_size, 100).to(device)\n",
    "    img = generator(noise, embed, word)\n",
    "    \n",
    "    # Save image\n",
    "    save_image(img[0].data.cpu().numpy(), path, name + str(i))\n",
    "\n",
    "# Run generation\n",
    "start_time = time.time()\n",
    "caption = \"A blue bird\"\n",
    "i = 1\n",
    "gen_own_bird(caption, caption, i)\n",
    "\n",
    "print(f\"Time taken: {time.time() - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T05:00:33.601750Z",
     "iopub.status.busy": "2025-04-30T05:00:33.601432Z",
     "iopub.status.idle": "2025-04-30T05:01:53.561354Z",
     "shell.execute_reply": "2025-04-30T05:01:53.560362Z",
     "shell.execute_reply.started": "2025-04-30T05:00:33.601727Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=2048, out_features=1000, bias=True)\n",
      "Total filenames: 11788\n",
      "Load captions from: /kaggle/input/projec/cv_seattention/cv_seattention/data/captions.pickle\n",
      "Load file names from: /kaggle/input/projec/cv_seattention/cv_seattention/data/test/filenames.pickle (2933)\n",
      "Dictionary size: 5450\n",
      "Embeddings number: 10\n",
      "2933\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0fd65920505445482a168b4e953f205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Build representations:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID value = 241.12779077598896\n",
      "inception score mean:  3.388645700601763\n",
      "inception score std:  0.36156838605361896\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "current_cwd = os.getcwd()\n",
    "src_path = '/'.join(current_cwd.split('/')[:-1])\n",
    "sys.path.append(src_path)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy import linalg\n",
    "from scipy.linalg import sqrtm\n",
    "from scipy.stats import entropy\n",
    "from torch.nn.functional import adaptive_avg_pool2d\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "#from sample import prepare_data, generate_images\n",
    "#from src.generator.model import Generator\n",
    "#from src.text_encoder.model import RNNEncoder\n",
    "#from utils import create_loader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def generate_images(generator: Generator, sentence_embeds: Tensor,word,\n",
    "                    device: torch.device) -> Tensor:\n",
    "    batch_size = sentence_embeds.shape[0]\n",
    "    noise = torch.randn(batch_size, 100).to(device)\n",
    "    return generator(noise, sentence_embeds,word)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class InceptionV3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.device = f'cuda:{0}' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = torch.hub.load('pytorch/vision:v0.6.0', 'inception_v3', pretrained=True).to(self.device)\n",
    "        print(self.model.fc)\n",
    "        self.linear = self.model.fc\n",
    "        self.model.fc, self.model.dropout = [nn.Sequential()] * 2\n",
    "      \n",
    "    @torch.no_grad()\n",
    "    def get_last_layer(self, x):\n",
    "        x = F.interpolate(x, size=300, mode='bilinear', align_corners=False, recompute_scale_factor=False)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "classifier = InceptionV3().to(device)\n",
    "classifier = classifier.eval()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "test_loader, n_words = create_loader(256, batch_size, \"/kaggle/input/projec/cv_seattention/cv_seattention/data\", \"test\")\n",
    "\n",
    "checkpoint = torch.load(\"/kaggle/input/projec/gennormal_220.pth\", map_location=device)\n",
    "#generator = Generator(n_channels=32, latent_dim=100).to(device)\n",
    "#generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "#generator.eval()\n",
    "\n",
    "\n",
    "\n",
    "generator = Generator(n_channels=32, latent_dim=100).to(device)\n",
    "generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "generator.eval()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "text_encoder = CLIPTextEncoder.load(\"/kaggle/input/projec/clip_text_encoder.pth\", n_words)\n",
    "text_encoder.to(device)\n",
    "\n",
    "for p in text_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "text_encoder = text_encoder.eval()\n",
    "\n",
    "\n",
    "\n",
    "def calculate_fid(repr1, repr2):\n",
    "    # shape of reprs: (-1, embed_dim)\n",
    "    \n",
    "    # shape of mus: (embed_dim, )\n",
    "    mu_r, mu_g = np.mean(repr1, axis=0), np.mean(repr2, axis=0)\n",
    "    # rowvar=False:\n",
    "    #     each column represents a variable, while the rows contain observations\n",
    "    # shape of sigmas: (embed_dim, embed_dim)\n",
    "    sigma_r, sigma_g = np.cov(repr1, rowvar=False), np.cov(repr2, rowvar=False)\n",
    "    \n",
    "    diff = mu_r - mu_g\n",
    "    diff_square_norm = diff.dot(diff)\n",
    "    \n",
    "    product = sigma_r.dot(sigma_g)\n",
    "    sqrt_product, _ = sqrtm(product, disp=False)\n",
    "    \n",
    "\n",
    "    if not np.isfinite(sqrt_product).all():\n",
    "        eye_matrix = np.eye(sigma_r.shape[0]) * 1e-8\n",
    "        sqrt_product = linalg.sqrtm((sigma_r + eye_matrix).dot(sigma_g + eye_matrix))\n",
    "    \n",
    "    # np.iscomplexobj:\n",
    "    #     Check for a complex type or an array of complex numbers.\n",
    "    #     The return value, True if x is of a complex type\n",
    "    #     or has at least one complex element.\n",
    "    if np.iscomplexobj(sqrt_product):\n",
    "        sqrt_product = sqrt_product.real\n",
    "\n",
    "    fid = diff_square_norm + np.trace(sigma_r + sigma_g - 2 * sqrt_product)\n",
    "    \n",
    "    return fid\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_representations():\n",
    "    real_reprs = np.zeros((len(test_loader) * batch_size, 2048))\n",
    "    fake_reprs = np.zeros((len(test_loader) * batch_size, 2048))\n",
    "    #real_reprs = np.zeros(1, 2048))\n",
    "    #fake_reprs = np.zeros(1, 2048))\n",
    "\n",
    "    \n",
    "    for i, batch in enumerate(tqdm(test_loader, desc=\"Build representations\")):\n",
    "        images, captions, captions_len, file_names = prepare_data(batch, device)\n",
    "        sent_emb, word = text_encoder(captions, captions_len)\n",
    "        sent_emb = sent_emb.detach()\n",
    "        fake_images = generate_images(generator, sent_emb,word, device)\n",
    "\n",
    "        clf_out_real = classifier.get_last_layer(images)\n",
    "        clf_out_fake = classifier.get_last_layer(fake_images)\n",
    "\n",
    "\n",
    "        real_reprs[i * batch_size: (i + 1) * batch_size] = clf_out_real.cpu().numpy()\n",
    "        fake_reprs[i * batch_size: (i + 1) * batch_size] = clf_out_fake.cpu().numpy()\n",
    "            \n",
    "    return real_reprs, fake_reprs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "real_values, fake_values = build_representations()\n",
    "real_values = torch.tensor(real_values)\n",
    "fake_values = torch.tensor(fake_values)\n",
    "\n",
    "\n",
    "fid_value = calculate_fid(real_values.numpy(), fake_values.numpy())\n",
    "#fid_value = calculate_fid(real_values, fake_values)\n",
    "print(f\"FID value = {fid_value}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def inception_score(reprs, batch_size):\n",
    "    def get_pred(x):\n",
    "        x = classifier.linear(torch.tensor(x, dtype=torch.float))\n",
    "        return F.softmax(x).data.cpu().numpy()\n",
    "\n",
    "\n",
    "    preds = np.zeros((reprs.shape[0], 1000))\n",
    "\n",
    "    splits = 0\n",
    "    for i in range(0, len(preds), batch_size):\n",
    "        aaai = reprs[i:i + batch_size]\n",
    "        aai = torch.tensor(aaai)\n",
    "        aai = aai.to(device)\n",
    "        z = get_pred(aai)\n",
    "        preds[i:i + batch_size] = z\n",
    "        splits += 1\n",
    "    \n",
    "    split_scores = []\n",
    "\n",
    "    for k in range(splits):\n",
    "        part = preds[k * batch_size: (k+1) * batch_size, :]\n",
    "        py = np.mean(part, axis=0)\n",
    "        \n",
    "        scores = []\n",
    "        for i in range(part.shape[0]):\n",
    "            pyx = part[i, :]\n",
    "            scores.append(entropy(pyx, py))\n",
    "            \n",
    "        split_scores.append(np.exp(np.mean(scores)))\n",
    "\n",
    "    return np.mean(split_scores), np.std(split_scores)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "a,b = inception_score(fake_values, batch_size)\n",
    "print('inception score mean: ',a)\n",
    "print('inception score std: ',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T05:07:01.217266Z",
     "iopub.status.busy": "2025-04-30T05:07:01.216625Z",
     "iopub.status.idle": "2025-04-30T05:13:12.735995Z",
     "shell.execute_reply": "2025-04-30T05:13:12.735197Z",
     "shell.execute_reply.started": "2025-04-30T05:07:01.217236Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total filenames: 11788\n",
      "Load captions from: /kaggle/input/projec/cv_seattention/cv_seattention/data/captions.pickle\n",
      "Load file names from: /kaggle/input/projec/cv_seattention/cv_seattention/data/test/filenames.pickle (2933)\n",
      "Dictionary size: 5450\n",
      "Embeddings number: 10\n",
      "2933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Epoch 220 =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3aa407dd10d46dcad6ee9884e6fc5b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Build representations:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID: 241.49\n",
      "Inception Score: mean=3.388, std=0.372\n",
      "\n",
      "===== Epoch 288 =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe77970b050e4fdeb1f2980b956b3f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Build representations:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID: 302.38\n",
      "Inception Score: mean=3.674, std=0.377\n",
      "\n",
      "===== Epoch 317 =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f6e26d2dc4c45fc9cac195d8ab30aa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Build representations:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID: 283.16\n",
      "Inception Score: mean=1.791, std=0.098\n",
      "\n",
      "===== Epoch 338 =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b981faa354164bc5b720d0dfed47c640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Build representations:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID: 286.19\n",
      "Inception Score: mean=2.334, std=0.145\n",
      "\n",
      "===== Epoch 368 =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "330c712e374c4c81be39457b3bdf2604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Build representations:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID: 290.92\n",
      "Inception Score: mean=1.884, std=0.103\n",
      "\n",
      "===== Epoch 388 =====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c1e037bb784fc59aca0690e768e8b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Build representations:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID: 239.64\n",
      "Inception Score: mean=2.884, std=0.247\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Insights ---\n",
      "Best FID: 239.64 at epoch 388\n",
      "Best Inception Score: 3.674 at epoch 288\n",
      "Observe the trends: FID should decrease and IS should increase as the model improves. Sudden jumps may indicate mode collapse or instability.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# List of generator checkpoint files and their epochs\n",
    "gen_ckpts = [\n",
    "    (220, \"/kaggle/input/projec/gennormal_220.pth\"),\n",
    "    (288, \"/kaggle/input/projec/gennormal_288.pth\"),\n",
    "    (317, \"/kaggle/input/projec/gennormal_317.pth\"),\n",
    "    (338, \"/kaggle/input/projec/gennormal_338.pth\"),\n",
    "    (368, \"/kaggle/input/projec/gennormal_368.pth\"),\n",
    "    (388, \"/kaggle/input/projec/gennormal_388.pth\"),\n",
    "]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load test loader and n_words\n",
    "batch_size = 32\n",
    "test_loader, n_words = create_loader(256, batch_size, \"/kaggle/input/projec/cv_seattention/cv_seattention/data\", \"test\")\n",
    "\n",
    "# Load CLIPTextEncoder\n",
    "text_encoder = CLIPTextEncoder.load(\"/kaggle/input/projec/clip_text_encoder.pth\", n_words).to(device)\n",
    "text_encoder.eval()\n",
    "for p in text_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# InceptionV3 class as in your code\n",
    "class InceptionV3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.device = f'cuda:{0}' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = torch.hub.load('pytorch/vision:v0.6.0', 'inception_v3', pretrained=True).to(self.device)\n",
    "        self.linear = self.model.fc\n",
    "        self.model.fc, self.model.dropout = [nn.Sequential()] * 2\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_last_layer(self, x):\n",
    "        x = F.interpolate(x, size=300, mode='bilinear', align_corners=False, recompute_scale_factor=False)\n",
    "        return self.model(x)\n",
    "\n",
    "classifier = InceptionV3().to(device).eval()\n",
    "\n",
    "def calculate_fid(repr1, repr2):\n",
    "    mu_r, mu_g = np.mean(repr1, axis=0), np.mean(repr2, axis=0)\n",
    "    sigma_r, sigma_g = np.cov(repr1, rowvar=False), np.cov(repr2, rowvar=False)\n",
    "    diff = mu_r - mu_g\n",
    "    diff_square_norm = diff.dot(diff)\n",
    "    product = sigma_r.dot(sigma_g)\n",
    "    sqrt_product, _ = linalg.sqrtm(product, disp=False)\n",
    "    if not np.isfinite(sqrt_product).all():\n",
    "        eye_matrix = np.eye(sigma_r.shape[0]) * 1e-8\n",
    "        sqrt_product = linalg.sqrtm((sigma_r + eye_matrix).dot(sigma_g + eye_matrix))\n",
    "    if np.iscomplexobj(sqrt_product):\n",
    "        sqrt_product = sqrt_product.real\n",
    "    fid = diff_square_norm + np.trace(sigma_r + sigma_g - 2 * sqrt_product)\n",
    "    return fid\n",
    "\n",
    "def inception_score(reprs, batch_size):\n",
    "    def get_pred(x):\n",
    "        x = classifier.linear(torch.tensor(x, dtype=torch.float))\n",
    "        return F.softmax(x).data.cpu().numpy()\n",
    "    preds = np.zeros((reprs.shape[0], 1000))\n",
    "    splits = 0\n",
    "    for i in range(0, len(preds), batch_size):\n",
    "        aaai = reprs[i:i + batch_size]\n",
    "        aai = torch.tensor(aaai).to(device)\n",
    "        z = get_pred(aai)\n",
    "        preds[i:i + batch_size] = z\n",
    "        splits += 1\n",
    "    split_scores = []\n",
    "    for k in range(splits):\n",
    "        part = preds[k * batch_size: (k+1) * batch_size, :]\n",
    "        py = np.mean(part, axis=0)\n",
    "        scores = []\n",
    "        for i in range(part.shape[0]):\n",
    "            pyx = part[i, :]\n",
    "            scores.append(entropy(pyx, py))\n",
    "        split_scores.append(np.exp(np.mean(scores)))\n",
    "    return np.mean(split_scores), np.std(split_scores)\n",
    "\n",
    "def build_representations(generator, text_encoder):\n",
    "    real_reprs = np.zeros((len(test_loader) * batch_size, 2048))\n",
    "    fake_reprs = np.zeros((len(test_loader) * batch_size, 2048))\n",
    "    idx = 0\n",
    "    for batch in tqdm(test_loader, desc=\"Build representations\"):\n",
    "        images, captions, captions_len, file_names = prepare_data(batch, device)\n",
    "        with torch.no_grad():\n",
    "            sent_emb, word = text_encoder(captions, captions_len)\n",
    "            sent_emb = sent_emb.detach()\n",
    "            word = word.detach()\n",
    "            fake_images = generator(torch.randn(images.size(0), 100).to(device), sent_emb, word)\n",
    "            clf_out_real = classifier.get_last_layer(images)\n",
    "            clf_out_fake = classifier.get_last_layer(fake_images)\n",
    "        real_reprs[idx: idx + images.size(0)] = clf_out_real.cpu().numpy()\n",
    "        fake_reprs[idx: idx + images.size(0)] = clf_out_fake.cpu().numpy()\n",
    "        idx += images.size(0)\n",
    "    return real_reprs, fake_reprs\n",
    "\n",
    "# Store results\n",
    "epochs = []\n",
    "fid_scores = []\n",
    "is_means = []\n",
    "is_stds = []\n",
    "\n",
    "for epoch, ckpt_path in gen_ckpts:\n",
    "    print(f\"\\n===== Epoch {epoch} =====\")\n",
    "    # Load generator\n",
    "    generator = Generator(n_channels=32, latent_dim=100).to(device)\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "    generator.eval()\n",
    "    # Build representations\n",
    "    real_values, fake_values = build_representations(generator, text_encoder)\n",
    "    # FID\n",
    "    fid_value = calculate_fid(real_values, fake_values)\n",
    "    # Inception Score\n",
    "    is_mean, is_std = inception_score(fake_values, batch_size)\n",
    "    # Print results\n",
    "    print(f\"FID: {fid_value:.2f}\")\n",
    "    print(f\"Inception Score: mean={is_mean:.3f}, std={is_std:.3f}\")\n",
    "    # Store for plotting\n",
    "    epochs.append(epoch)\n",
    "    fid_scores.append(fid_value)\n",
    "    is_means.append(is_mean)\n",
    "    is_stds.append(is_std)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, fid_scores, 'bo-', label='FID')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('FID Score')\n",
    "plt.title('FID Score vs Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.errorbar(epochs, is_means, yerr=is_stds, fmt='ro-', capsize=5, label='Inception Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Inception Score')\n",
    "plt.title('Inception Score vs Epoch')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Insights\n",
    "print(\"\\n--- Insights ---\")\n",
    "print(f\"Best FID: {min(fid_scores):.2f} at epoch {epochs[np.argmin(fid_scores)]}\")\n",
    "print(f\"Best Inception Score: {max(is_means):.3f} at epoch {epochs[np.argmax(is_means)]}\")\n",
    "print(\"Observe the trends: FID should decrease and IS should increase as the model improves. Sudden jumps may indicate mode collapse or instability.\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7070685,
     "sourceId": 11621156,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
