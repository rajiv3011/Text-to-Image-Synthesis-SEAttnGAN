{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T13:12:48.679704Z",
     "iopub.status.busy": "2025-04-30T13:12:48.679395Z",
     "iopub.status.idle": "2025-04-30T13:12:48.706411Z",
     "shell.execute_reply": "2025-04-30T13:12:48.705295Z",
     "shell.execute_reply.started": "2025-04-30T13:12:48.679682Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class RNNEncoder(nn.Module):\n",
    "    def __init__(self, ntoken, ninput=300, drop_prob=0.5,\n",
    "                 nhidden=128, nlayers=1, bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.n_steps = 18\n",
    "        self.ntoken = ntoken  # size of the dictionary\n",
    "        self.ninput = ninput  # size of each embedding vector\n",
    "        self.drop_prob = drop_prob  # probability of an element to be zeroed\n",
    "        self.nlayers = nlayers  # Number of recurrent layers\n",
    "        self.bidirectional = bidirectional\n",
    "        if bidirectional:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "        # number of features in the hidden state\n",
    "        self.nhidden = nhidden // self.num_directions\n",
    "\n",
    "        self.encoder = nn.Embedding(self.ntoken, self.ninput)\n",
    "        self.drop = nn.Dropout(self.drop_prob)\n",
    "\n",
    "        # dropout: If non-zero, introduces a dropout layer on\n",
    "        # the outputs of each RNN layer except the last layer\n",
    "        self.rnn = nn.LSTM(self.ninput, self.nhidden,\n",
    "                           self.nlayers, batch_first=True,\n",
    "                           dropout=self.drop_prob,\n",
    "                           bidirectional=self.bidirectional)\n",
    "\n",
    "\n",
    "    def forward(self, captions, cap_lens):\n",
    "        # input: torch.LongTensor of size batch x n_steps\n",
    "        # --> emb: batch x n_steps x ninput\n",
    "        emb = self.drop(self.encoder(captions))\n",
    "        #\n",
    "        # Returns: a PackedSequence object\n",
    "        cap_lens = cap_lens.data.tolist()\n",
    "        emb = pack_padded_sequence(emb, cap_lens, batch_first=True)\n",
    "   \n",
    "        output, hidden = self.rnn(emb)\n",
    "        # PackedSequence object\n",
    "        # --> (batch, seq_len, hidden_size * num_directions)\n",
    "        output = pad_packed_sequence(output, batch_first=True)[0]\n",
    "        words_emb = output.transpose(1, 2)\n",
    "\n",
    "        sent_emb = hidden[0].transpose(0, 1).contiguous()\n",
    "        sent_emb = sent_emb.view(-1, self.nhidden * self.num_directions)\n",
    "        return sent_emb, words_emb\n",
    "\n",
    "    @staticmethod\n",
    "    def load(weights_path: str, ntoken: int) -> 'RNNEncoder':\n",
    "        text_encoder = RNNEncoder(ntoken, nhidden=256)\n",
    "        state_dict = torch.load(weights_path, map_location=lambda storage, loc: storage)\n",
    "        text_encoder.load_state_dict(state_dict)\n",
    "        return text_encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T13:12:48.877619Z",
     "iopub.status.busy": "2025-04-30T13:12:48.877217Z",
     "iopub.status.idle": "2025-04-30T13:12:48.902508Z",
     "shell.execute_reply": "2025-04-30T13:12:48.901358Z",
     "shell.execute_reply.started": "2025-04-30T13:12:48.877592Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "def conv1x1(in_planes, out_planes):\n",
    "    \"\"\"1x1 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1,\n",
    "                   padding=0, bias=False)\n",
    "\n",
    "class GlobalAttentionGeneral(nn.Module):\n",
    "    def __init__(self, idf, cdf):\n",
    "        super().__init__()\n",
    "        self.conv_context = conv1x1(cdf, idf)\n",
    "        self.sm = nn.Softmax(dim=1)\n",
    "        self.mask = None\n",
    "\n",
    "    def applyMask(self, mask):\n",
    "        self.mask = mask  # batch x sourceL\n",
    "\n",
    "    def forward(self, input, context):\n",
    "        \"\"\"\n",
    "        input: batch x idf x ih x iw (queryL=ihxiw)\n",
    "        context: batch x cdf x sourceL\n",
    "        \"\"\"\n",
    "        ih, iw = input.size(2), input.size(3)\n",
    "        queryL = ih * iw\n",
    "        batch_size, sourceL = context.size(0), context.size(2)\n",
    "\n",
    "        # --> batch x queryL x idf\n",
    "        target = input.view(batch_size, -1, queryL)\n",
    "        targetT = torch.transpose(target, 1, 2).contiguous()\n",
    "        \n",
    "        # batch x cdf x sourceL --> batch x cdf x sourceL x 1\n",
    "        sourceT = context.unsqueeze(3)\n",
    "        # --> batch x idf x sourceL\n",
    "        sourceT = self.conv_context(sourceT).squeeze(3)\n",
    "\n",
    "        # Get attention\n",
    "        attn = torch.bmm(targetT, sourceT)  # batch x queryL x sourceL\n",
    "        attn = attn.view(batch_size * queryL, sourceL)\n",
    "        \n",
    "        if self.mask is not None:\n",
    "            mask = self.mask.repeat(queryL, 1)\n",
    "            attn.data.masked_fill_(mask.data, -float('inf'))\n",
    "        \n",
    "        attn = self.sm(attn)\n",
    "        attn = attn.view(batch_size, queryL, sourceL)\n",
    "        attn = torch.transpose(attn, 1, 2).contiguous()\n",
    "\n",
    "        # Apply attention\n",
    "        weightedContext = torch.bmm(sourceT, attn)\n",
    "        weightedContext = weightedContext.view(batch_size, -1, ih, iw)\n",
    "        attn = attn.view(batch_size, -1, ih, iw)\n",
    "\n",
    "        return weightedContext, attn\n",
    "\n",
    "class AffineBlock(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.gamma_mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        self.beta_mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_normal_(module.weight)\n",
    "                nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, x: Tensor, sentence_embed: Tensor) -> Tensor:\n",
    "        scale_param = self.gamma_mlp(sentence_embed).unsqueeze(-1).unsqueeze(-1)\n",
    "        shift_param = self.beta_mlp(sentence_embed).unsqueeze(-1).unsqueeze(-1)\n",
    "        return scale_param * x + shift_param\n",
    "\n",
    "class ResidualBlockG(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, text_dim: int = 256):\n",
    "        super().__init__()\n",
    "        hidden_dim = text_dim // 2\n",
    "        \n",
    "        self.affine1 = AffineBlock(text_dim, hidden_dim, in_channels)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.affine2 = AffineBlock(text_dim, hidden_dim, out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.skip = nn.Conv2d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else nn.Identity()\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x: Tensor, sentence_embed: Tensor) -> Tensor:\n",
    "        residual = self.skip(x)\n",
    "        x = F.leaky_relu(self.affine1(x, sentence_embed), 0.2)\n",
    "        x = self.conv1(x)\n",
    "        x = F.leaky_relu(self.affine2(x, sentence_embed), 0.2)\n",
    "        x = self.conv2(x)\n",
    "        return residual + self.gamma * x\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, n_channels: int = 32, latent_dim: int = 100):\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        \n",
    "        # Initial projection (4x4 spatial size)\n",
    "        self.linear_in = nn.Linear(latent_dim, 8 * n_channels * 4 * 4)\n",
    "        \n",
    "        # Residual blocks with progressive upsampling\n",
    "        self.res_blocks = nn.ModuleList([\n",
    "            ResidualBlockG(8 * n_channels, 8 * n_channels),\n",
    "            ResidualBlockG(8 * n_channels, 4 * n_channels),\n",
    "            ResidualBlockG(4 * n_channels, 2 * n_channels),\n",
    "            ResidualBlockG(2 * n_channels, n_channels),\n",
    "        ])\n",
    "        \n",
    "        # Attention mechanism at 64x64 resolution\n",
    "        self.att = GlobalAttentionGeneral(n_channels, 256)\n",
    "        self.att_conv = nn.Conv2d(2 * n_channels, n_channels, kernel_size=1)\n",
    "        \n",
    "        # Final upsampling to 256x256\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),  # 64->128\n",
    "            nn.Conv2d(n_channels, n_channels//2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n_channels//2),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            \n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),  # 128->256\n",
    "            nn.Conv2d(n_channels//2, n_channels//4, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n_channels//4),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "        \n",
    "        # Output layers\n",
    "        self.conv_out = nn.Sequential(\n",
    "            nn.Conv2d(n_channels//4, 3, kernel_size=3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise: Tensor, sentence_embed: Tensor, word_embed: Tensor) -> Tensor:\n",
    "        # Initial projection\n",
    "        x = self.linear_in(noise).view(-1, 8 * self.n_channels, 4, 4)\n",
    "        \n",
    "        # Process through residual blocks with 2x upsampling each\n",
    "        for block in self.res_blocks:\n",
    "            x = block(x, sentence_embed)\n",
    "            x = F.interpolate(x, scale_factor=2, mode='nearest')  # 4->8->16->32->64\n",
    "        \n",
    "        # Apply attention at 64x64 resolution\n",
    "        attn_out, _ = self.att(x, word_embed)\n",
    "        x = torch.cat([x, attn_out], dim=1)\n",
    "        x = self.att_conv(x)\n",
    "        \n",
    "        # Final upsampling to 256x256\n",
    "        x = self.upsample(x)\n",
    "        return self.conv_out(x)\n",
    "        \n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T13:12:48.904464Z",
     "iopub.status.busy": "2025-04-30T13:12:48.904174Z",
     "iopub.status.idle": "2025-04-30T13:12:50.150603Z",
     "shell.execute_reply": "2025-04-30T13:12:50.149602Z",
     "shell.execute_reply.started": "2025-04-30T13:12:48.904444Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Generator                                [24, 3, 256, 256]         --\n",
       "├─Linear: 1-1                            [24, 4096]                413,696\n",
       "├─ModuleList: 1-2                        --                        --\n",
       "│    └─ResidualBlockG: 2-1               [24, 256, 4, 4]           1\n",
       "│    │    └─Identity: 3-1                [24, 256, 4, 4]           --\n",
       "│    │    └─AffineBlock: 3-2             [24, 256, 4, 4]           131,840\n",
       "│    │    └─Conv2d: 3-3                  [24, 256, 4, 4]           590,080\n",
       "│    │    └─AffineBlock: 3-4             [24, 256, 4, 4]           131,840\n",
       "│    │    └─Conv2d: 3-5                  [24, 256, 4, 4]           590,080\n",
       "│    └─ResidualBlockG: 2-2               [24, 128, 8, 8]           1\n",
       "│    │    └─Conv2d: 3-6                  [24, 128, 8, 8]           32,896\n",
       "│    │    └─AffineBlock: 3-7             [24, 256, 8, 8]           131,840\n",
       "│    │    └─Conv2d: 3-8                  [24, 128, 8, 8]           295,040\n",
       "│    │    └─AffineBlock: 3-9             [24, 128, 8, 8]           98,816\n",
       "│    │    └─Conv2d: 3-10                 [24, 128, 8, 8]           147,584\n",
       "│    └─ResidualBlockG: 2-3               [24, 64, 16, 16]          1\n",
       "│    │    └─Conv2d: 3-11                 [24, 64, 16, 16]          8,256\n",
       "│    │    └─AffineBlock: 3-12            [24, 128, 16, 16]         98,816\n",
       "│    │    └─Conv2d: 3-13                 [24, 64, 16, 16]          73,792\n",
       "│    │    └─AffineBlock: 3-14            [24, 64, 16, 16]          82,304\n",
       "│    │    └─Conv2d: 3-15                 [24, 64, 16, 16]          36,928\n",
       "│    └─ResidualBlockG: 2-4               [24, 32, 32, 32]          1\n",
       "│    │    └─Conv2d: 3-16                 [24, 32, 32, 32]          2,080\n",
       "│    │    └─AffineBlock: 3-17            [24, 64, 32, 32]          82,304\n",
       "│    │    └─Conv2d: 3-18                 [24, 32, 32, 32]          18,464\n",
       "│    │    └─AffineBlock: 3-19            [24, 32, 32, 32]          74,048\n",
       "│    │    └─Conv2d: 3-20                 [24, 32, 32, 32]          9,248\n",
       "├─GlobalAttentionGeneral: 1-3            [24, 32, 64, 64]          --\n",
       "│    └─Conv2d: 2-5                       [24, 32, 18, 1]           8,192\n",
       "│    └─Softmax: 2-6                      [98304, 18]               --\n",
       "├─Conv2d: 1-4                            [24, 32, 64, 64]          2,080\n",
       "├─Sequential: 1-5                        [24, 8, 256, 256]         --\n",
       "│    └─Upsample: 2-7                     [24, 32, 128, 128]        --\n",
       "│    └─Conv2d: 2-8                       [24, 16, 128, 128]        4,624\n",
       "│    └─BatchNorm2d: 2-9                  [24, 16, 128, 128]        32\n",
       "│    └─LeakyReLU: 2-10                   [24, 16, 128, 128]        --\n",
       "│    └─Upsample: 2-11                    [24, 16, 256, 256]        --\n",
       "│    └─Conv2d: 2-12                      [24, 8, 256, 256]         1,160\n",
       "│    └─BatchNorm2d: 2-13                 [24, 8, 256, 256]         16\n",
       "│    └─LeakyReLU: 2-14                   [24, 8, 256, 256]         --\n",
       "├─Sequential: 1-6                        [24, 3, 256, 256]         --\n",
       "│    └─Conv2d: 2-15                      [24, 3, 256, 256]         219\n",
       "│    └─Tanh: 2-16                        [24, 3, 256, 256]         --\n",
       "==========================================================================================\n",
       "Total params: 3,066,279\n",
       "Trainable params: 3,066,279\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 6.87\n",
       "==========================================================================================\n",
       "Input size (MB): 0.48\n",
       "Forward/backward pass size (MB): 401.25\n",
       "Params size (MB): 12.27\n",
       "Estimated Total Size (MB): 413.99\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchinfo\n",
    "\n",
    "gen = Generator(n_channels=32, latent_dim=100)\n",
    "\n",
    "noise = torch.rand((24, 100))\n",
    "sent = torch.rand((24, 256))\n",
    "word = torch.rand((24, 256, 18))\n",
    "\n",
    "torchinfo.summary(gen, input_data=(noise, sent, word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T13:12:50.151886Z",
     "iopub.status.busy": "2025-04-30T13:12:50.151583Z",
     "iopub.status.idle": "2025-04-30T13:12:53.774635Z",
     "shell.execute_reply": "2025-04-30T13:12:53.773666Z",
     "shell.execute_reply.started": "2025-04-30T13:12:50.151857Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ShallowResNetDiscriminator(nn.Module):\n",
    "    def __init__(self, n_c=32, sentence_embed_dim=256):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "        # Use only the first two blocks of ResNet18\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            resnet.conv1,  # Initial conv layer\n",
    "            resnet.bn1,\n",
    "            resnet.relu,\n",
    "            resnet.maxpool,\n",
    "            resnet.layer1,  # First ResNet block\n",
    "            resnet.layer2,  # Second ResNet block\n",
    "        )\n",
    "\n",
    "        # Output channels from layer2 in ResNet-18 is 128\n",
    "        self.extra_layers = nn.Sequential(\n",
    "            nn.Conv2d(128, n_c * 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(n_c * 16, n_c * 16, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Conv2d(n_c * 16, n_c * 16, kernel_size=4, stride=2, padding=1),\n",
    "        )\n",
    "\n",
    "        # Final classification layers\n",
    "        in_c_logit = 16 * n_c + sentence_embed_dim\n",
    "        self.img_sentence_forward = nn.Sequential(\n",
    "            nn.Conv2d(in_c_logit, n_c * 2, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(n_c * 2, 1, kernel_size=2, stride=1, padding=0, bias=False)\n",
    "        )\n",
    "\n",
    "    def build_embeds(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Extract feature embeddings from the input image using ResNet and additional conv layers.\n",
    "        Expected output shape: [batch_size, n_c*16, 2, 2] for typical 256x256 inputs.\n",
    "        \"\"\"\n",
    "        out = self.feature_extractor(image)  # Shape: [batch, 128, h/4, w/4]\n",
    "        out = self.extra_layers(out)  # Shape: [batch, n_c*16, 2, 2]\n",
    "        return out\n",
    "\n",
    "    def get_logits(self, image_embed: torch.Tensor, sentence_embed: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Combine image features and sentence embeddings, then compute real/fake logits.\n",
    "        \"\"\"\n",
    "        # Reshape sentence_embed: [batch, 256] -> [batch, 256, 1, 1] and then repeat to match image spatial dims.\n",
    "        sentence_embed = sentence_embed.view(-1, 256, 1, 1).repeat(1, 1, image_embed.shape[2], image_embed.shape[3])\n",
    "        h_c_code = torch.cat((image_embed, sentence_embed), 1)  # Concatenate along channel dimension\n",
    "        logits = self.img_sentence_forward(h_c_code)\n",
    "        return logits\n",
    "\n",
    "    def forward(self, image: torch.Tensor, sentence_embed: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the shallow discriminator.\n",
    "        \"\"\"\n",
    "        image_embed = self.build_embeds(image)\n",
    "        logits = self.get_logits(image_embed, sentence_embed)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T13:12:53.776065Z",
     "iopub.status.busy": "2025-04-30T13:12:53.775727Z",
     "iopub.status.idle": "2025-04-30T13:12:53.787919Z",
     "shell.execute_reply": "2025-04-30T13:12:53.787038Z",
     "shell.execute_reply.started": "2025-04-30T13:12:53.776045Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "class ResidualBlockD(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Main convolution path\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        # Shortcut path (channel and spatial adjustment)\n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False) \n",
    "            if in_channels != out_channels else nn.Identity(),\n",
    "            nn.AvgPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.shortcut(x) + self.gamma * self.conv(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, n_c: int = 32, sentence_embed_dim: int = 256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Image processing pathway\n",
    "        self.img_encoder = nn.Sequential(\n",
    "            # Initial convolution (no residual)\n",
    "            nn.Conv2d(3, n_c, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "            # Residual downsampling blocks\n",
    "            ResidualBlockD(n_c * 1, n_c * 2),  # 256x256 -> 128x128\n",
    "            ResidualBlockD(n_c * 2, n_c * 4),  # 128x128 -> 64x64\n",
    "            ResidualBlockD(n_c * 4, n_c * 8),  # 64x64 -> 32x32\n",
    "            ResidualBlockD(n_c * 8, n_c * 16), # 32x32 -> 16x16\n",
    "            \n",
    "            # Final downsampling\n",
    "            nn.Conv2d(n_c*16, n_c*16, 4, 2, 1),  # 16x16 -> 8x8\n",
    "            nn.Conv2d(n_c*16, n_c*16, 4, 2, 1)    # 8x8 -> 4x4\n",
    "        )\n",
    "\n",
    "        # Text-image fusion\n",
    "        self.judge_net = nn.Sequential(\n",
    "            nn.Conv2d(n_c*16 + sentence_embed_dim, n_c*2, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(n_c*2, 1, 4, 1, 0)  # Final 1x1 output\n",
    "        )\n",
    "\n",
    "    def build_embeds(self, image: Tensor) -> Tensor:\n",
    "        \"\"\"Extract image features (same as original)\"\"\"\n",
    "        return self.img_encoder(image)\n",
    "\n",
    "    def get_logits(self, image_embed: Tensor, sentence_embed: Tensor) -> Tensor:\n",
    "        \"\"\"Fuse image and text features (same interface)\"\"\"\n",
    "        # Expand text to spatial dimensions\n",
    "        sentence_embed = sentence_embed.view(-1, 256, 1, 1).expand(-1, -1, 4, 4)\n",
    "        \n",
    "        # Concatenate and classify\n",
    "        combined = torch.cat((image_embed, sentence_embed), dim=1)\n",
    "        return self.judge_net(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T13:12:53.791271Z",
     "iopub.status.busy": "2025-04-30T13:12:53.790898Z",
     "iopub.status.idle": "2025-04-30T13:12:54.138648Z",
     "shell.execute_reply": "2025-04-30T13:12:54.137971Z",
     "shell.execute_reply.started": "2025-04-30T13:12:53.791238Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as random\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "\n",
    "class DFGANDataset(Dataset):\n",
    "    def __init__(self, data_dir: str, split: str = \"train\", transform: Optional[Compose] = None):\n",
    "        self.split = split\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "        self.split_dir = os.path.join(data_dir, split)\n",
    "        self.captions_path = os.path.join(self.data_dir, \"captions.pickle\")\n",
    "        self.filenames_path = os.path.join(self.split_dir, \"filenames.pickle\")\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        self.embeddings_num = 10\n",
    "\n",
    "        self.normalize = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "\n",
    "        self.images_dir = os.path.join(self.data_dir, \"CUB_200_2011/CUB_200_2011/images\")\n",
    "        self.bbox_path = os.path.join(self.data_dir, \"CUB_200_2011/CUB_200_2011/bounding_boxes.txt\")\n",
    "        self.images_path = os.path.join(self.data_dir, \"CUB_200_2011/CUB_200_2011/images.txt\")\n",
    "\n",
    "        self.bbox = self._load_bbox()\n",
    "\n",
    "        self.file_names, self.captions, self.code2word, self.word2code = self._load_text_data()\n",
    "\n",
    "        self.n_words = len(self.code2word)\n",
    "        self.num_examples = len(self.file_names)\n",
    "\n",
    "        self._print_info()\n",
    "\n",
    "    def _print_info(self):\n",
    "        print(f\"Total filenames: {len(self.bbox)}\")\n",
    "        print(f\"Load captions from: {self.captions_path}\")\n",
    "        print(f\"Load file names from: {self.filenames_path} ({self.num_examples})\")\n",
    "        print(f\"Dictionary size: {self.n_words}\")\n",
    "        print(f\"Embeddings number: {self.embeddings_num}\")\n",
    "\n",
    "    def _load_bbox(self) -> Dict[str, List[int]]:\n",
    "        df_bbox = pd.read_csv(self.bbox_path, delim_whitespace=True, header=None).astype(int)\n",
    "\n",
    "        df_image_names = pd.read_csv(self.images_path, delim_whitespace=True, header=None)\n",
    "        image_names = df_image_names[1].tolist()\n",
    "\n",
    "        filename_bbox = dict()\n",
    "        for i, file_name in enumerate(image_names):\n",
    "            bbox = df_bbox.iloc[i][1:].tolist()\n",
    "            filename_bbox[file_name[:-4]] = bbox\n",
    "\n",
    "        return filename_bbox\n",
    "\n",
    "    def _load_text_data(self) -> Tuple[List[str], List[List[int]],\n",
    "                                       Dict[int, str], Dict[str, int]]:\n",
    "        with open(self.captions_path, 'rb') as file:\n",
    "            train_captions, test_captions, code2word, word2code = pickle.load(file)\n",
    "\n",
    "        filenames = self._load_filenames()\n",
    "\n",
    "        if self.split == 'train':\n",
    "            return filenames, train_captions, code2word, word2code\n",
    "\n",
    "        return filenames, test_captions, code2word, word2code\n",
    "\n",
    "    def _load_filenames(self) -> List[str]:\n",
    "        if os.path.isfile(self.filenames_path):\n",
    "            with open(self.filenames_path, 'rb') as file:\n",
    "                return pickle.load(file)\n",
    "\n",
    "        raise ValueError(f\"File {self.filenames_path} does not exist\")\n",
    "\n",
    "    def _get_caption(self, caption_idx: int) -> Tuple[np.ndarray, int]:\n",
    "        caption = np.array(self.captions[caption_idx])\n",
    "        pad_caption = np.zeros((18, 1), dtype='int64')\n",
    "\n",
    "        if len(caption) <= 18:\n",
    "            pad_caption[:len(caption), 0] = caption\n",
    "            return pad_caption, len(caption)\n",
    "\n",
    "        indices = list(np.arange(len(caption)))\n",
    "        np.random.shuffle(indices)\n",
    "        pad_caption[:, 0] = caption[np.sort(indices[:18])]\n",
    "\n",
    "        return pad_caption, 18\n",
    "\n",
    "    def _get_image(self, image_path: str, bbox: List[int]) -> Tensor:\n",
    "\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        width, height = image.size\n",
    "\n",
    "        r = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n",
    "        center_x = int((2 * bbox[0] + bbox[2]) / 2)\n",
    "        center_y = int((2 * bbox[1] + bbox[3]) / 2)\n",
    "\n",
    "        y1 = np.maximum(0, center_y - r)\n",
    "        y2 = np.minimum(height, center_y + r)\n",
    "        x1 = np.maximum(0, center_x - r)\n",
    "        x2 = np.minimum(width, center_x + r)\n",
    "\n",
    "        image = image.crop((x1, y1, x2, y2))\n",
    "        image = self.normalize(self.transform(image))\n",
    "\n",
    "        return image\n",
    "\n",
    "    def _get_random_caption(self, idx: int) -> Tuple[np.ndarray, int]:\n",
    "        caption_shift = random.randint(0, self.embeddings_num-1)\n",
    "        caption_idx = idx * self.embeddings_num + caption_shift\n",
    "\n",
    "        if caption_idx >= len(self.captions):\n",
    "            caption_idx = len(self.captions) - 1 \n",
    "            \n",
    "        return self._get_caption(caption_idx)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Tensor, np.ndarray, int, str]:\n",
    "        file_name = self.file_names[idx]\n",
    "        image = self._get_image(f\"{self.images_dir}/{file_name}.jpg\", self.bbox[file_name])\n",
    "\n",
    "        encoded_caption, caption_len = self._get_random_caption(idx)\n",
    "\n",
    "        return image, encoded_caption, caption_len, file_name\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T13:12:54.139829Z",
     "iopub.status.busy": "2025-04-30T13:12:54.139474Z",
     "iopub.status.idle": "2025-04-30T13:12:54.146147Z",
     "shell.execute_reply": "2025-04-30T13:12:54.145198Z",
     "shell.execute_reply.started": "2025-04-30T13:12:54.139809Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def prepare_data(batch: Tuple[Tensor, Tensor, Tensor, Tuple[str]],\n",
    "                 device: torch.device) -> Tuple[Tensor, Tensor, Tensor, List[str]]:\n",
    "    images, captions, captions_len, file_names = batch\n",
    "\n",
    "    sorted_cap_lens, sorted_cap_indices = torch.sort(captions_len, 0, True)\n",
    "    sorted_cap_lens = sorted_cap_lens.to(device)\n",
    "\n",
    "    sorted_images = images[sorted_cap_indices].to(device)\n",
    "    sorted_captions = captions[sorted_cap_indices].squeeze().to(device)\n",
    "    sorted_file_names = [file_names[i] for i in sorted_cap_indices.numpy()]\n",
    "\n",
    "    return sorted_images, sorted_captions, sorted_cap_lens, sorted_file_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T13:12:54.147468Z",
     "iopub.status.busy": "2025-04-30T13:12:54.147167Z",
     "iopub.status.idle": "2025-04-30T13:12:54.165294Z",
     "shell.execute_reply": "2025-04-30T13:12:54.164435Z",
     "shell.execute_reply.started": "2025-04-30T13:12:54.147442Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Subset\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_loader(imsize: int, batch_size: int, data_dir: str, split: str) -> DataLoader:\n",
    "    assert split in [\"train\", \"test\"], \"Wrong split type, expected train or test\"\n",
    "    image_transform = transforms.Compose([\n",
    "        transforms.Resize(int(imsize * 76 / 64)),\n",
    "        transforms.RandomCrop(imsize),\n",
    "        transforms.RandomHorizontalFlip()\n",
    "    ])\n",
    "\n",
    "    dataset = DFGANDataset(data_dir, split, image_transform)\n",
    "    \n",
    "    n_words = dataset.n_words\n",
    "    \n",
    "    subset_size=6000\n",
    "    shuffled_indices = torch.randperm(len(dataset))[:6000].tolist()\n",
    "    dataset = Subset(dataset, shuffled_indices)\n",
    "    \n",
    "\n",
    "    print(len(dataset))\n",
    "\n",
    "    return DataLoader(dataset, batch_size=batch_size, drop_last=True,shuffle=True),n_words\n",
    "\n",
    "\n",
    "def fix_seed(seed: int = 123321):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    print(f\"Seed {seed} fixed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T13:12:54.166687Z",
     "iopub.status.busy": "2025-04-30T13:12:54.166419Z",
     "iopub.status.idle": "2025-04-30T13:12:54.190892Z",
     "shell.execute_reply": "2025-04-30T13:12:54.189787Z",
     "shell.execute_reply.started": "2025-04-30T13:12:54.166667Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Tuple, List\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.utils as vutils\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import trange\n",
    "\n",
    "class DeepFusionGAN:\n",
    "    def __init__(self, n_words, encoder_weights_path: str, image_save_path: str, gen_path_save: str, use_pretrained_discriminator: bool = True):\n",
    "        super().__init__()\n",
    "        self.image_save_path = image_save_path\n",
    "        self.gen_path_save = gen_path_save\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.generator = Generator(n_channels=32, latent_dim=100).to(self.device)\n",
    "\n",
    "        if use_pretrained_discriminator:\n",
    "            self.discriminator = ShallowResNetDiscriminator(n_c=32).to(self.device)\n",
    "        else:\n",
    "            self.discriminator = Discriminator(n_c=32).to(self.device)\n",
    "\n",
    "        self.text_encoder = RNNEncoder.load(encoder_weights_path, n_words).to(self.device)\n",
    "        for p in self.text_encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.text_encoder.eval()\n",
    "\n",
    "        self.g_optim = torch.optim.Adam(self.generator.parameters(), lr=0.0003, betas=(0.5, 0.999))\n",
    "        self.d_optim = torch.optim.Adam(self.discriminator.parameters(), lr=5e-6, betas=(0.5, 0.999))\n",
    "\n",
    "    def _compute_gp(self, images: Tensor, sentence_embeds: Tensor) -> Tensor:\n",
    "        batch_size = images.shape[0]\n",
    "        images_interpolated = images.data.requires_grad_()\n",
    "        sentences_interpolated = sentence_embeds.data.requires_grad_()\n",
    "        \n",
    "        embeds = self.discriminator.build_embeds(images_interpolated)\n",
    "        logits = self.discriminator.get_logits(embeds, sentences_interpolated)\n",
    "        \n",
    "        grad_outputs = torch.ones_like(logits)\n",
    "        grads = torch.autograd.grad(\n",
    "            outputs=logits,\n",
    "            inputs=(images_interpolated, sentences_interpolated),\n",
    "            grad_outputs=grad_outputs,\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )\n",
    "        \n",
    "        grad_0 = grads[0].reshape(batch_size, -1)\n",
    "        grad_1 = grads[1].reshape(batch_size, -1)\n",
    "        \n",
    "        grad = torch.cat((grad_0, grad_1), dim=1)\n",
    "        grad_norm = grad.norm(2, dim=1)\n",
    "        return grad_norm\n",
    "\n",
    "    def fit(self, train_loader: DataLoader, num_epochs: int = 500, checkpoint_path: str = None) -> Tuple[List[float], List[float], List[float]]:\n",
    "        g_losses_epoch, d_losses_epoch, d_gp_losses_epoch = [], [], []\n",
    "        lambda_gp = 1.0\n",
    "        start_epoch = 150\n",
    "\n",
    "        if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "            # start_epoch = self._load_gen_weights(checkpoint_path)\n",
    "            print(f\"Resuming from epoch {start_epoch}\")\n",
    "\n",
    "        path=\"/kaggle/input/workin/gennormal_200.pth\"\n",
    "        start_epoch=self._load_gen_weights(path)\n",
    "\n",
    "        for epoch in trange(start_epoch, num_epochs + start_epoch, desc=\"Training DeepFusionGAN\"):\n",
    "            g_losses, d_losses, d_gp_losses = [], [], []\n",
    "            for batch in train_loader:\n",
    "                images, captions, captions_len, _ = prepare_data(batch, self.device)\n",
    "                batch_size = images.shape[0]\n",
    "\n",
    "                sentence_embeds, words_embs = self.text_encoder(captions, captions_len)\n",
    "                sentence_embeds, words_embs = sentence_embeds.detach(), words_embs.detach()\n",
    "\n",
    "                for _ in range(3): \n",
    "                    real_logits = self.discriminator(images, sentence_embeds)\n",
    "                    d_loss_real = -torch.mean(real_logits)\n",
    "\n",
    "                    noise = torch.randn(batch_size, 100, device=self.device)\n",
    "                    fake_images = self.generator(noise, sentence_embeds, words_embs)\n",
    "                    fake_logits = self.discriminator(fake_images.detach(), sentence_embeds)\n",
    "                    d_loss_fake = torch.mean(fake_logits)\n",
    "\n",
    "                    d_loss_gp = torch.mean((self._compute_gp(images, sentence_embeds) - 1) ** 2)\n",
    "                    d_loss = d_loss_real + d_loss_fake + lambda_gp * d_loss_gp\n",
    "\n",
    "                    self.d_optim.zero_grad()\n",
    "                    d_loss.backward()\n",
    "                    self.d_optim.step()\n",
    "\n",
    "                    d_losses.append(d_loss.item())\n",
    "                    d_gp_losses.append(d_loss_gp.item())\n",
    "\n",
    "              \n",
    "                noise = torch.randn(batch_size, 100, device=self.device)\n",
    "                fake_images = self.generator(noise, sentence_embeds, words_embs)\n",
    "                fake_logits = self.discriminator(fake_images, sentence_embeds)\n",
    "                g_loss = -torch.mean(fake_logits)\n",
    "\n",
    "                self.g_optim.zero_grad()\n",
    "                g_loss.backward()\n",
    "                self.g_optim.step()\n",
    "\n",
    "                g_losses.append(g_loss.item())\n",
    "\n",
    "            g_losses_epoch.append(sum(g_losses) / len(g_losses))\n",
    "            d_losses_epoch.append(sum(d_losses) / len(d_losses))\n",
    "            d_gp_losses_epoch.append(sum(d_gp_losses) / len(d_gp_losses))\n",
    "\n",
    "            self._save_fake_image(fake_images, epoch)\n",
    "            self._save_gen_weights(epoch)\n",
    "            print(f\"Epoch {epoch}: G Loss {g_losses_epoch[-1]:.4f}, D Loss {d_losses_epoch[-1]:.4f}, GP Loss {d_gp_losses_epoch[-1]:.4f}\")\n",
    "\n",
    "        return g_losses_epoch, d_losses_epoch, d_gp_losses_epoch\n",
    "\n",
    "    def _save_fake_image(self, fake_images: Tensor, epoch: int):\n",
    "        img_path = os.path.join(self.image_save_path, f\"fake_wgangp_epoch_{epoch}.png\")\n",
    "        vutils.save_image(fake_images.data, img_path, normalize=True)\n",
    "\n",
    "    def _save_gen_weights(self, epoch: int):\n",
    "        gen_path = os.path.join(self.gen_path_save, f\"gennormal_{epoch}.pth\")\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'generator_state_dict': self.generator.state_dict(),\n",
    "            'discriminator_state_dict': self.discriminator.state_dict(),\n",
    "            'g_optim_state_dict': self.g_optim.state_dict(),\n",
    "            'd_optim_state_dict': self.d_optim.state_dict(),\n",
    "        }\n",
    "        torch.save(checkpoint, gen_path)\n",
    "        print(f\"Model checkpoint saved at epoch {epoch} to {gen_path}\")\n",
    "\n",
    "    def _load_gen_weights(self, checkpoint_path: str) -> int:\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        self.generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "        self.discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "        self.g_optim.load_state_dict(checkpoint['g_optim_state_dict'])\n",
    "        self.d_optim.load_state_dict(checkpoint['d_optim_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        print(f\"Checkpoint loaded from {checkpoint_path} - Starting from epoch {start_epoch}\")\n",
    "        return start_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T13:12:54.237730Z",
     "iopub.status.busy": "2025-04-30T13:12:54.237426Z",
     "iopub.status.idle": "2025-04-30T13:12:54.257664Z",
     "shell.execute_reply": "2025-04-30T13:12:54.256810Z",
     "shell.execute_reply.started": "2025-04-30T13:12:54.237705Z"
    }
   },
   "outputs": [],
   "source": [
    "def train() -> Tuple[List[float], List[float], List[float]]:\n",
    "    fix_seed()\n",
    "\n",
    "    data_path = \"/kaggle/input/cv-seattention/cv_seattention/data\"\n",
    "    encoder_weights_path = \"/kaggle/input/cv-seattention/cv_seattention/text_encoder_weights/text_encoder200.pth\"\n",
    "    image_save_path = \"/kaggle/working/gen_images\"\n",
    "    gen_path_save = \"/kaggle/working/gen_weights\"\n",
    "\n",
    "    os.makedirs(image_save_path, exist_ok=True)\n",
    "    os.makedirs(gen_path_save, exist_ok=True)\n",
    "\n",
    "    train_loader, n_words = create_loader(256, 32, data_path, \"train\")\n",
    "  # model = DeepFusionGAN(n_words=n_words,\n",
    "    #                      encoder_weights_path=encoder_weights_path,\n",
    "    #                      image_save_path=image_save_path,\n",
    "    #                      gen_path_save=gen_path_save)\n",
    "    model = DeepFusionGAN(\n",
    "        n_words = train_loader.dataset.dataset.n_words,\n",
    "        encoder_weights_path=encoder_weights_path,\n",
    "        image_save_path=image_save_path,\n",
    "        gen_path_save=gen_path_save,\n",
    "        use_pretrained_discriminator=True  # Switch to small trainable D\n",
    "    )  \n",
    "\n",
    "    # Load your specific checkpoint\n",
    "    checkpoint_path = os.path.join(gen_path_save, \"/kaggle/input/workin/gennormal_200.pth\")\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=model.device)\n",
    "\n",
    "    \n",
    "    # Load model states\n",
    "    model.generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "    # model.discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "    model.discriminator.load_state_dict(checkpoint['discriminator_state_dict'], strict=False)\n",
    "\n",
    "    # Load optimizer states\n",
    "    model.g_optim.load_state_dict(checkpoint['g_optim_state_dict'])\n",
    "    model.d_optim.load_state_dict(checkpoint['d_optim_state_dict'])\n",
    "    start_epoch = 200\n",
    "    # start_epoch = checkpoint['epoch'] + 1  # Resume from next epoch\n",
    "    print(f\"Loaded checkpoint from epoch {start_epoch} , resuming from epoch {start_epoch}\")\n",
    "    \n",
    "    num_epochs = 20\n",
    "    \n",
    "\n",
    "    return model.fit(train_loader, num_epochs)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T13:12:54.258974Z",
     "iopub.status.busy": "2025-04-30T13:12:54.258648Z",
     "iopub.status.idle": "2025-04-30T13:12:54.276595Z",
     "shell.execute_reply": "2025-04-30T13:12:54.275336Z",
     "shell.execute_reply.started": "2025-04-30T13:12:54.258953Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "current_cwd = os.getcwd()\n",
    "src_path = '/'.join(current_cwd.split('/')[:-1])\n",
    "sys.path.append(src_path)\n",
    "\n",
    "g_losses_epoch, d_losses_epoch, d_gp_losses_epoch = train()\n",
    "\n",
    "path = \"/kaggle/working/loss\"\n",
    "\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "filenames = [f'{path}/loss1.csv', f'{path}/loss2.csv', f'{path}/loss3.csv']\n",
    "\n",
    "loss_values = [g_losses_epoch, d_losses_epoch, d_gp_losses_epoch]\n",
    "\n",
    "for i in range(len(loss_values)):\n",
    "    filename = filenames[i]\n",
    "    with open(filename, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Loss'])\n",
    "        for loss in loss_values[i]:  \n",
    "            writer.writerow([loss])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T13:12:54.297656Z",
     "iopub.status.busy": "2025-04-30T13:12:54.296788Z",
     "iopub.status.idle": "2025-04-30T13:12:54.315735Z",
     "shell.execute_reply": "2025-04-30T13:12:54.314967Z",
     "shell.execute_reply.started": "2025-04-30T13:12:54.297622Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_images(generator: Generator, sentence_embeds: Tensor,\n",
    "                    device: torch.device) -> Tensor:\n",
    "    batch_size = sentence_embeds.shape[0]\n",
    "    noise = torch.randn(batch_size, 100).to(device)\n",
    "    return generator(noise, sentence_embeds)\n",
    "\n",
    "\n",
    "def save_image(image: np.ndarray, save_dir: str, file_name: str):\n",
    "    # [-1, 1] --> [0, 255]\n",
    "    image = (image + 1.0) * 127.5\n",
    "    image = image.astype(np.uint8)\n",
    "    image = np.transpose(image, (1, 2, 0))\n",
    "    image = Image.fromarray(image)\n",
    "    fullpath = os.path.join(save_dir, f\"{file_name.replace('/', '_')}.png\")\n",
    "    image.save(fullpath)\n",
    "\n",
    "\n",
    "def sample(generator: Generator, text_encoder: RNNEncoder, batch, save_dir: str):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    images, captions, captions_len, file_names = prepare_data(batch, device)\n",
    "    sent_emb = text_encoder(captions, captions_len).detach()\n",
    "\n",
    "    fake_images = generate_images(generator, sent_emb, device)\n",
    "\n",
    "    for i in range(images.shape[0]):\n",
    "        im = fake_images[i].data.cpu().numpy()\n",
    "        save_image(im, save_dir, file_names[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T13:21:49.336049Z",
     "iopub.status.busy": "2025-04-30T13:21:49.335724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total filenames: 11788\n",
      "Load captions from: /kaggle/input/cv-seattention/cv_seattention/data/captions.pickle\n",
      "Load file names from: /kaggle/input/cv-seattention/cv_seattention/data/test/filenames.pickle (2933)\n",
      "Dictionary size: 5450\n",
      "Embeddings number: 10\n",
      "2933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dry-run Epoch 150...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af0f786c407f4b39b236cfa468f02728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building repr:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ shapes: (32, 2048), (32, 2048)\n",
      "Dry-run Epoch 160...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fbcb5d108484c0da71efdafd3770e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building repr:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ shapes: (32, 2048), (32, 2048)\n",
      "Dry-run Epoch 180...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0644bc036e6b45a8a15101a866359ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building repr:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ shapes: (32, 2048), (32, 2048)\n",
      "Dry-run Epoch 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70ed434be4d1493584763de2230b1d1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building repr:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ shapes: (32, 2048), (32, 2048)\n",
      "\n",
      "All dry-runs passed. Proceeding to full evaluation...\n",
      "\n",
      "\n",
      "=== Full run Epoch 150 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eac98655f0343cb85960cd46c0a1f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building repr:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  FID=280.10 | IS=2.99 ± 0.50\n",
      "\n",
      "=== Full run Epoch 160 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17987485d3c047b1a2836ca46a42e6fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building repr:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  FID=299.19 | IS=3.21 ± 0.43\n",
      "\n",
      "=== Full run Epoch 180 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6fd5d29911e44bc97f2343bc5374a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building repr:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.linalg import sqrtm\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# === Generator checkpoints ===\n",
    "gen_ckpts = [\n",
    "    (150, \"/kaggle/input/workin/gennormal_150.pth\"),\n",
    "    (160, \"/kaggle/input/workin/gennormal_158.pth\"),\n",
    "    (180, \"/kaggle/input/workin/gennormal_180.pth\"),\n",
    "    (200, \"/kaggle/input/workin/gennormal_200.pth\"),\n",
    "]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 32\n",
    "\n",
    "test_loader, n_words = create_loader(\n",
    "    256, batch_size,\n",
    "    \"/kaggle/input/cv-seattention/cv_seattention/data\",\n",
    "    \"test\"\n",
    ")\n",
    "\n",
    "# initialize generator & encoder once\n",
    "dummy_ckpt = torch.load(gen_ckpts[-1][1], map_location=device)\n",
    "generator = Generator(n_channels=32, latent_dim=100).to(device)\n",
    "generator.load_state_dict(dummy_ckpt['generator_state_dict'])\n",
    "generator.eval()\n",
    "\n",
    "text_encoder = RNNEncoder.load(\n",
    "    \"/kaggle/input/cv-seattention/cv_seattention/text_encoder_weights/text_encoder200.pth\",\n",
    "    n_words\n",
    ").to(device).eval()\n",
    "for p in text_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# InceptionV3 feature extractor\n",
    "class InceptionV3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = torch.hub.load(\n",
    "            'pytorch/vision:v0.6.0',\n",
    "            'inception_v3',\n",
    "            pretrained=True\n",
    "        ).to(device).eval()\n",
    "        self.linear = self.model.fc\n",
    "        self.model.fc = nn.Identity()\n",
    "        self.model.dropout = nn.Identity()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_last_layer(self, x):\n",
    "        x = F.interpolate(x, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "        return self.model(x)\n",
    "\n",
    "classifier = InceptionV3().to(device).eval()\n",
    "\n",
    "# FID & IS functions\n",
    "def calculate_fid(real_feats, fake_feats):\n",
    "    mu1, mu2 = real_feats.mean(0), fake_feats.mean(0)\n",
    "    s1, s2 = np.cov(real_feats, rowvar=False), np.cov(fake_feats, rowvar=False)\n",
    "    diff = mu1 - mu2\n",
    "    cm = sqrtm(s1 @ s2)\n",
    "    if not np.isfinite(cm).all():\n",
    "        eps = np.eye(s1.shape[0]) * 1e-8\n",
    "        cm = sqrtm((s1+eps) @ (s2+eps))\n",
    "    if np.iscomplexobj(cm): cm = cm.real\n",
    "    return float(diff@diff + np.trace(s1 + s2 - 2*cm))\n",
    "\n",
    "def inception_score(fake_feats, bs=32):\n",
    "    preds = []\n",
    "    for i in range(0, len(fake_feats), bs):\n",
    "        b = torch.tensor(fake_feats[i:i+bs], device=device)\n",
    "        p = F.softmax(classifier.linear(b), dim=1).detach().cpu().numpy()\n",
    "        preds.append(p)\n",
    "    preds = np.vstack(preds)\n",
    "    py = preds.mean(0)\n",
    "    kl = preds * (np.log(preds+1e-10) - np.log(py+1e-10))\n",
    "    return float(np.exp(np.mean(kl.sum(1)))), float(np.std(kl.sum(1)))\n",
    "\n",
    "def build_representations(generator, text_encoder, max_batches=None):\n",
    "    real_list, fake_list = [], []\n",
    "    for i, batch in enumerate(tqdm(test_loader, desc=\"Building repr\")):\n",
    "        if max_batches and i >= max_batches:\n",
    "            break\n",
    "        imgs, caps, lens, _ = prepare_data(batch, device)\n",
    "        with torch.no_grad():\n",
    "            se, we = text_encoder(caps, lens)\n",
    "            noise = torch.randn(imgs.size(0), 100, device=device)\n",
    "            fakes = generator(noise, se, we)\n",
    "            real_list.append(classifier.get_last_layer(imgs).cpu().numpy())\n",
    "            fake_list.append(classifier.get_last_layer(fakes).cpu().numpy())\n",
    "    return np.vstack(real_list), np.vstack(fake_list)\n",
    "\n",
    "# === 1) Dry-run each checkpoint on 1 batch ===\n",
    "for epoch, path in gen_ckpts:\n",
    "    print(f\"Dry-run Epoch {epoch}...\")\n",
    "    ckpt = torch.load(path, map_location=device)\n",
    "    generator.load_state_dict(ckpt['generator_state_dict'])\n",
    "    try:\n",
    "        r, f = build_representations(generator, text_encoder, max_batches=1)\n",
    "        print(f\"  ✓ shapes: {r.shape}, {f.shape}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error at checkpoint {path}: {e}\")\n",
    "\n",
    "print(\"\\nAll dry-runs passed. Proceeding to full evaluation...\\n\")\n",
    "\n",
    "# === 2) Full evaluation ===\n",
    "epochs, fid_scores, is_means, is_stds = [], [], [], []\n",
    "for epoch, path in gen_ckpts:\n",
    "    print(f\"\\n=== Full run Epoch {epoch} ===\")\n",
    "    ckpt = torch.load(path, map_location=device)\n",
    "    generator.load_state_dict(ckpt['generator_state_dict'])\n",
    "    real_feats, fake_feats = build_representations(generator, text_encoder)\n",
    "    fid = calculate_fid(real_feats, fake_feats)\n",
    "    ism, iss = inception_score(fake_feats, batch_size)\n",
    "    print(f\"  FID={fid:.2f} | IS={ism:.2f} ± {iss:.2f}\")\n",
    "    epochs.append(epoch)\n",
    "    fid_scores.append(fid)\n",
    "    is_means.append(ism)\n",
    "    is_stds.append(iss)\n",
    "\n",
    "# === Plot & summary ===\n",
    "print(\"\\nResults per epoch:\")\n",
    "for e, f, im, isd in zip(epochs, fid_scores, is_means, is_stds):\n",
    "    print(f\" Epoch {e}: FID {f:.2f}, IS {im:.2f}±{isd:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, fid_scores,'b-o'); plt.title(\"FID\"); plt.xlabel(\"Epoch\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.errorbar(epochs, is_means, yerr=is_stds, fmt='r-o', capsize=3); plt.title(\"IS\"); plt.xlabel(\"Epoch\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "best_f = epochs[np.argmin(fid_scores)]\n",
    "best_i = epochs[np.argmax(is_means)]\n",
    "print(f\"\\nBest FID @ epoch {best_f}, Best IS @ epoch {best_i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-30T13:21:44.882099Z",
     "iopub.status.idle": "2025-04-30T13:21:44.882468Z",
     "shell.execute_reply": "2025-04-30T13:21:44.882345Z",
     "shell.execute_reply.started": "2025-04-30T13:21:44.882327Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from scipy.linalg import sqrtm  # <-- use SciPy’s matrix sqrt\n",
    "# from tqdm.auto import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "# from scipy.stats import entropy\n",
    "\n",
    "# # === Generator checkpoints ===\n",
    "# gen_ckpts = [\n",
    "#     (150, \"/kaggle/input/workin/gennormal_150.pth\"),\n",
    "#     (160, \"/kaggle/input/workin/gennormal_158.pth\"),\n",
    "#     (180, \"/kaggle/input/workin/gennormal_180.pth\"),\n",
    "#     (200, \"/kaggle/input/workin/gennormal_200.pth\"),\n",
    "# ]\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # === Load data and text encoder ===\n",
    "# batch_size = 32\n",
    "# test_loader, n_words = create_loader(\n",
    "#     256, batch_size,\n",
    "#     \"/kaggle/input/cv-seattention/cv_seattention/data\",\n",
    "#     \"test\"\n",
    "# )\n",
    "\n",
    "# # Load one generator checkpoint just to initialize the model architecture\n",
    "# checkpoint = torch.load(\"/kaggle/input/workin/gennormal_200.pth\", map_location=device)\n",
    "# generator = Generator(n_channels=32, latent_dim=100).to(device)\n",
    "# generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "\n",
    "# # Load and freeze your RNN text encoder\n",
    "# text_encoder = RNNEncoder.load(\n",
    "#     \"/kaggle/input/cv-seattention/cv_seattention/text_encoder_weights/text_encoder200.pth\",\n",
    "#     n_words\n",
    "# ).to(device).eval()\n",
    "# for p in text_encoder.parameters():\n",
    "#     p.requires_grad = False\n",
    "\n",
    "# # === InceptionV3 for FID/IS ===\n",
    "# class InceptionV3(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#         self.model = torch.hub.load(\n",
    "#             'pytorch/vision:v0.6.0',\n",
    "#             'inception_v3',\n",
    "#             pretrained=True\n",
    "#         ).to(self.device).eval()\n",
    "#         self.linear = self.model.fc\n",
    "#         self.model.fc = nn.Identity()\n",
    "#         self.model.dropout = nn.Identity()\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def get_last_layer(self, x):\n",
    "#         x = F.interpolate(\n",
    "#             x,\n",
    "#             size=(299, 299),\n",
    "#             mode='bilinear',\n",
    "#             align_corners=False\n",
    "#         )\n",
    "#         return self.model(x)\n",
    "\n",
    "# classifier = InceptionV3().to(device).eval()\n",
    "\n",
    "# # === FID calculation ===\n",
    "# def calculate_fid(real_feats: np.ndarray, fake_feats: np.ndarray) -> float:\n",
    "#     mu1, mu2 = real_feats.mean(axis=0), fake_feats.mean(axis=0)\n",
    "#     sigma1 = np.cov(real_feats, rowvar=False)\n",
    "#     sigma2 = np.cov(fake_feats, rowvar=False)\n",
    "#     diff = mu1 - mu2\n",
    "\n",
    "#     covmean = sqrtm(sigma1 @ sigma2)\n",
    "#     if not np.isfinite(covmean).all():  # numerical stability\n",
    "#         offset = np.eye(sigma1.shape[0]) * 1e-8\n",
    "#         covmean = sqrtm((sigma1 + offset) @ (sigma2 + offset))\n",
    "#     if np.iscomplexobj(covmean):\n",
    "#         covmean = covmean.real\n",
    "\n",
    "#     fid = diff @ diff + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
    "#     return float(fid)\n",
    "\n",
    "# # === Inception Score ===\n",
    "# # === Inception Score ===\n",
    "# def inception_score(fake_feats: np.ndarray, batch_size: int = 32) -> Tuple[float, float]:\n",
    "#     preds = []\n",
    "#     for i in range(0, len(fake_feats), batch_size):\n",
    "#         batch = torch.tensor(\n",
    "#             fake_feats[i:i + batch_size],\n",
    "#             dtype=torch.float,\n",
    "#             device=device\n",
    "#         )\n",
    "#         logits = classifier.linear(batch)\n",
    "#         # detach before moving to CPU/Numpy\n",
    "#         p = F.softmax(logits, dim=1).detach().cpu().numpy()\n",
    "#         preds.append(p)\n",
    "#     preds = np.vstack(preds)\n",
    "\n",
    "#     py = preds.mean(axis=0)\n",
    "#     kl = preds * (np.log(preds + 1e-10) - np.log(py + 1e-10))\n",
    "#     scores = np.exp(np.mean(np.sum(kl, axis=1)))\n",
    "#     return float(scores), float(np.std(np.sum(kl, axis=1)))\n",
    "\n",
    "\n",
    "# # === Compute Features ===\n",
    "# def build_representations(generator, text_encoder):\n",
    "#     real_feats, fake_feats = [], []\n",
    "#     generator.eval()\n",
    "\n",
    "#     for batch in tqdm(test_loader, desc=\"Generating features\"):\n",
    "#         images, captions, cap_lens, _ = prepare_data(batch, device)\n",
    "#         with torch.no_grad():\n",
    "#             sent_emb, word_emb = text_encoder(captions, cap_lens)\n",
    "#             noise = torch.randn(images.size(0), 100, device=device)\n",
    "#             fakes = generator(noise, sent_emb, word_emb)\n",
    "\n",
    "#             real_arr = classifier.get_last_layer(images).cpu().numpy()\n",
    "#             fake_arr = classifier.get_last_layer(fakes).cpu().numpy()\n",
    "\n",
    "#             real_feats.append(real_arr)\n",
    "#             fake_feats.append(fake_arr)\n",
    "\n",
    "#     real_feats = np.vstack(real_feats)\n",
    "#     fake_feats = np.vstack(fake_feats)\n",
    "#     return real_feats, fake_feats\n",
    "\n",
    "# # === Run analysis ===\n",
    "# epochs, fid_scores, is_means, is_stds = [], [], [], []\n",
    "\n",
    "# for epoch, ckpt_path in gen_ckpts:\n",
    "#     print(f\"\\n--- Epoch {epoch} ---\")\n",
    "#     gen_ckpt = torch.load(ckpt_path, map_location=device)\n",
    "#     generator.load_state_dict(gen_ckpt['generator_state_dict'])\n",
    "\n",
    "#     real_feats, fake_feats = build_representations(generator, text_encoder)\n",
    "\n",
    "#     fid = calculate_fid(real_feats, fake_feats)\n",
    "#     is_mean, is_std = inception_score(fake_feats, batch_size)\n",
    "\n",
    "#     print(f\"FID: {fid:.2f} | IS: {is_mean:.2f} ± {is_std:.2f}\")\n",
    "#     epochs.append(epoch)\n",
    "#     fid_scores.append(fid)\n",
    "#     is_means.append(is_mean)\n",
    "#     is_stds.append(is_std)\n",
    "\n",
    "# # === Print all results ===\n",
    "# print(\"\\n=== All Epoch Results ===\")\n",
    "# for e, f, im, isd in zip(epochs, fid_scores, is_means, is_stds):\n",
    "#     print(f\"Epoch {e}: FID = {f:.2f}, IS = {im:.2f} ± {isd:.2f}\")\n",
    "\n",
    "# # === Plot results ===\n",
    "# plt.figure(figsize=(12, 5))\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(epochs, fid_scores, 'b-o', label='FID')\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"FID Score\")\n",
    "# plt.title(\"FID vs Epoch\")\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.errorbar(epochs, is_means, yerr=is_stds, fmt='r-o', capsize=5, label='Inception Score')\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Inception Score\")\n",
    "# plt.title(\"Inception Score vs Epoch\")\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # === Summary ===\n",
    "# best_fid_epoch = epochs[np.argmin(fid_scores)]\n",
    "# best_is_epoch = epochs[np.argmax(is_means)]\n",
    "# print(\"\\n--- Summary ---\")\n",
    "# print(f\"Best FID: {min(fid_scores):.2f} at Epoch {best_fid_epoch}\")\n",
    "# print(f\"Best IS: {max(is_means):.2f} at Epoch {best_is_epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-30T13:13:32.322282Z",
     "iopub.status.idle": "2025-04-30T13:13:32.322576Z",
     "shell.execute_reply": "2025-04-30T13:13:32.322456Z",
     "shell.execute_reply.started": "2025-04-30T13:13:32.322443Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Ensure correct path handling\n",
    "current_cwd = os.getcwd()\n",
    "src_path = '/'.join(current_cwd.split('/')[:-1])\n",
    "sys.path.append(src_path)\n",
    "\n",
    "\n",
    "# Initialize generator\n",
    "generator = Generator(n_channels=32, latent_dim=100).to(device)\n",
    "\n",
    "# Load generator checkpoint\n",
    "checkpoint_path = \"/kaggle/input/workin/gennormal_200.pth\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "generator.load_state_dict(checkpoint['generator_state_dict'])  # Corrected\n",
    "generator.eval()  # Set to evaluation mode\n",
    "\n",
    "# Create data loader\n",
    "train_loader,n_words = create_loader(256, 24, \"/kaggle/input/cv-seattention/cv_seattention/data\", \"test\")\n",
    "\n",
    "# Load text encoder\n",
    "text_encoder = RNNEncoder.load(\"/kaggle/input/cv-seattention/cv_seattention/text_encoder_weights/text_encoder200.pth\", n_words)\n",
    "text_encoder.to(device)\n",
    "\n",
    "# Freeze text encoder parameters\n",
    "for p in text_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "text_encoder.eval()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def gen_own_bird(word_caption, name, i):\n",
    "    dataset = train_loader.dataset.dataset\n",
    "    codes = [dataset.word2code[w] for w in word_caption.lower().split()]\n",
    "    \n",
    "    caption = np.array(codes)\n",
    "    pad_caption = np.zeros((18, 1), dtype='int64')\n",
    "\n",
    "    if len(caption) <= 18:\n",
    "        pad_caption[:len(caption), 0] = caption\n",
    "        len_ = len(caption)\n",
    "    else:\n",
    "        indices = list(np.arange(len(caption)))\n",
    "        np.random.shuffle(indices)\n",
    "        pad_caption[:, 0] = caption[np.sort(indices[:18])]\n",
    "        len_ = 18\n",
    "\n",
    "    tensor1 = torch.tensor(pad_caption).reshape(1, -1).to(device)\n",
    "    tensor2 = torch.tensor([len_]).to(device)\n",
    "\n",
    "    # Encode text\n",
    "    embed, word = text_encoder(tensor1, tensor2)\n",
    "\n",
    "    # Generate image\n",
    "    batch_size = embed.shape[0]\n",
    "    noise = torch.randn(batch_size, 100).to(device)\n",
    "    img = generator(noise, embed, word)\n",
    "\n",
    "    # Convert to numpy image\n",
    "    img_np = img[0].data.cpu().numpy()\n",
    "    img_np = (img_np + 1.0) / 2.0  # Rescale from [-1, 1] to [0, 1]\n",
    "    img_np = np.transpose(img_np, (1, 2, 0))  # CHW to HWC\n",
    "\n",
    "    # Save image\n",
    "    save_image(img[0].data.cpu().numpy(), path, name + str(i))\n",
    "\n",
    "    # Plot image\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(img_np)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Generated: '{word_caption}'\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Run generation\n",
    "start_time = time.time()\n",
    "caption = \"a small bird with a red breast eyebrow and crown black and white wings with two white wing bars and black feet and tarsus\"\n",
    "i = 1\n",
    "gen_own_bird(caption, caption, i)\n",
    "\n",
    "print(f\"Time taken: {time.time() - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-30T13:13:32.323876Z",
     "iopub.status.idle": "2025-04-30T13:13:32.324197Z",
     "shell.execute_reply": "2025-04-30T13:13:32.324040Z",
     "shell.execute_reply.started": "2025-04-30T13:13:32.324029Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7022970,
     "sourceId": 11240870,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6996791,
     "sourceId": 11597085,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7273397,
     "sourceId": 11624887,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
