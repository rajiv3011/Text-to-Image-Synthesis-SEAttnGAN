# Text-to-Image-Synthesis-SEAttnGAN
Exploring improvements to **SEA-AttnGAN** for text-to-image synthesis using **InfoNCE**, **WGAN-GP**, and **CLIP** on the **CUB-200 dataset**.

---

## Introduction
Text-to-image synthesis is an exciting direction in Generative AI, enabling realistic images from textual descriptions.  
In this project, we experiment with **SEA Attention GAN (SEAttnGAN)** on the **CUB-200 bird dataset** and propose improvements using:

- **Contrastive InfoNCE loss** ‚Üí better text‚Äìimage alignment  
- **WGAN-GP with a ResNet discriminator** ‚Üí more stable training  
- **CLIP text encoder** ‚Üí richer semantic embeddings  

We analyze performance using **FreÃÅchet Inception Distance (FID)** and showcase **qualitative results**.

---

## Literature Review 

- **Latent Diffusion Models (LDMs)** ‚Üí efficient high-resolution synthesis via latent space.  
- **DF-GAN** ‚Üí one-stage generator with MA-GP discriminator.  
- **SEAttnGAN** ‚Üí simplified, parameter-efficient AttnGAN.  
- **unCLIP** ‚Üí diffusion + CLIP embeddings for diversity and editing.  

---

## Model Architecture

- **Dataset**: CUB-200 (200 bird species, 11,788 images, each with 10 captions)  
- **Text Encoder**: BiLSTM (baseline), CLIP (experiment)  
- **SEA Attention Block**:  
  - Word-level attention (aligns words with visual features)  
  - Sentence-level affine modulation (scales & shifts features)  
- **Generator**: Progressive upsampling ‚Üí 256√ó256 images  
- **Discriminator**: CNN + residual layers + text fusion  
- **Losses**:  
  - Hinge adversarial loss (baseline)  
  - Matching-Aware Loss  
  - InfoNCE contrastive loss  
  - WGAN-GP loss with gradient penalty  
---

## Experiments

### üîπ Experiment 1: Baseline
- Loss: Adversarial + Matching-Aware  
- Epochs: 500  
- Training Time: 70h (T4 GPU)  

### üîπ Experiment 2: +InfoNCE Loss *Best*
- Added contrastive InfoNCE loss (Œª = 0.1)  
- Epochs: 500  
- Training Time: 70h  
- **Best results (lowest FID)**  

### üîπ Experiment 3: ResNet Discriminator + WGAN-GP
- Pretrained ResNet-18 backbone  
- Generator exploited discriminator ‚Üí required balancing updates  
- Epochs: 200  
- Training Time: 40h  

### üîπ Experiment 4: CLIP Encoder
- CLIP (ViT-B/32) + adapter  
- Epochs: 389  
- Training Time: 53h  

---

## Results

### Quantitative (FID Scores)
| Model Variant          | Best FID ‚Üì |
|-------------------------|------------|
| Baseline               | **139.79** |
| +InfoNCE Loss          | **118.76** |
| ResNet-WGAN-GP         | ~217       |
| CLIP Encoder           | ~239       |

## Authors
- Rajiv Chaudhary (ai22btech11021@iith.ac.in)  
- Siddhesh Gholap (ai22btech11007@iith.ac.in)  
- Sudarshan Shivashankar (ai22btech11027@iith.ac.in)  
- Sai Satwik (ai22btech11025@iith.ac.in)  
*(IIT Hyderabad)* 
