{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-30T16:13:33.984930Z",
     "iopub.status.busy": "2025-04-30T16:13:33.984345Z",
     "iopub.status.idle": "2025-04-30T16:14:15.122846Z",
     "shell.execute_reply": "2025-04-30T16:14:15.122299Z",
     "shell.execute_reply.started": "2025-04-30T16:13:33.984897Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        os.path.join(dirname, filename)\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T16:15:06.970510Z",
     "iopub.status.busy": "2025-04-30T16:15:06.970031Z",
     "iopub.status.idle": "2025-04-30T16:15:11.985760Z",
     "shell.execute_reply": "2025-04-30T16:15:11.985118Z",
     "shell.execute_reply.started": "2025-04-30T16:15:06.970487Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class RNNEncoder(nn.Module):\n",
    "    def __init__(self, ntoken, ninput=300, drop_prob=0.5,\n",
    "                 nhidden=128, nlayers=1, bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.n_steps = 18\n",
    "        self.ntoken = ntoken  # size of the dictionary\n",
    "        self.ninput = ninput  # size of each embedding vector\n",
    "        self.drop_prob = drop_prob  # probability of an element to be zeroed\n",
    "        self.nlayers = nlayers  # Number of recurrent layers\n",
    "        self.bidirectional = bidirectional\n",
    "        if bidirectional:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "        # number of features in the hidden state\n",
    "        self.nhidden = nhidden // self.num_directions\n",
    "\n",
    "        self.encoder = nn.Embedding(self.ntoken, self.ninput)\n",
    "        self.drop = nn.Dropout(self.drop_prob)\n",
    "\n",
    "        # dropout: If non-zero, introduces a dropout layer on\n",
    "        # the outputs of each RNN layer except the last layer\n",
    "        self.rnn = nn.LSTM(self.ninput, self.nhidden,\n",
    "                           self.nlayers, batch_first=True,\n",
    "                           dropout=self.drop_prob,\n",
    "                           bidirectional=self.bidirectional)\n",
    "\n",
    "\n",
    "    def forward(self, captions, cap_lens):\n",
    "        # input: torch.LongTensor of size batch x n_steps\n",
    "        # --> emb: batch x n_steps x ninput\n",
    "        emb = self.drop(self.encoder(captions))\n",
    "        #\n",
    "        # Returns: a PackedSequence object\n",
    "        cap_lens = cap_lens.data.tolist()\n",
    "        emb = pack_padded_sequence(emb, cap_lens, batch_first=True)\n",
    "        # #hidden and memory (num_layers * num_directions, batch, hidden_size):\n",
    "        # tensor containing the initial hidden state for each element in batch.\n",
    "        # #output (batch, seq_len, hidden_size * num_directions)\n",
    "        # #or a PackedSequence object:\n",
    "        # tensor containing output features (h_t) from the last layer of RNN\n",
    "        output, hidden = self.rnn(emb)\n",
    "        # PackedSequence object\n",
    "        # --> (batch, seq_len, hidden_size * num_directions)\n",
    "        output = pad_packed_sequence(output, batch_first=True)[0]\n",
    "        words_emb = output.transpose(1, 2)\n",
    "        #print('word',words_emb.shape)\n",
    "        # output = self.drop(output)\n",
    "        # --> batch x hidden_size*num_directions x seq_len\n",
    "        # --> batch x num_directions*hidden_size\n",
    "        sent_emb = hidden[0].transpose(0, 1).contiguous()\n",
    "        sent_emb = sent_emb.view(-1, self.nhidden * self.num_directions)\n",
    "        return sent_emb, words_emb\n",
    "\n",
    "    @staticmethod\n",
    "    def load(weights_path: str, ntoken: int) -> 'RNNEncoder':\n",
    "        text_encoder = RNNEncoder(ntoken, nhidden=256)\n",
    "        state_dict = torch.load(weights_path, map_location=lambda storage, loc: storage)\n",
    "        text_encoder.load_state_dict(state_dict)\n",
    "        return text_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T16:15:24.579835Z",
     "iopub.status.busy": "2025-04-30T16:15:24.579129Z",
     "iopub.status.idle": "2025-04-30T16:15:24.600052Z",
     "shell.execute_reply": "2025-04-30T16:15:24.599206Z",
     "shell.execute_reply.started": "2025-04-30T16:15:24.579807Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "def conv1x1(in_planes, out_planes):\n",
    "    \"\"\"1x1 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1,\n",
    "                   padding=0, bias=False)\n",
    "\n",
    "class GlobalAttentionGeneral(nn.Module):\n",
    "    def __init__(self, idf, cdf):\n",
    "        super().__init__()\n",
    "        self.conv_context = conv1x1(cdf, idf)\n",
    "        self.sm = nn.Softmax(dim=1)\n",
    "        self.mask = None\n",
    "\n",
    "    def applyMask(self, mask):\n",
    "        self.mask = mask  # batch x sourceL\n",
    "\n",
    "    def forward(self, input, context):\n",
    "        \"\"\"\n",
    "        input: batch x idf x ih x iw (queryL=ihxiw)\n",
    "        context: batch x cdf x sourceL\n",
    "        \"\"\"\n",
    "        ih, iw = input.size(2), input.size(3)\n",
    "        queryL = ih * iw\n",
    "        batch_size, sourceL = context.size(0), context.size(2)\n",
    "\n",
    "        # --> batch x queryL x idf\n",
    "        target = input.view(batch_size, -1, queryL)\n",
    "        targetT = torch.transpose(target, 1, 2).contiguous()\n",
    "        \n",
    "        # batch x cdf x sourceL --> batch x cdf x sourceL x 1\n",
    "        sourceT = context.unsqueeze(3)\n",
    "        # --> batch x idf x sourceL\n",
    "        sourceT = self.conv_context(sourceT).squeeze(3)\n",
    "\n",
    "        # Get attention\n",
    "        attn = torch.bmm(targetT, sourceT)  # batch x queryL x sourceL\n",
    "        attn = attn.view(batch_size * queryL, sourceL)\n",
    "        \n",
    "        if self.mask is not None:\n",
    "            mask = self.mask.repeat(queryL, 1)\n",
    "            attn.data.masked_fill_(mask.data, -float('inf'))\n",
    "        \n",
    "        attn = self.sm(attn)\n",
    "        attn = attn.view(batch_size, queryL, sourceL)\n",
    "        attn = torch.transpose(attn, 1, 2).contiguous()\n",
    "\n",
    "        # Apply attention\n",
    "        weightedContext = torch.bmm(sourceT, attn)\n",
    "        weightedContext = weightedContext.view(batch_size, -1, ih, iw)\n",
    "        attn = attn.view(batch_size, -1, ih, iw)\n",
    "\n",
    "        return weightedContext, attn\n",
    "\n",
    "class AffineBlock(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.gamma_mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        self.beta_mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_normal_(module.weight)\n",
    "                nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, x: Tensor, sentence_embed: Tensor) -> Tensor:\n",
    "        scale_param = self.gamma_mlp(sentence_embed).unsqueeze(-1).unsqueeze(-1)\n",
    "        shift_param = self.beta_mlp(sentence_embed).unsqueeze(-1).unsqueeze(-1)\n",
    "        return scale_param * x + shift_param\n",
    "\n",
    "class ResidualBlockG(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, text_dim: int = 256):\n",
    "        super().__init__()\n",
    "        hidden_dim = text_dim // 2\n",
    "        \n",
    "        self.affine1 = AffineBlock(text_dim, hidden_dim, in_channels)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.affine2 = AffineBlock(text_dim, hidden_dim, out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.skip = nn.Conv2d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else nn.Identity()\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x: Tensor, sentence_embed: Tensor) -> Tensor:\n",
    "        residual = self.skip(x)\n",
    "        x = F.leaky_relu(self.affine1(x, sentence_embed), 0.2)\n",
    "        x = self.conv1(x)\n",
    "        x = F.leaky_relu(self.affine2(x, sentence_embed), 0.2)\n",
    "        x = self.conv2(x)\n",
    "        return residual + self.gamma * x\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, n_channels: int = 32, latent_dim: int = 100):\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        \n",
    "        # Initial projection (4x4 spatial size)\n",
    "        self.linear_in = nn.Linear(latent_dim, 8 * n_channels * 4 * 4)\n",
    "        \n",
    "        # Residual blocks with progressive upsampling\n",
    "        self.res_blocks = nn.ModuleList([\n",
    "            ResidualBlockG(8 * n_channels, 8 * n_channels),\n",
    "            ResidualBlockG(8 * n_channels, 4 * n_channels),\n",
    "            ResidualBlockG(4 * n_channels, 2 * n_channels),\n",
    "            ResidualBlockG(2 * n_channels, n_channels),\n",
    "        ])\n",
    "        \n",
    "        # Attention mechanism at 64x64 resolution\n",
    "        self.att = GlobalAttentionGeneral(n_channels, 256)\n",
    "        self.att_conv = nn.Conv2d(2 * n_channels, n_channels, kernel_size=1)\n",
    "        \n",
    "        # Final upsampling to 256x256\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),  # 64->128\n",
    "            nn.Conv2d(n_channels, n_channels//2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n_channels//2),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            \n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),  # 128->256\n",
    "            nn.Conv2d(n_channels//2, n_channels//4, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n_channels//4),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "        \n",
    "        # Output layers\n",
    "        self.conv_out = nn.Sequential(\n",
    "            nn.Conv2d(n_channels//4, 3, kernel_size=3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise: Tensor, sentence_embed: Tensor, word_embed: Tensor) -> Tensor:\n",
    "        # Initial projection\n",
    "        x = self.linear_in(noise).view(-1, 8 * self.n_channels, 4, 4)\n",
    "        \n",
    "        # Process through residual blocks with 2x upsampling each\n",
    "        for block in self.res_blocks:\n",
    "            x = block(x, sentence_embed)\n",
    "            x = F.interpolate(x, scale_factor=2, mode='nearest')  # 4->8->16->32->64\n",
    "        \n",
    "        # Apply attention at 64x64 resolution\n",
    "        attn_out, _ = self.att(x, word_embed)\n",
    "        x = torch.cat([x, attn_out], dim=1)\n",
    "        x = self.att_conv(x)\n",
    "        \n",
    "        # Final upsampling to 256x256\n",
    "        x = self.upsample(x)\n",
    "        return self.conv_out(x)\n",
    "        \n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T16:15:31.966885Z",
     "iopub.status.busy": "2025-04-30T16:15:31.966057Z",
     "iopub.status.idle": "2025-04-30T16:15:33.013005Z",
     "shell.execute_reply": "2025-04-30T16:15:33.012260Z",
     "shell.execute_reply.started": "2025-04-30T16:15:31.966850Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Generator                                [24, 3, 256, 256]         --\n",
       "├─Linear: 1-1                            [24, 4096]                413,696\n",
       "├─ModuleList: 1-2                        --                        --\n",
       "│    └─ResidualBlockG: 2-1               [24, 256, 4, 4]           1\n",
       "│    │    └─Identity: 3-1                [24, 256, 4, 4]           --\n",
       "│    │    └─AffineBlock: 3-2             [24, 256, 4, 4]           131,840\n",
       "│    │    └─Conv2d: 3-3                  [24, 256, 4, 4]           590,080\n",
       "│    │    └─AffineBlock: 3-4             [24, 256, 4, 4]           131,840\n",
       "│    │    └─Conv2d: 3-5                  [24, 256, 4, 4]           590,080\n",
       "│    └─ResidualBlockG: 2-2               [24, 128, 8, 8]           1\n",
       "│    │    └─Conv2d: 3-6                  [24, 128, 8, 8]           32,896\n",
       "│    │    └─AffineBlock: 3-7             [24, 256, 8, 8]           131,840\n",
       "│    │    └─Conv2d: 3-8                  [24, 128, 8, 8]           295,040\n",
       "│    │    └─AffineBlock: 3-9             [24, 128, 8, 8]           98,816\n",
       "│    │    └─Conv2d: 3-10                 [24, 128, 8, 8]           147,584\n",
       "│    └─ResidualBlockG: 2-3               [24, 64, 16, 16]          1\n",
       "│    │    └─Conv2d: 3-11                 [24, 64, 16, 16]          8,256\n",
       "│    │    └─AffineBlock: 3-12            [24, 128, 16, 16]         98,816\n",
       "│    │    └─Conv2d: 3-13                 [24, 64, 16, 16]          73,792\n",
       "│    │    └─AffineBlock: 3-14            [24, 64, 16, 16]          82,304\n",
       "│    │    └─Conv2d: 3-15                 [24, 64, 16, 16]          36,928\n",
       "│    └─ResidualBlockG: 2-4               [24, 32, 32, 32]          1\n",
       "│    │    └─Conv2d: 3-16                 [24, 32, 32, 32]          2,080\n",
       "│    │    └─AffineBlock: 3-17            [24, 64, 32, 32]          82,304\n",
       "│    │    └─Conv2d: 3-18                 [24, 32, 32, 32]          18,464\n",
       "│    │    └─AffineBlock: 3-19            [24, 32, 32, 32]          74,048\n",
       "│    │    └─Conv2d: 3-20                 [24, 32, 32, 32]          9,248\n",
       "├─GlobalAttentionGeneral: 1-3            [24, 32, 64, 64]          --\n",
       "│    └─Conv2d: 2-5                       [24, 32, 18, 1]           8,192\n",
       "│    └─Softmax: 2-6                      [98304, 18]               --\n",
       "├─Conv2d: 1-4                            [24, 32, 64, 64]          2,080\n",
       "├─Sequential: 1-5                        [24, 8, 256, 256]         --\n",
       "│    └─Upsample: 2-7                     [24, 32, 128, 128]        --\n",
       "│    └─Conv2d: 2-8                       [24, 16, 128, 128]        4,624\n",
       "│    └─BatchNorm2d: 2-9                  [24, 16, 128, 128]        32\n",
       "│    └─LeakyReLU: 2-10                   [24, 16, 128, 128]        --\n",
       "│    └─Upsample: 2-11                    [24, 16, 256, 256]        --\n",
       "│    └─Conv2d: 2-12                      [24, 8, 256, 256]         1,160\n",
       "│    └─BatchNorm2d: 2-13                 [24, 8, 256, 256]         16\n",
       "│    └─LeakyReLU: 2-14                   [24, 8, 256, 256]         --\n",
       "├─Sequential: 1-6                        [24, 3, 256, 256]         --\n",
       "│    └─Conv2d: 2-15                      [24, 3, 256, 256]         219\n",
       "│    └─Tanh: 2-16                        [24, 3, 256, 256]         --\n",
       "==========================================================================================\n",
       "Total params: 3,066,279\n",
       "Trainable params: 3,066,279\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 6.87\n",
       "==========================================================================================\n",
       "Input size (MB): 0.48\n",
       "Forward/backward pass size (MB): 401.25\n",
       "Params size (MB): 12.27\n",
       "Estimated Total Size (MB): 413.99\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchinfo\n",
    "\n",
    "gen = Generator(n_channels=32, latent_dim=100)\n",
    "\n",
    "noise = torch.rand((24, 100))\n",
    "sent = torch.rand((24, 256))\n",
    "word = torch.rand((24, 256, 18))\n",
    "\n",
    "torchinfo.summary(gen, input_data=(noise, sent, word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T16:16:06.978491Z",
     "iopub.status.busy": "2025-04-30T16:16:06.977985Z",
     "iopub.status.idle": "2025-04-30T16:16:06.988118Z",
     "shell.execute_reply": "2025-04-30T16:16:06.987339Z",
     "shell.execute_reply.started": "2025-04-30T16:16:06.978467Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "class ResidualBlockD(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Main convolution path\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        # Shortcut path (channel and spatial adjustment)\n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False) \n",
    "            if in_channels != out_channels else nn.Identity(),\n",
    "            nn.AvgPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.shortcut(x) + self.gamma * self.conv(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, n_c: int = 32, sentence_embed_dim: int = 256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Image processing pathway\n",
    "        self.img_encoder = nn.Sequential(\n",
    "            # Initial convolution (no residual)\n",
    "            nn.Conv2d(3, n_c, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "            # Residual downsampling blocks\n",
    "            ResidualBlockD(n_c * 1, n_c * 2),  # 256x256 -> 128x128\n",
    "            ResidualBlockD(n_c * 2, n_c * 4),  # 128x128 -> 64x64\n",
    "            ResidualBlockD(n_c * 4, n_c * 8),  # 64x64 -> 32x32\n",
    "            ResidualBlockD(n_c * 8, n_c * 16), # 32x32 -> 16x16\n",
    "            \n",
    "            # Final downsampling\n",
    "            nn.Conv2d(n_c*16, n_c*16, 4, 2, 1),  # 16x16 -> 8x8\n",
    "            nn.Conv2d(n_c*16, n_c*16, 4, 2, 1)    # 8x8 -> 4x4\n",
    "        )\n",
    "\n",
    "        # Text-image fusion\n",
    "        self.judge_net = nn.Sequential(\n",
    "            nn.Conv2d(n_c*16 + sentence_embed_dim, n_c*2, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(n_c*2, 1, 4, 1, 0)  # Final 1x1 output\n",
    "        )\n",
    "\n",
    "    def build_embeds(self, image: Tensor) -> Tensor:\n",
    "        \"\"\"Extract image features (same as original)\"\"\"\n",
    "        return self.img_encoder(image)\n",
    "\n",
    "    def get_logits(self, image_embed: Tensor, sentence_embed: Tensor) -> Tensor:\n",
    "        \"\"\"Fuse image and text features (same interface)\"\"\"\n",
    "        # Expand text to spatial dimensions\n",
    "        sentence_embed = sentence_embed.view(-1, 256, 1, 1).expand(-1, -1, 4, 4)\n",
    "        \n",
    "        # Concatenate and classify\n",
    "        combined = torch.cat((image_embed, sentence_embed), dim=1)\n",
    "        return self.judge_net(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T16:16:12.943789Z",
     "iopub.status.busy": "2025-04-30T16:16:12.943097Z",
     "iopub.status.idle": "2025-04-30T16:16:16.228900Z",
     "shell.execute_reply": "2025-04-30T16:16:16.228119Z",
     "shell.execute_reply.started": "2025-04-30T16:16:12.943764Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as random\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "\n",
    "class DFGANDataset(Dataset):\n",
    "    def __init__(self, data_dir: str, split: str = \"train\", transform: Optional[Compose] = None):\n",
    "        self.split = split\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "        self.split_dir = os.path.join(data_dir, split)\n",
    "        self.captions_path = os.path.join(self.data_dir, \"captions.pickle\")\n",
    "        self.filenames_path = os.path.join(self.split_dir, \"filenames.pickle\")\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        self.embeddings_num = 10\n",
    "\n",
    "        self.normalize = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "\n",
    "        self.images_dir = os.path.join(self.data_dir, \"CUB_200_2011/CUB_200_2011/images\")\n",
    "        self.bbox_path = os.path.join(self.data_dir, \"CUB_200_2011/CUB_200_2011/bounding_boxes.txt\")\n",
    "        self.images_path = os.path.join(self.data_dir, \"CUB_200_2011/CUB_200_2011/images.txt\")\n",
    "\n",
    "        self.bbox = self._load_bbox()\n",
    "\n",
    "        self.file_names, self.captions, self.code2word, self.word2code = self._load_text_data()\n",
    "\n",
    "        self.n_words = len(self.code2word)\n",
    "        self.num_examples = len(self.file_names)\n",
    "\n",
    "        self._print_info()\n",
    "\n",
    "    def _print_info(self):\n",
    "       # print(f\"Total filenames: {len(self.bbox)}\")\n",
    "        #print(f\"Load captions from: {self.captions_path}\")\n",
    "        #print(f\"Load file names from: {self.filenames_path} ({self.num_examples})\")\n",
    "        #print(f\"Dictionary size: {self.n_words}\")\n",
    "        #print(f\"Embeddings number: {self.embeddings_num}\")\n",
    "        print()\n",
    "\n",
    "    def _load_bbox(self) -> Dict[str, List[int]]:\n",
    "        df_bbox = pd.read_csv(self.bbox_path, delim_whitespace=True, header=None).astype(int)\n",
    "\n",
    "        df_image_names = pd.read_csv(self.images_path, delim_whitespace=True, header=None)\n",
    "        image_names = df_image_names[1].tolist()\n",
    "\n",
    "        filename_bbox = dict()\n",
    "        for i, file_name in enumerate(image_names):\n",
    "            bbox = df_bbox.iloc[i][1:].tolist()\n",
    "            filename_bbox[file_name[:-4]] = bbox\n",
    "\n",
    "        return filename_bbox\n",
    "\n",
    "    def _load_text_data(self) -> Tuple[List[str], List[List[int]],\n",
    "                                       Dict[int, str], Dict[str, int]]:\n",
    "        with open(self.captions_path, 'rb') as file:\n",
    "            train_captions, test_captions, code2word, word2code = pickle.load(file)\n",
    "\n",
    "        filenames = self._load_filenames()\n",
    "\n",
    "        if self.split == 'train':\n",
    "            return filenames, train_captions, code2word, word2code\n",
    "\n",
    "        return filenames, test_captions, code2word, word2code\n",
    "\n",
    "    def _load_filenames(self) -> List[str]:\n",
    "        if os.path.isfile(self.filenames_path):\n",
    "            with open(self.filenames_path, 'rb') as file:\n",
    "                return pickle.load(file)\n",
    "\n",
    "        raise ValueError(f\"File {self.filenames_path} does not exist\")\n",
    "\n",
    "    def _get_caption(self, caption_idx: int) -> Tuple[np.ndarray, int]:\n",
    "        caption = np.array(self.captions[caption_idx])\n",
    "        pad_caption = np.zeros((18, 1), dtype='int64')\n",
    "\n",
    "        if len(caption) <= 18:\n",
    "            pad_caption[:len(caption), 0] = caption\n",
    "            return pad_caption, len(caption)\n",
    "\n",
    "        indices = list(np.arange(len(caption)))\n",
    "        np.random.shuffle(indices)\n",
    "        pad_caption[:, 0] = caption[np.sort(indices[:18])]\n",
    "\n",
    "        return pad_caption, 18\n",
    "\n",
    "    def _get_image(self, image_path: str, bbox: List[int]) -> Tensor:\n",
    "\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        width, height = image.size\n",
    "\n",
    "        r = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n",
    "        center_x = int((2 * bbox[0] + bbox[2]) / 2)\n",
    "        center_y = int((2 * bbox[1] + bbox[3]) / 2)\n",
    "\n",
    "        y1 = np.maximum(0, center_y - r)\n",
    "        y2 = np.minimum(height, center_y + r)\n",
    "        x1 = np.maximum(0, center_x - r)\n",
    "        x2 = np.minimum(width, center_x + r)\n",
    "\n",
    "        image = image.crop((x1, y1, x2, y2))\n",
    "        image = self.normalize(self.transform(image))\n",
    "\n",
    "        return image\n",
    "\n",
    "    def _get_random_caption(self, idx: int) -> Tuple[np.ndarray, int]:\n",
    "        caption_shift = random.randint(0, self.embeddings_num-1)\n",
    "        caption_idx = idx * self.embeddings_num + caption_shift\n",
    "\n",
    "        if caption_idx >= len(self.captions):\n",
    "            caption_idx = len(self.captions) - 1 \n",
    "            \n",
    "        return self._get_caption(caption_idx)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Tensor, np.ndarray, int, str]:\n",
    "        file_name = self.file_names[idx]\n",
    "        image = self._get_image(f\"{self.images_dir}/{file_name}.jpg\", self.bbox[file_name])\n",
    "\n",
    "        encoded_caption, caption_len = self._get_random_caption(idx)\n",
    "\n",
    "        return image, encoded_caption, caption_len, file_name\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T16:16:28.855076Z",
     "iopub.status.busy": "2025-04-30T16:16:28.854681Z",
     "iopub.status.idle": "2025-04-30T16:16:28.860597Z",
     "shell.execute_reply": "2025-04-30T16:16:28.859808Z",
     "shell.execute_reply.started": "2025-04-30T16:16:28.855052Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def prepare_data(batch: Tuple[Tensor, Tensor, Tensor, Tuple[str]],\n",
    "                 device: torch.device) -> Tuple[Tensor, Tensor, Tensor, List[str]]:\n",
    "    images, captions, captions_len, file_names = batch\n",
    "\n",
    "    sorted_cap_lens, sorted_cap_indices = torch.sort(captions_len, 0, True)\n",
    "    sorted_cap_lens = sorted_cap_lens.to(device)\n",
    "\n",
    "    sorted_images = images[sorted_cap_indices].to(device)\n",
    "    sorted_captions = captions[sorted_cap_indices].squeeze().to(device)\n",
    "    sorted_file_names = [file_names[i] for i in sorted_cap_indices.numpy()]\n",
    "\n",
    "    return sorted_images, sorted_captions, sorted_cap_lens, sorted_file_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T16:16:35.697192Z",
     "iopub.status.busy": "2025-04-30T16:16:35.696906Z",
     "iopub.status.idle": "2025-04-30T16:16:35.707141Z",
     "shell.execute_reply": "2025-04-30T16:16:35.706264Z",
     "shell.execute_reply.started": "2025-04-30T16:16:35.697170Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Subset\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "#from objects.dataset import DFGANDataset\n",
    "\n",
    "\n",
    "def create_loader(imsize: int, batch_size: int, data_dir: str, split: str) -> DataLoader:\n",
    "    assert split in [\"train\", \"test\"], \"Wrong split type, expected train or test\"\n",
    "    image_transform = transforms.Compose([\n",
    "        transforms.Resize(int(imsize * 76 / 64)),\n",
    "        transforms.RandomCrop(imsize),\n",
    "        transforms.RandomHorizontalFlip()\n",
    "    ])\n",
    "\n",
    "    dataset = DFGANDataset(data_dir, split, image_transform)\n",
    "    \n",
    "    n_words = dataset.n_words\n",
    "    \n",
    "    subset_size=6000\n",
    "    shuffled_indices = torch.randperm(len(dataset))[:6000].tolist()\n",
    "    dataset = Subset(dataset, shuffled_indices)\n",
    "    \n",
    "\n",
    "    print(len(dataset))\n",
    "\n",
    "    return DataLoader(dataset, batch_size=batch_size, drop_last=True,shuffle=True),n_words\n",
    "\n",
    "\n",
    "def fix_seed(seed: int = 123321):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    print(f\"Seed {seed} fixed\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T16:16:42.650116Z",
     "iopub.status.busy": "2025-04-30T16:16:42.649840Z",
     "iopub.status.idle": "2025-04-30T16:16:42.845942Z",
     "shell.execute_reply": "2025-04-30T16:16:42.845203Z",
     "shell.execute_reply.started": "2025-04-30T16:16:42.650094Z"
    }
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "from typing import Tuple, List\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.utils as vutils\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import trange\n",
    "\n",
    "#from discriminator.model import Discriminator\n",
    "#from generator.model import Generator\n",
    "#from objects.utils import prepare_data\n",
    "#from text_encoder.model import RNNEncoder\n",
    "\n",
    "class DeepFusionGAN:\n",
    "    def __init__(self, n_words, encoder_weights_path: str, image_save_path: str, gen_path_save: str):\n",
    "        super().__init__()\n",
    "        self.image_save_path = image_save_path\n",
    "        self.gen_path_save = gen_path_save\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.generator = Generator(n_channels=32, latent_dim=100).to(self.device)\n",
    "        self.discriminator = Discriminator(n_c=32).to(self.device)\n",
    "\n",
    "        self.text_encoder = RNNEncoder.load(encoder_weights_path, n_words)\n",
    "        self.text_encoder.to(self.device)\n",
    "\n",
    "        for p in self.text_encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.text_encoder.eval()\n",
    "\n",
    "        # self.g_optim = torch.optim.Adam(self.generator.parameters(), lr=0.0001, betas=(0.0, 0.9))\n",
    "        # self.d_optim = torch.optim.Adam(self.discriminator.parameters(), lr=0.0004, betas=(0.0, 0.9))\n",
    "        self.g_optim = torch.optim.Adam(self.generator.parameters(), lr=0.0002,betas=(0.5, 0.99))\n",
    "        self.d_optim = torch.optim.Adam(self.discriminator.parameters(), lr=0.0008, betas=(0.5, 0.99))\n",
    "\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def _zero_grad(self):\n",
    "        self.d_optim.zero_grad()\n",
    "        self.g_optim.zero_grad()\n",
    "\n",
    "    def _compute_gp(self, images: Tensor, sentence_embeds: Tensor) -> Tensor:\n",
    "        batch_size = images.shape[0]\n",
    "\n",
    "        images_interpolated = images.data.requires_grad_()\n",
    "        sentences_interpolated = sentence_embeds.data.requires_grad_()\n",
    "\n",
    "        embeds = self.discriminator.build_embeds(images_interpolated)\n",
    "        logits = self.discriminator.get_logits(embeds, sentences_interpolated)\n",
    "\n",
    "        grad_outputs = torch.ones_like(logits)\n",
    "        grads = torch.autograd.grad(\n",
    "            outputs=logits,\n",
    "            inputs=(images_interpolated, sentences_interpolated),\n",
    "            grad_outputs=grad_outputs,\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )\n",
    "\n",
    "        grad_0 = grads[0].reshape(batch_size, -1)\n",
    "        grad_1 = grads[1].reshape(batch_size, -1)\n",
    "\n",
    "        grad = torch.cat((grad_0, grad_1), dim=1)\n",
    "        grad_norm = grad.norm(2, 1)\n",
    "\n",
    "        return grad_norm\n",
    "\n",
    "    def fit(self, train_loader: DataLoader, num_epochs: int = 500,checkpoint_path: str = None) -> Tuple[List[float], List[float], List[float]]:\n",
    "        g_losses_epoch, d_losses_epoch, d_gp_losses_epoch = [], [], []\n",
    "\n",
    "        start_epoch = 0\n",
    "        if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "            start_epoch = self._load_gen_weights(checkpoint_path)\n",
    "            print(f\"Resuming from epoch {start_epoch}\")\n",
    "\n",
    "        path=\"/kaggle/input/seagan_479/pytorch/default/1/gennormal_479.pth\"\n",
    "        start_epoch=self._load_gen_weights(path)\n",
    "            \n",
    "        for epoch in trange(start_epoch,num_epochs+start_epoch, desc=\"Train Deep Fusion GAN\"):\n",
    "\n",
    "            g_losses, d_losses, d_gp_losses = [], [], []\n",
    "            for batch in train_loader:\n",
    "                images, captions, captions_len, _ = prepare_data(batch, self.device)\n",
    "                batch_size = images.shape[0]\n",
    "\n",
    "\n",
    "                sentence_embeds, words_embs = self.text_encoder(captions, captions_len)\n",
    "                sentence_embeds, words_embs = sentence_embeds.detach(), words_embs.detach()\n",
    "                #sentence_embeds, word_embeds = self.text_encoder(captions, captions_len).detach()\n",
    "                #print(\"images\",images.shape)\n",
    "                real_embeds = self.discriminator.build_embeds(images)\n",
    "                #print(\"real_embeds\",real_embeds.shape)\n",
    "                #print(\"sent\",sentence_embeds.shape)\n",
    "                real_logits = self.discriminator.get_logits(real_embeds, sentence_embeds)\n",
    "                #print(\"real_\",real_logits.shape)\n",
    "                d_loss_real = self.relu(1.0 - real_logits).mean()\n",
    "\n",
    "                shift_embeds = real_embeds[:(batch_size - 1)]\n",
    "                shift_sentence_embeds = sentence_embeds[1:batch_size]\n",
    "                shift_real_image_embeds = self.discriminator.get_logits(shift_embeds, shift_sentence_embeds)\n",
    "\n",
    "                d_loss_mismatch = self.relu(1.0 + shift_real_image_embeds).mean()\n",
    "\n",
    "                noise = torch.randn(batch_size, 100).to(self.device)\n",
    "\n",
    "                #print(\"noise:\", noise.shape)\n",
    "                #print(\"sent \",sentence_embeds.shape)\n",
    "#                print(\"word\",words_embs.shape)\n",
    "                \n",
    "                fake_images = self.generator(noise, sentence_embeds, words_embs)\n",
    "               # print(\"fake\",fake_images.shape)\n",
    "               # if 6==6:\n",
    "               #     return [],[],[]\n",
    "                \n",
    "\n",
    "                fake_embeds = self.discriminator.build_embeds(fake_images.detach())\n",
    "                fake_logits = self.discriminator.get_logits(fake_embeds, sentence_embeds)\n",
    "\n",
    "                d_loss_fake = self.relu(1.0 + fake_logits).mean()\n",
    "\n",
    "                d_loss = d_loss_real + (d_loss_fake + d_loss_mismatch) / 2.0\n",
    "\n",
    "                self._zero_grad()\n",
    "                d_loss.backward()\n",
    "                self.d_optim.step()\n",
    "\n",
    "                d_losses.append(d_loss.item())\n",
    "\n",
    "                grad_l2norm = self._compute_gp(images, sentence_embeds)\n",
    "                d_loss_gp = 2.0 * torch.mean(grad_l2norm ** 6)\n",
    "\n",
    "                self._zero_grad()\n",
    "                d_loss_gp.backward()\n",
    "                self.d_optim.step()\n",
    "\n",
    "                d_gp_losses.append(d_loss_gp.item())\n",
    "\n",
    "                fake_embeds = self.discriminator.build_embeds(fake_images)\n",
    "                fake_logits = self.discriminator.get_logits(fake_embeds, sentence_embeds)\n",
    "                g_loss = -fake_logits.mean()\n",
    "\n",
    "                self._zero_grad()\n",
    "                g_loss.backward()\n",
    "                self.g_optim.step()\n",
    "\n",
    "                g_losses.append(g_loss.item())\n",
    "\n",
    "\n",
    "            g_losses_epoch.append(np.mean(g_losses))\n",
    "            d_losses_epoch.append(np.mean(d_losses))\n",
    "            d_gp_losses_epoch.append(np.mean(d_gp_losses))\n",
    "\n",
    "\n",
    "            if epoch % 1 == 0:\n",
    "             self._save_fake_image(fake_images, epoch)\n",
    "             self._save_gen_weights(epoch)\n",
    "\n",
    "\n",
    "            #print('g_losses_epoch', g_losses_epoch)\n",
    "            #print('d_losses_epoch', d_losses_epoch)\n",
    "            #print('gp_losses_epoch', d_gp_losses_epoch)\n",
    "\n",
    "\n",
    "\n",
    "        return g_losses_epoch, d_losses_epoch, d_gp_losses_epoch\n",
    "\n",
    "    def _save_fake_image(self, fake_images: Tensor, epoch: int):\n",
    "        img_path = os.path.join(self.image_save_path, f\"fake_samplenormal_epoch_{epoch}.png\")\n",
    "        vutils.save_image(fake_images.data, img_path, normalize=True)\n",
    "\n",
    "\n",
    "    def _save_gen_weights(self, epoch: int):\n",
    "        gen_path = os.path.join(self.gen_path_save, f\"gennormal_{epoch}.pth\")\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'generator_state_dict': self.generator.state_dict(),\n",
    "            'discriminator_state_dict': self.discriminator.state_dict(),\n",
    "            'g_optim_state_dict': self.g_optim.state_dict(),\n",
    "            'd_optim_state_dict': self.d_optim.state_dict(),\n",
    "        }\n",
    "        torch.save(checkpoint, gen_path)\n",
    "        print(f\"Model checkpoint saved at epoch {epoch} to {gen_path}\")\n",
    "\n",
    "    def _load_gen_weights(self, checkpoint_path: str) -> int:\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        self.generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "        self.discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "        self.g_optim.load_state_dict(checkpoint['g_optim_state_dict'])\n",
    "        self.d_optim.load_state_dict(checkpoint['d_optim_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        print(f\"Checkpoint loaded from {checkpoint_path} - Starting from epoch {start_epoch}\")\n",
    "        return start_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T16:16:51.427914Z",
     "iopub.status.busy": "2025-04-30T16:16:51.427637Z",
     "iopub.status.idle": "2025-04-30T16:16:51.434606Z",
     "shell.execute_reply": "2025-04-30T16:16:51.433870Z",
     "shell.execute_reply.started": "2025-04-30T16:16:51.427886Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Tuple\n",
    "\n",
    "#from model import DeepFusionGAN\n",
    "#from utils import create_loader, fix_seed\n",
    "def train() -> Tuple[List[float], List[float], List[float]]:\n",
    "    fix_seed()\n",
    "\n",
    "    data_path = \"/kaggle/input/sea-attention-gan1/cv_seattention/data\"\n",
    "    encoder_weights_path = \"/kaggle/input/sea-attention-gan1/cv_seattention/text_encoder_weights/text_encoder200.pth\"\n",
    "    image_save_path = \"/kaggle/working/gen_images\"\n",
    "    gen_path_save = \"/kaggle/working/gen_weights\"\n",
    "\n",
    "    os.makedirs(image_save_path, exist_ok=True)\n",
    "    os.makedirs(gen_path_save, exist_ok=True)\n",
    "\n",
    "    train_loader,n_words= create_loader(256, 32, data_path, \"train\")\n",
    "    model = DeepFusionGAN(n_words=n_words,\n",
    "                          encoder_weights_path=encoder_weights_path,\n",
    "                          image_save_path=image_save_path,\n",
    "                          gen_path_save=gen_path_save)\n",
    "\n",
    "    checkpoint_path = None\n",
    "    checkpoints = sorted(\n",
    "        [f for f in os.listdir(gen_path_save) if f.startswith(\"gennormal_\") and f.endswith(\".pth\")],\n",
    "        key=lambda x: int(x.split('_')[-1].split('.')[0])  # Extract the epoch number from the filename\n",
    "    )\n",
    "\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = checkpoints[-1]  # Get the latest one\n",
    "        checkpoint_path = os.path.join(gen_path_save, latest_checkpoint)\n",
    "        print(f\"Resuming from checkpoint: {checkpoint_path}\")\n",
    "\n",
    "    num_epochs=20\n",
    "    #print(len(train_loader))\n",
    "    #i=0\n",
    "    #for batch in train_loader:\n",
    "    #    images, captions, captions_len, _ = prepare_data(batch,device)\n",
    "    #    batch_size = images.shape[0]\n",
    "    #    i=i+batch_size\n",
    "    #print(i)\n",
    "\n",
    "    #if 5==5:\n",
    "    #    return [],[],[]\n",
    "    return model.fit(train_loader,num_epochs)\n",
    "\n",
    "\n",
    "#if __name__ == '__main__':\n",
    " #   train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T16:16:58.963437Z",
     "iopub.status.busy": "2025-04-30T16:16:58.963104Z",
     "iopub.status.idle": "2025-04-30T16:16:58.967514Z",
     "shell.execute_reply": "2025-04-30T16:16:58.966646Z",
     "shell.execute_reply.started": "2025-04-30T16:16:58.963413Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "current_cwd = os.getcwd()\n",
    "src_path = '/'.join(current_cwd.split('/')[:-1])\n",
    "sys.path.append(src_path)\n",
    "#from train import train\n",
    "#from utils import plot_losses\n",
    "\n",
    "g_losses_epoch, d_losses_epoch, d_gp_losses_epoch = train()\n",
    "\n",
    "path = \"/kaggle/working/loss\"\n",
    "\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "filenames = [f'{path}/loss1.csv', f'{path}/loss2.csv', f'{path}/loss3.csv']\n",
    "\n",
    "loss_values = [g_losses_epoch, d_losses_epoch, d_gp_losses_epoch]\n",
    "\n",
    "for i in range(len(loss_values)):\n",
    "    filename = filenames[i]\n",
    "    with open(filename, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Loss'])\n",
    "        for loss in loss_values[i]:  \n",
    "            writer.writerow([loss])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import torchvision.utils as vutils\n",
    "from torch import Tensor\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Ensure correct path handling\n",
    "current_cwd = os.getcwd()\n",
    "src_path = '/'.join(current_cwd.split('/')[:-1])\n",
    "sys.path.append(src_path)\n",
    "\n",
    "# Initialize generator\n",
    "generator = Generator(n_channels=32, latent_dim=100).to(device)\n",
    "\n",
    "# Load generator checkpoint\n",
    "checkpoint_path = r\"/kaggle/input/seagan_379/pytorch/default/1/gennormal_379.pth\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "generator.load_state_dict(checkpoint[\"generator_state_dict\"])\n",
    "generator.eval()\n",
    "\n",
    "# Create data loader\n",
    "train_loader, n_words = create_loader(256, 24, r\"/kaggle/input/sea-attention-gan1/cv_seattention/data\", \"test\")\n",
    "\n",
    "# Load text encoder\n",
    "text_encoder = RNNEncoder.load(r\"/kaggle/input/sea-attention-gan1/cv_seattention/text_encoder_weights/text_encoder200.pth\", n_words)\n",
    "text_encoder.to(device)\n",
    "text_encoder.eval()\n",
    "\n",
    "# Create output directory\n",
    "output_path = r\"/kaggle/working\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "def save_image_grid(images: Tensor, filename: str, nrow: int = 8, normalize: bool = True):\n",
    "    \"\"\"Save a grid of images using torchvision.utils.save_image\"\"\"\n",
    "    # Ensure the images are on CPU\n",
    "    images = images.cpu()\n",
    "    \n",
    "    # Create save path\n",
    "    save_path = os.path.join(output_path, filename)\n",
    "    \n",
    "    # Save the grid\n",
    "    vutils.save_image(\n",
    "        images,\n",
    "        save_path,\n",
    "        nrow=nrow,\n",
    "        normalize=normalize,\n",
    "        padding=2,\n",
    "        scale_each=True\n",
    "    )\n",
    "\n",
    "def generate_multiple_images(word_caption: str, base_name: str, num_images: int = 16):\n",
    "    \"\"\"\n",
    "    Generates multiple images from a single caption and saves them in a grid\n",
    "    \n",
    "    Args:\n",
    "        word_caption: Text caption to generate images from\n",
    "        base_name: Base name for saving the output file\n",
    "        num_images: Number of different images to generate (default: 16)\n",
    "    \"\"\"\n",
    "    dataset = train_loader.dataset.dataset\n",
    "    codes = [dataset.word2code[w] for w in word_caption.lower().split()]\n",
    "    \n",
    "    # Prepare caption tensor\n",
    "    caption = np.array(codes)\n",
    "    pad_caption = np.zeros((18, 1), dtype='int64')\n",
    "    \n",
    "    if len(caption) <= 18:\n",
    "        pad_caption[:len(caption), 0] = caption\n",
    "        len_ = len(caption)\n",
    "    else:\n",
    "        indices = list(np.arange(len(caption)))\n",
    "        np.random.shuffle(indices)\n",
    "        pad_caption[:, 0] = caption[np.sort(indices[:18])]\n",
    "        len_ = 18\n",
    "    \n",
    "    # Create text tensors - we'll process one at a time to avoid dimension issues\n",
    "    caption_tensor = torch.tensor(pad_caption).reshape(1, -1).to(device)\n",
    "    length_tensor = torch.tensor([len_]).to(device)\n",
    "    \n",
    "    # Generate all images\n",
    "    generated_images = []\n",
    "    for _ in range(num_images):\n",
    "        # Encode text\n",
    "        embed, word = text_encoder(caption_tensor, length_tensor)\n",
    "        \n",
    "        # Generate image with different noise each time\n",
    "        noise = torch.randn(1, 100).to(device)\n",
    "        with torch.no_grad():\n",
    "            img = generator(noise, embed, word)\n",
    "            generated_images.append(img)\n",
    "    \n",
    "    # Combine all generated images into a single tensor\n",
    "    generated_images = torch.cat(generated_images, dim=0)\n",
    "    \n",
    "    # Save the grid of images\n",
    "    grid_filename = f\"grid_{base_name}.png\"\n",
    "    save_image_grid(\n",
    "        generated_images,\n",
    "        grid_filename,\n",
    "        nrow=int(num_images**0.5)\n",
    "    )\n",
    "    \n",
    "    print(f\"Saved generated images grid to: {os.path.join(output_path, grid_filename)}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    \n",
    "    caption = \"red bird with white belly and black beak\"\n",
    "    generate_multiple_images(\n",
    "        word_caption=caption,\n",
    "        base_name=\"yelow_white_bird\",\n",
    "        num_images=16  # Will create 4x4 grid\n",
    "    )\n",
    "    \n",
    "    print(f\"Time taken: {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T16:20:56.235820Z",
     "iopub.status.busy": "2025-04-30T16:20:56.235384Z",
     "iopub.status.idle": "2025-04-30T16:20:56.584086Z",
     "shell.execute_reply": "2025-04-30T16:20:56.583368Z",
     "shell.execute_reply.started": "2025-04-30T16:20:56.235799Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=2048, out_features=1000, bias=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy import linalg\n",
    "from scipy.linalg import sqrtm\n",
    "from scipy.stats import entropy\n",
    "from torch.nn.functional import adaptive_avg_pool2d\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "def generate_images(generator: Generator, sentence_embeds: Tensor,word,\n",
    "                    device: torch.device) -> Tensor:\n",
    "    batch_size = sentence_embeds.shape[0]\n",
    "    noise = torch.randn(batch_size, 100).to(device)\n",
    "    return generator(noise, sentence_embeds,word)\n",
    "\n",
    "class InceptionV3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.device = f'cuda:{0}' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = torch.hub.load('pytorch/vision:v0.6.0', 'inception_v3', pretrained=True).to(self.device)\n",
    "        print(self.model.fc)\n",
    "        self.linear = self.model.fc\n",
    "        self.model.fc, self.model.dropout = [nn.Sequential()] * 2\n",
    "      \n",
    "    @torch.no_grad()\n",
    "    def get_last_layer(self, x):\n",
    "        x = F.interpolate(x, size=300, mode='bilinear', align_corners=False, recompute_scale_factor=False)\n",
    "        return self.model(x)\n",
    "\n",
    "classifier = InceptionV3().to(device)\n",
    "classifier = classifier.eval()\n",
    "\n",
    "def calculate_fid(repr1, repr2):\n",
    "    mu_r, mu_g = np.mean(repr1, axis=0), np.mean(repr2, axis=0)\n",
    "    sigma_r, sigma_g = np.cov(repr1, rowvar=False), np.cov(repr2, rowvar=False)\n",
    "    \n",
    "    diff = mu_r - mu_g\n",
    "    diff_square_norm = diff.dot(diff)\n",
    "    \n",
    "    product = sigma_r.dot(sigma_g)\n",
    "    sqrt_product, _ = sqrtm(product, disp=False)\n",
    "    \n",
    "\n",
    "    if not np.isfinite(sqrt_product).all():\n",
    "        eye_matrix = np.eye(sigma_r.shape[0]) * 1e-8\n",
    "        sqrt_product = linalg.sqrtm((sigma_r + eye_matrix).dot(sigma_g + eye_matrix))\n",
    "\n",
    "    if np.iscomplexobj(sqrt_product):\n",
    "        sqrt_product = sqrt_product.real\n",
    "\n",
    "    fid = diff_square_norm + np.trace(sigma_r + sigma_g - 2 * sqrt_product)\n",
    "    \n",
    "    return fid\n",
    "\n",
    "\n",
    "def build_representations():\n",
    "    real_reprs = np.zeros((len(test_loader) * batch_size, 2048))\n",
    "    fake_reprs = np.zeros((len(test_loader) * batch_size, 2048))\n",
    "\n",
    "    \n",
    "    for i, batch in enumerate(tqdm(test_loader, desc=\"Build representations\")):\n",
    "        images, captions, captions_len, file_names = prepare_data(batch, device)\n",
    "        sent_emb, word = text_encoder(captions, captions_len)\n",
    "        sent_emb = sent_emb.detach()\n",
    "        fake_images = generate_images(generator, sent_emb,word, device)\n",
    "\n",
    "        clf_out_real = classifier.get_last_layer(images)\n",
    "        clf_out_fake = classifier.get_last_layer(fake_images)\n",
    "\n",
    "        real_reprs[i * batch_size: (i + 1) * batch_size] = clf_out_real.cpu().numpy()\n",
    "        fake_reprs[i * batch_size: (i + 1) * batch_size] = clf_out_fake.cpu().numpy()\n",
    "            \n",
    "    return real_reprs, fake_reprs\n",
    "\n",
    "\n",
    "def inception_score(reprs, batch_size):\n",
    "    def get_pred(x):\n",
    "        x = classifier.linear(torch.tensor(x, dtype=torch.float))\n",
    "        return F.softmax(x).data.cpu().numpy()\n",
    "\n",
    "\n",
    "    preds = np.zeros((reprs.shape[0], 1000))\n",
    "\n",
    "    splits = 0\n",
    "    for i in range(0, len(preds), batch_size):\n",
    "        aaai = reprs[i:i + batch_size]\n",
    "        aai = torch.tensor(aaai)\n",
    "        aai = aai.to(device)\n",
    "        z = get_pred(aai)\n",
    "        preds[i:i + batch_size] = z\n",
    "        splits += 1\n",
    "    \n",
    "    split_scores = []\n",
    "\n",
    "    for k in range(splits):\n",
    "        part = preds[k * batch_size: (k+1) * batch_size, :]\n",
    "        py = np.mean(part, axis=0)\n",
    "        \n",
    "        scores = []\n",
    "        for i in range(part.shape[0]):\n",
    "            pyx = part[i, :]\n",
    "            scores.append(entropy(pyx, py))\n",
    "            \n",
    "        split_scores.append(np.exp(np.mean(scores)))\n",
    "\n",
    "    return np.mean(split_scores), np.std(split_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T16:33:13.509189Z",
     "iopub.status.busy": "2025-04-30T16:33:13.508908Z",
     "iopub.status.idle": "2025-04-30T16:41:54.222657Z",
     "shell.execute_reply": "2025-04-30T16:41:54.221872Z",
     "shell.execute_reply.started": "2025-04-30T16:33:13.509166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2933\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "762d4e0a100d494e911f4783fad12737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Build representations:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID value = 238.65533367623294\n",
      "inception score mean:  3.101332777577382\n",
      "inception score std:  0.3259751282801736\n",
      "\n",
      "2933\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1797870d544a2b94d4cb39ad49718d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Build representations:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID value = 234.6789936578817\n",
      "inception score mean:  3.5348597509169926\n",
      "inception score std:  0.3699208026649979\n",
      "\n",
      "2933\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "490f6f2bf009414bba38edc690e9ca6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Build representations:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID value = 207.765198018991\n",
      "inception score mean:  4.3050277785852025\n",
      "inception score std:  0.40731560328392735\n",
      "\n",
      "2933\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "058f7e61fa2c453e99edef6ecef41109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Build representations:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID value = 206.3760463636976\n",
      "inception score mean:  3.8590585991231956\n",
      "inception score std:  0.38370402962249206\n",
      "\n",
      "2933\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64297df640d340c79b7b762b850888ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Build representations:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID value = 180.56736417723857\n",
      "inception score mean:  4.375464148279356\n",
      "inception score std:  0.44043150717757507\n",
      "\n",
      "2933\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb38428fd6c4135a85df6e49c42bb53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Build representations:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID value = 172.21383511081666\n",
      "inception score mean:  4.402163436464314\n",
      "inception score std:  0.5124797928285968\n",
      "\n",
      "2933\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa57716b5d34983bfb147578b99d9df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Build representations:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID value = 170.6698478020421\n",
      "inception score mean:  4.897913669967756\n",
      "inception score std:  0.5682481218881025\n",
      "\n",
      "2933\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd03f45bb744f0b907bb5dcbf1a5d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Build representations:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID value = 157.61166460359553\n",
      "inception score mean:  4.573826642566516\n",
      "inception score std:  0.4525234629671817\n",
      "\n",
      "2933\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc23ad63e59f4ade8e1f03175f251efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Build representations:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID value = 158.51134397783883\n",
      "inception score mean:  4.572810571636573\n",
      "inception score std:  0.5735019684376201\n",
      "\n",
      "2933\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "905df25990284576b208f5646d9643ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Build representations:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID value = 139.71830456297522\n",
      "inception score mean:  4.7751866008746005\n",
      "inception score std:  0.5978062723976851\n"
     ]
    }
   ],
   "source": [
    "model_path=[\"/kaggle/input/seagan_49/pytorch/default/1/gennormal_49.pth\",\n",
    "           \"/kaggle/input/seagan_99/pytorch/default/1/gennormal_99.pth\",\n",
    "           \"/kaggle/input/seagan_149/pytorch/default/1/gennormal_149.pth\",\n",
    "           \"/kaggle/input/seagan_199/pytorch/default/1/gennormal_199.pth\",\n",
    "           \"/kaggle/input/seagan_249/pytorch/default/1/gennormal_249.pth\",\n",
    "           \"/kaggle/input/seagan_299/pytorch/default/1/gennormal_299.pth\",\n",
    "           \"/kaggle/input/seagan_350/pytorch/default/1/gennormal_350.pth\",\n",
    "           \"/kaggle/input/seagan_399/pytorch/default/1/gennormal_399.pth\",\n",
    "           \"/kaggle/input/seagan_360/pytorch/default/1/gennormal_460.pth\",\n",
    "           \"/kaggle/input/seagan_499/pytorch/default/1/gennormal_499.pth\" ]\n",
    "\n",
    "fid_test=[]\n",
    "inception_test=[]\n",
    "for i in range(10):\n",
    "    batch_size = 32\n",
    "    test_loader, n_words = create_loader(256, batch_size, \"/kaggle/input/sea-attention-gan1/cv_seattention/data\", \"test\")\n",
    "\n",
    "    checkpoint = torch.load(model_path[i], map_location=device)\n",
    "\n",
    "    generator = Generator(n_channels=32, latent_dim=100).to(device)\n",
    "    generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "    generator.eval()\n",
    "\n",
    "    text_encoder = RNNEncoder.load(\"/kaggle/input/sea-attention-gan1/cv_seattention/text_encoder_weights/text_encoder200.pth\", n_words)\n",
    "    text_encoder.to(device)\n",
    "\n",
    "    for p in text_encoder.parameters():\n",
    "        p.requires_grad = False\n",
    "    text_encoder = text_encoder.eval()\n",
    "\n",
    "    real_values, fake_values = build_representations()\n",
    "    real_values = torch.tensor(real_values)\n",
    "    fake_values = torch.tensor(fake_values)\n",
    "\n",
    "    fid_value = calculate_fid(real_values.numpy(), fake_values.numpy())\n",
    "    fid_test.append(fid_value)\n",
    "    print(f\"FID value = {fid_value}\")\n",
    "\n",
    "\n",
    "    a,b = inception_score(fake_values, batch_size)\n",
    "    inception_test.append(a)\n",
    "    print('inception score mean: ',a)\n",
    "    print('inception score std: ',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T16:52:24.004107Z",
     "iopub.status.busy": "2025-04-30T16:52:24.003828Z",
     "iopub.status.idle": "2025-04-30T16:52:24.143198Z",
     "shell.execute_reply": "2025-04-30T16:52:24.142516Z",
     "shell.execute_reply.started": "2025-04-30T16:52:24.004086Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=[50,100,150,200,250,300,350,400,450,500]\n",
    "plt.plot(x,fid_test)\n",
    "plt.title(\"Fid vs epochs\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"fid\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T16:56:46.967477Z",
     "iopub.status.busy": "2025-04-30T16:56:46.967169Z",
     "iopub.status.idle": "2025-04-30T16:56:47.084821Z",
     "shell.execute_reply": "2025-04-30T16:56:47.084133Z",
     "shell.execute_reply.started": "2025-04-30T16:56:46.967454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.101332777577382, 3.5348597509169926, 4.3050277785852025, 3.8590585991231956, 4.375464148279356, 4.402163436464314, 4.897913669967756, 4.573826642566516, 4.572810571636573, 4.7751866008746005]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=[50,100,150,200,250,300,350,400,450,500]\n",
    "print(inception_test)\n",
    "plt.plot(x,inception_test)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6984726,
     "sourceId": 11188673,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 323381,
     "modelInstanceId": 302863,
     "sourceId": 365081,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 323383,
     "modelInstanceId": 302865,
     "sourceId": 365083,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 323384,
     "modelInstanceId": 302866,
     "sourceId": 365084,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 323385,
     "modelInstanceId": 302867,
     "sourceId": 365086,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 323386,
     "modelInstanceId": 302868,
     "sourceId": 365087,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 323387,
     "modelInstanceId": 302869,
     "sourceId": 365088,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 323388,
     "modelInstanceId": 302870,
     "sourceId": 365090,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 323390,
     "modelInstanceId": 302872,
     "sourceId": 365092,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 323391,
     "modelInstanceId": 302873,
     "sourceId": 365093,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 323392,
     "modelInstanceId": 302874,
     "sourceId": 365094,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 324413,
     "modelInstanceId": 303944,
     "sourceId": 366561,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 324423,
     "modelInstanceId": 303955,
     "sourceId": 366574,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 324429,
     "modelInstanceId": 303960,
     "sourceId": 366580,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
